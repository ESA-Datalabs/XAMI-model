{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from pycocotools import mask as maskUtils\n",
    "import cv2\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "import os\n",
    "# !pip install astropy\n",
    "import astropy\n",
    "from astropy.io import fits\n",
    "from scipy.interpolate import interp1d\n",
    "from astropy.visualization import ZScaleInterval, ImageNormalize\n",
    "import torch\n",
    "import random\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import my files\n",
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OM_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code/scaled_raw/'\n",
    "OM_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Roboflow annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0==1:\n",
    "\n",
    "    def display_masks(image_path, masks):\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "    \n",
    "        for mask in masks:\n",
    "            if isinstance(mask[0], list):  # If the mask is a polygon\n",
    "                polygon_points = np.array(mask[0]).reshape(-1, 2)\n",
    "                polygon = Polygon(polygon_points, edgecolor='g', facecolor='none')\n",
    "                ax.add_patch(polygon)\n",
    "            else:  # If the mask is RLE\n",
    "                binary_mask = maskUtils.decode(mask)\n",
    "                ax.imshow(binary_mask, alpha=0.5, cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "    # Load the JSON file\n",
    "    dir_train_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code/OM_sky_images-6/train/'\n",
    "    json_file_path = dir_train_path+'_annotations.coco.json'\n",
    "    \n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f) # dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n",
    "    \n",
    "    # Iterate through each image and its annotations\n",
    "    for image_info in data['images']:\n",
    "        image_id = image_info['id']\n",
    "        image_path = dir_train_path + image_info['file_name']\n",
    "        \n",
    "        # Find annotations for the current image\n",
    "        annotations = [anno for anno in data['annotations'] if anno['image_id'] == image_id]\n",
    "        # annotations: dict_keys(['id', 'image_id', 'category_id', 'bbox', 'area', 'segmentation', 'iscrowd'])\n",
    "        \n",
    "        # Extract and display masks for the image\n",
    "        masks = [anno['segmentation'] for anno in annotations]\n",
    "        display_masks(image_path, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mobile SAM *\n",
    ">\n",
    "> MobileSAM is approximately 5 times smaller and 7 times faster than the current FastSAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/')\n",
    "\n",
    "import cv2\n",
    "import mobile_sam\n",
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-master/weights/mobile_sam.pt\"\n",
    "\n",
    "device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device=device)\n",
    "mobile_sam_model.eval();\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import math\n",
    "\n",
    "def MobileSAM_predict(image_path, show_annots = True, mask_on_negative = None):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # using the pixel mean and std specific to each image instead of the standard one.\n",
    "        image_T = np.transpose(image, (2, 1, 0))\n",
    "        pixel_mean = torch.as_tensor([np.mean(image_T[0]), np.mean(image_T[1]),np.mean(image_T[2])], dtype=torch.float, device=device)\n",
    "        pixel_std = torch.as_tensor([np.std(image_T[0]), np.std(image_T[1]),np.std(image_T[2])], dtype=torch.float, device=device)\n",
    "        \n",
    "        mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
    "                \n",
    "        predictor = SamPredictor(mobile_sam_model)\n",
    "        predictor.set_image(image)\n",
    "        \n",
    "        masks_np, iou_predictions_np, low_res_masks_np = predictor.predict()\n",
    "        if show_annots:\n",
    "            image_bgr = cv2.imread(image_path)\n",
    "            \n",
    "            mask_generator = SamAutomaticMaskGenerator(mobile_sam_model, points_per_side=None, point_grids=img_points)\n",
    "            mobile_sam_result = mask_generator.generate(image)\n",
    "            mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "            \n",
    "            if mask_on_negative is not None:\n",
    "                mobile_sam_result = remove_masks(sam_result=mobile_sam_result, mask_on_negative=mask_on_negative, threshold=50, remove_big_masks=True, img_shape = image.shape)\n",
    "    \n",
    "            detections = sv.Detections.from_sam(mobile_sam_result)\n",
    "        \n",
    "            annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "            image = Image.fromarray(annotated_image)\n",
    "            output_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/mobile_sam/\"+image_path.split('/')[-1].replace(\".png\", \"_mobile_sam_nonnegative.png\")\n",
    "            image.save(output_path)\n",
    "            \n",
    "        '''\n",
    "        predictor.predict() returns:\n",
    "              (np.ndarray): The output masks in CxHxW format, where C is the\n",
    "                number of masks, and (H, W) is the original image size.\n",
    "              (np.ndarray): An array of length C containing the model's\n",
    "                predictions for the quality of each mask.\n",
    "              (np.ndarray): An array of shape CxHxW, where C is the number\n",
    "                of masks and H=W=256. These low resolution logits can be passed to\n",
    "                a subsequent iteration as mask input.\n",
    "        '''\n",
    "        \n",
    "        return masks_np, iou_predictions_np, low_res_masks_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "#     extracted_bboxes_points = json.load(f)\n",
    "extracted_bboxes_points['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "image_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code/scaled_raw/clahe_S0720251301_L.png'\n",
    "\n",
    "with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "    extracted_bboxes_points = json.load(f)\n",
    "\n",
    "img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                       if filename in image_path.split(\"/\")[-1]])\n",
    "img_points = img_points/255.0\n",
    "\n",
    "mask_on_negative = mask_and_plot_image(image_path.replace(\".png\", \".fits\").replace('clahe_', ''))\n",
    "\n",
    "start_time = time.time()\n",
    "output_mobile_sam = MobileSAM_predict(image_path, mask_on_negative=mask_on_negative)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Mobile SAM predict time/img: {end_time-start_time} s\")\n",
    "image = cv2.imread('/workspace/raid/OM_DeepLearning/XMM_OM_dataset/mobile_sam/'+image_path.split('/')[-1].replace(\".png\", \"_mobile_sam_nonnegative.png\"))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.title('Mobile SAM pred. on image input')\n",
    "plt.imsave('mobile_sam_entire_image.png', image)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(image), np.std(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(img_points[0][:, 0]*255, img_points[0][:, 1]*255, s=10, c='red')\n",
    "# plt.gca().invert_yaxis() \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stophere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN3DPGZSn57p"
   },
   "source": [
    "# Install Segment Anything Model (SAM) and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1H9YruJen0Q8",
    "outputId": "9098dc8e-3476-4166-d48e-d88e3ba76267"
   },
   "outputs": [],
   "source": [
    "# %cd {HOME}\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3CtzYroC2Lb",
    "outputId": "9e3c46a8-69d0-42cc-dbfe-01a435fa4daf"
   },
   "outputs": [],
   "source": [
    "# !pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VeYIWh1iDWW"
   },
   "source": [
    "### Download SAM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aszw1OxBwowI",
    "outputId": "64f3e115-2442-4181-9a7e-4eafdb4464f2"
   },
   "outputs": [],
   "source": [
    "# %cd {HOME}\n",
    "# !mkdir {HOME}/weights\n",
    "# %cd {HOME}/weights\n",
    "\n",
    "# !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ SAM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlhbd_f4xfiJ"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6_9PSZupghA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())] # find all available GPUs in the cluster\n",
    "DEVICE = torch.device(f'cuda:7' if torch.cuda.is_available() else 'cpu') # take one available\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
    "sam.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ The predictor function (can remove masks on negative pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "    extracted_bboxes_points = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code/scaled_raw/clahe_S0720251301_L.png\"\n",
    "mask_on_negative = mask_and_plot_image(\"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/S0720251301_L.fits\")\n",
    "\n",
    "img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                       if filename in image_path.split(\"/\")[-1]])\n",
    "img_points = img_points/255\n",
    "\n",
    "start_time = time.time()\n",
    "_, _, annotated_image1 = SAM_predictor(SamAutomaticMaskGenerator, sam, image_path, mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "print(f\"original SAM predict time/img: {time.time()-start_time} s\")\n",
    "\n",
    "start_time2 = time.time()\n",
    "_, _, annotated_image2 = SAM_predictor(SamAutomaticMaskGenerator, sam, image_path, mask_on_negative=mask_on_negative, img_grid_points=img_points)\n",
    "print(f\"original SAM predict time/img: {time.time()-start_time2} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs= plt.subplots(1, 3, figsize=(10, 10)) \n",
    "\n",
    "# axs[0].imshow(annotated_image1)\n",
    "# axs[0].set_title(f'Original SAM inference \\ntime: {round(end_time-start_time, 3)}s, {1024} grid points', \\\n",
    "#                  fontfamily='monospace', fontsize=10)\n",
    "\n",
    "# axs[1].imshow(annotated_image2)\n",
    "# axs[1].set_title(f'Original SAM inference\\ntime: {round(end_time2-start_time2, 3)}s, {img_points.shape[1]} grid points', \\\n",
    "#                 fontfamily='monospace', fontsize=10)\n",
    "\n",
    "# axs[2].imshow(cv2.cvtColor(cv2.imread(image_path2), cv2.COLOR_BGR2RGB))\n",
    "# axs[2].scatter(img_points[0][:, 0]*255, img_points[0][:, 1]*255, s=10, c='red')\n",
    "# axs[2].set_title(f'Extracted sources grid points', \\\n",
    "#                  fontfamily='monospace', fontsize=10)\n",
    "# axs[2].set_aspect('equal', 'box')  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig('plots/sam_grid_points_comparison.png', dpi=1000)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "\n",
    "generate_all_predictions = True\n",
    "\n",
    "inference_times = []\n",
    "if generate_all_predictions:\n",
    "    dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "    for file_ in dir_files:\n",
    "        try:\n",
    "            if '.png' in file_ and 'clahe' in file_:\n",
    "                \n",
    "                mask_on_negative = mask_and_plot_image(OM_dir+file_.replace('.png', '.fits').replace('clahe_', ''))\n",
    "                img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                                       if filename in file_.split(\"/\")[-1]])\n",
    "                img_points = img_points/255.0\n",
    "                \n",
    "                start_time = time.time()\n",
    "                _, _, annotated_image = SAM_predictor(SamAutomaticMaskGenerator, sam, OM_dir+file_, mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                print(f\"original SAM predict time/img: {end_time-start_time} s\")\n",
    "                inference_times.append(end_time-start_time)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    # with open('cell_SAM_predict_with_threshold.txt', 'w') as f:\n",
    "    #     f.write(str(cap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(inference_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "\n",
    "generate_all_predictions = True\n",
    "\n",
    "inference_times = []\n",
    "if generate_all_predictions:\n",
    "    dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "    for file_ in dir_files:\n",
    "        try:\n",
    "            if '.png' in file_ and 'clahe' in file_:\n",
    "                \n",
    "                mask_on_negative = mask_and_plot_image(OM_dir+file_.replace('.png', '.fits').replace('clahe_', ''))\n",
    "                img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                                       if filename in file_.split(\"/\")[-1]])\n",
    "                img_points = img_points/255.0\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "                if len(img_points)==0:\n",
    "                    img_points = None\n",
    "                    \n",
    "                _, _, annotated_image = SAM_predictor(SamAutomaticMaskGenerator, sam, OM_dir+file_, mask_on_negative=mask_on_negative, \n",
    "                                                           img_grid_points=img_points)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                print(f\"original SAM predict time/img: {end_time-start_time} s\")\n",
    "                inference_times.append(end_time-start_time)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(inference_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stophere2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture cap\n",
    "# # print(f\"Segmented images with threshold masks-on-negative.\")\n",
    "# generate_all_predictions = True\n",
    "\n",
    "# if generate_all_predictions:\n",
    "        \n",
    "#     dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "#     # print(dir_files)\n",
    "#     for file_ in dir_files:\n",
    "#         try:\n",
    "#             if '.png' in file_ and 'clahe' in file_:\n",
    "#                 mask_on_negative = mask_and_plot_image(OM_dir+file_.replace('.png', '.fits').replace('clahe_', ''))\n",
    "#                 SAM_predictor(OM_dir+file_, mask_on_negative)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             pass\n",
    "    \n",
    "#     # with open('cell_SAM_predict_with_threshold.txt', 'w') as f:\n",
    "#     #     f.write(str(cap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Generate annotation json file (COCO format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download Roboflow annotations, v5\n",
    "\n",
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "# project = rf.workspace(\"orij\").project(\"om_sky_images\")\n",
    "# dataset = project.version(5).download(\"coco-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    print(box[0], box[1], box[2],  box[3]) \n",
    "    w, h = box[2], box[3] \n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def numpy_to_list(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: numpy_to_list(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [numpy_to_list(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Initialize a dictionary to store all images and annotations\n",
    "categories = [{\"id\":0,\"name\":\"Artifacts-Sky-objects\"},\\\n",
    "            {\"id\":1,\"name\":\"other\"},\\\n",
    "            {\"id\":2,\"name\":\"ray\"},\\\n",
    "            {\"id\":3,\"name\":\"read-out-streak\"},\\\n",
    "            {\"id\":4,\"name\":\"smoke-ring\"},\\\n",
    "            {\"id\":5,\"name\":\"star\"},\\\n",
    "            {\"id\":6,\"name\":\"star-ghost\"},\\\n",
    "            {\"id\":7,\"name\":\"star-loop\"},\\\n",
    "            {\"id\":8,\"name\":\"star-ring\"}]\n",
    "\n",
    "# coco_style_annotations = {'categories': categories, 'images': [], 'annotations': []}\n",
    "coco_style_annotations = {'annotations': []}\n",
    "\n",
    "\n",
    "def get_SAM_annotations(IMAGE_PATH, mask_on_negative = None, output_mode=\"binary_mask\"):\n",
    "    \"\"\"\n",
    "    This function calls SAM (Segment Anything) and gets annotations for a given image.\n",
    "    Args:\n",
    "        IMAGE_PATH (str): The path to the image file.\n",
    "        remove_masks_on_negative (bool, optional): If True, masks on negative detections are removed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the SAM results, detections, and the annotated image.\n",
    "    \"\"\"\n",
    "    image_bgr = cv2.imread(IMAGE_PATH)\n",
    "    annotated_image, detections, sam_result = None, None, None\n",
    "\n",
    "    # try:\n",
    "    if True:\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        mask_generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode)\n",
    "        \n",
    "        sam_result = mask_generator.generate(image_rgb)\n",
    "        if mask_on_negative is not None:\n",
    "            sam_result = remove_masks(sam_result=sam_result,mask_on_negative=mask_on_negative, threshold=50)\n",
    "            \n",
    "        mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "        annotated_image = mask_annotator.annotate(scene=image_rgb.copy(), detections=detections)\n",
    "        # plt.imshow(annotated_image)\n",
    "        # plt.show()\n",
    "    # except Exception as e:\n",
    "    #     print(\"Exception:\\n\", IMAGE_PATH, e)\n",
    "    #     pass\n",
    "        \n",
    "    return sam_result, detections, annotated_image\n",
    "\n",
    "\n",
    "# Run SAM for .png files in the directory and create the annotation json file in ~COCO format\n",
    "\n",
    "def generate_json_file(input_dir, coco_style_annotations):\n",
    "    for file_ in os.listdir(input_dir):\n",
    "    # if True:\n",
    "        # file_ = 'S0720251301_L.png'\n",
    "        if \"png\" in file_:\n",
    "\n",
    "            print(file_)\n",
    "            mask_on_negative = mask_and_plot_image(input_dir+file_.replace('.png', '.fits'))\n",
    "            sam_result_i, detections_i, annotated_image_i = get_SAM_annotations(input_dir+file_, mask_on_negative.astype(int))\n",
    "            \n",
    "            sam_result_i = numpy_to_list(sam_result_i)\n",
    "            img = cv2.imread(os.path.join(input_dir, file_))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "            height, width, _ = img.shape\n",
    "            # print(k)\n",
    "            # plt.figure(figsize=(10,10))\n",
    "            # print(type(annotated_image_i))\n",
    "            # plt.imshow(annotated_image_i)\n",
    "            # # show_box(annotation['bbox'], plt.gca())\n",
    "            # # plt.axis('off')\n",
    "            # plt.show()\n",
    "            # coco_style_annotations['images'].append({\n",
    "            #             'id': file_.split('.')[0],\n",
    "            #             'license': 1,\n",
    "            #             'file_name': input_dir+file_,\n",
    "            #             'height':height, \n",
    "            #             'width': width,  \n",
    "            #             'date_captured': datetime.now().isoformat(), \n",
    "            #         })\n",
    "            k=0\n",
    "            for annotation in sam_result_i:\n",
    "                # xyhw = annotation['bbox']\n",
    "                # if (xyhw[2] >2 or xyhw[3] >2) and (xyhw[2]*1.0/height < 0.7 and xyhw[3]*1.0/width < 0.7):\n",
    "                if True:\n",
    "                    coco_style_annotations['annotations'].append({\n",
    "                            'id': f'{file_.split(\".\")[0]}_mask{k}',\n",
    "                            'image_id': file_.split('.')[0], \n",
    "                            'category_id': 0,  \n",
    "                            'segmentation': annotation['segmentation'],\n",
    "                            'area': annotation['area'],\n",
    "                            'bbox': annotation['bbox'],\n",
    "                            'iscrowd': 0,\n",
    "                        })\n",
    "                    k+=1\n",
    "    \n",
    "    with open('SAM_annotations_coco_style_v2.json', 'w') as f:\n",
    "    # with open('SAM_annotations_coco_style_img1.json', 'w') as f:\n",
    "        json.dump(coco_style_annotations, f)\n",
    "\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/'\n",
    "generate_json_file(input_dir, coco_style_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Generate annotation json file (VOC format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "upload_project = rf.workspace(\"orij\").project(\"xmm_om_images_v4-contrast-512\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and export annotations in VOC format to Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import my files\n",
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils, voc_annotate_and_Roboflow_export\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "reload(voc_annotate_and_Roboflow_export)\n",
    "\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "from voc_annotate_and_Roboflow_export import * # moved the files there\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/zscaled_512_rescaled_SAM_stats/'\n",
    "\n",
    "k=0\n",
    "if 1==1:\n",
    "    input_fits_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'\n",
    "    for file_ in os.listdir(input_dir):\n",
    "        # if '.png' in file_ and k<200:\n",
    "        if True:\n",
    "            mask_on_negative = mask_and_plot_image(input_fits_dir+file_.replace('.png', '.fits'), plot_=False)\n",
    "    \n",
    "            sam_result_i, detections_i, annotated_image_i = SAM_predictor(SamAutomaticMaskGenerator, sam, input_dir+file_, \n",
    "                                                                          mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "            if sam_result_i is not None and detections_i is not None and annotated_image_i is not None:\n",
    "                objects = []\n",
    "                for annotation in sam_result_i: # a mask over an image is a binary array with shape (img_h, img_w)\n",
    "                    polygon = binary_image_to_polygon(annotation['segmentation'])\n",
    "                    # plot_polygon(polygon[0], annotated_image_i) # to see the masks polygons\n",
    "                    objects.append({\n",
    "                        'name': 'star',\n",
    "                        'bbox': annotation['bbox'],\n",
    "                        'segmentations': polygon[0]\n",
    "                    })\n",
    "                    \n",
    "                create_annotation(filename=file_, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "                image_path = input_dir+file_\n",
    "                annotation_filename = \"/workspace/raid/OM_DeepLearning/XMM_OM_code/\"+file_.replace(\".png\", \".xml\")\n",
    "                \n",
    "                upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "                os.remove(annotation_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir(input_dir)[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export images and annotations in VOC format to Roboflow (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "# project = rf.workspace(\"orij\").project(\"xmm_om_images-contrast-512-v5\")\n",
    "# dataset = project.version(1).download(\"voc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Image upload complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "upload_project = rf.workspace(\"orij\").project(\"xmm_om_images-contrast-512-v5\")\n",
    "\n",
    "dataset_images_folder = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/zscaled_512_rescaled_SAM_stats/'\n",
    "annotations_voc_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images-contrast-512-v5-1/train/'\n",
    "annotations_ = []\n",
    "\n",
    "for annot in os.listdir(annotations_voc_dir):\n",
    "    if annot.endswith('.xml'):\n",
    "        annotations_.append(annot)\n",
    "\n",
    "for image_name in os.listdir(dataset_images_folder):\n",
    "    image_path = os.path.join(dataset_images_folder, image_name)\n",
    "    if os.path.isfile(image_path):\n",
    "        annotations_voc_filename = [annotation for annotation in annotations_ if annotation.startswith(image_path.split('/')[-1].replace('.', '_'))]\n",
    "        upload_project.upload(image_path, annotations_voc_dir+annotations_voc_filename[0], overwrite=True)\n",
    "\n",
    "print(\"Image upload complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi3C4uDWo10h"
   },
   "source": [
    "## AutomaticMask Generation\n",
    "\n",
    "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0Pm0RYArgm9"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# IMAGE_NAME = \"/workspace/raid/OM_DeepLearning/XMM_OM_code/gaussian_.png\"\n",
    "# IMAGE_PATH = os.path.join(OM_dir,IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdgL88fUuelk"
   },
   "source": [
    "### Generate masks with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!The predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # usually the pip install \"opencv-python-headless<4.3\" solves this problem:\n",
    "# # AttributeError: partially initialized module 'cv2' has no attribute '_registerMatType' (most likely due to a circular import)\n",
    "# # !pip install \"opencv-python-headless<4.3\"\n",
    "# # !pip install jupyter-bbox-widget\n",
    "# import cv2\n",
    "# import supervision as sv\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# # !pip install astropy\n",
    "# import astropy\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from astropy.io import fits\n",
    "# from scipy.interpolate import interp1d\n",
    "# from astropy.visualization import ZScaleInterval, ImageNormalize\n",
    "\n",
    "# def SAM_predictor(IMAGE_PATH, remove_masks_on_negative = False):\n",
    "    \n",
    "#     image_bgr = cv2.imread(IMAGE_PATH)\n",
    "#     # print(IMAGE_PATH)\n",
    "#     annotated_image = None\n",
    "#     try:\n",
    "#         image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "#         # image_rgb = image_bgr\n",
    "    \n",
    "#         sam_result = mask_generator.generate(image_rgb)\n",
    "#         output_file = IMAGE_PATH.replace(\"scaled_raw\", \"segmented_SAM\").replace(\".png\", \"_segmented.png\")\n",
    "        \n",
    "#         if remove_masks_on_negative:\n",
    "#             sam_to_keep = []\n",
    "#             for segm_index in range(len(sam_result)):\n",
    "#                 if (np.any((sam_result[segm_index]['segmentation']==1) & (mask_on_negative==1)))==0:\n",
    "#                     sam_to_keep.append(sam_result[segm_index])\n",
    "#             sam_result = sam_to_keep\n",
    "#             output_file = IMAGE_PATH.replace(\"scaled_raw\", \"segmented_SAM\").replace(\".png\", \"_segmented_removed_negative.png\")\n",
    "            \n",
    "#         mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "#         detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "#         annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "#         image = Image.fromarray(annotated_image)\n",
    "#         image.save(output_file)\n",
    "    \n",
    "#         sv.plot_images_grid(\n",
    "#             images=[image_bgr, annotated_image],\n",
    "#             grid_size=(1, 2),\n",
    "#             titles=['source image', 'segmented image']\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(\"Exception:\\n\", e)\n",
    "#         pass\n",
    "        \n",
    "#     return image_bgr, annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM_predictor(\"/workspace/raid/OM_DeepLearning/colored_atrous_wavelet_decomposition.png\")\n",
    "# SAM_predictor(IMAGE_NAME)\n",
    "\n",
    "# for file_ in os.listdir(OM_dir+\"scaled_raw/\"):\n",
    "#     if \"png\" in file_:\n",
    "#         SAM_predictor(OM_dir+\"scaled_raw/\"+file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def save_4(image1, image2, image3, image4):\n",
    "#     fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    \n",
    "#     # Plot each image in the corresponding subplot\n",
    "#     axs[0, 0].imshow(image1, cmap=\"gray\") \n",
    "#     axs[0, 0].axis('off')\n",
    "#     axs[0, 0].set_title('source image w/o distribution')\n",
    "    \n",
    "#     axs[0, 1].imshow(image2, cmap=\"gray\")\n",
    "#     axs[0, 1].axis('off')\n",
    "#     axs[0, 1].set_title('modified image w/o distribution')\n",
    "    \n",
    "#     axs[1, 0].imshow(image3, cmap=\"gray\")\n",
    "#     axs[1, 0].axis('off')\n",
    "#     axs[1, 0].set_title('source image w/ distribution')\n",
    "    \n",
    "#     axs[1, 1].imshow(image4, cmap=\"gray\")\n",
    "#     axs[1, 1].axis('off')\n",
    "#     axs[1, 1].set_title('modified image w/ distribution')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # Save the figure\n",
    "#     plt.savefig('2x2_images_grid.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "#     # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# image_bgr, annotated_image = SAM_predictor(\"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/S0112231601_V.png\")\n",
    "# image_bgr_filled, annotated_image_filled = SAM_predictor(\"/workspace/raid/OM_DeepLearning/XMM_OM_code/S0112231601_V_gaussian.png\")\n",
    "\n",
    "# sv.plot_images_grid(\n",
    "#     images=[image_bgr, annotated_image, image_bgr_filled, annotated_image_filled],\n",
    "#     grid_size=(2, 2),\n",
    "#     titles=['source image', 'segmented image', 'source image w/ distribution', 'segmented image w/ distribution']\n",
    "# )\n",
    "\n",
    "# save_4(image_bgr, annotated_image, image_bgr_filled, annotated_image_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # image_bgr, annotated_image = SAM_predictor(\"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/S0655570201_L.png\")\n",
    "# # image_bgr_filled, annotated_image_filled = SAM_predictor(\"/workspace/raid/OM_DeepLearning/XMM_OM_code/S0655570201_L_gaussian.png\")\n",
    "\n",
    "# sv.plot_images_grid(\n",
    "#     images=[image_bgr, annotated_image, image_bgr_filled, annotated_image_filled],\n",
    "#     grid_size=(2, 2),\n",
    "#     titles=['source image', 'segmented image', 'source image w/ distribution', 'segmented image w/ distribution']\n",
    "# )\n",
    "# save_4(image_bgr, annotated_image, image_bgr_filled, annotated_image_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUNhAvdPjZ-Y"
   },
   "source": [
    "### Output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxO265XOymA2"
   },
   "source": [
    "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
    "\n",
    "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
    "* `area` - `[int]` - the area of the mask in pixels\n",
    "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
    "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
    "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
    "* `stability_score` - `[float]` - an additional measure of mask quality\n",
    "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsdFDDQnjhkP"
   },
   "source": [
    "### Interaction with segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CS_WhY60XMNL",
    "outputId": "7affc2c7-87c3-4981-a5a9-c7e135622d32"
   },
   "outputs": [],
   "source": [
    "# masks = [\n",
    "#     mask['segmentation']\n",
    "#     for mask\n",
    "#     in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
    "# ]\n",
    "\n",
    "# sv.plot_images_grid(\n",
    "#     images=masks,\n",
    "#     grid_size=(9, int(len(masks) / 8)),\n",
    "#     size=(16, 16)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
