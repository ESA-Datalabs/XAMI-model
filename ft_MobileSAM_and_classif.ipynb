{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e370c6",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba636f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "import json\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.cuda.set_device(6) # ‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è\n",
    "\n",
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "import predictor_utils\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc164c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images_v4-contrast-512-5-7/train/'\n",
    "json_file_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images_v4-contrast-512-5-7/train/_annotations.coco.json'\n",
    "\n",
    "# COCO segmentation bboxes are in XYWH format\n",
    "with open(json_file_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "ground_truth_masks, bbox_coords, classes, class_categories = get_coords_and_masks_from_json(input_dir, data) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c7d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_images = [input_dir+img_data['file_name'] for img_data in data['images']]\n",
    "\n",
    "image_paths_no_augm = []\n",
    "\n",
    "for im_path in raw_images:\n",
    "    has_annots = 0\n",
    "    for k, v in bbox_coords.items():\n",
    "        if im_path.split('/')[-1] in k:\n",
    "            has_annots = 1\n",
    "            \n",
    "    if has_annots==0:\n",
    "                print(\"Img doesn't have annotations after filtering.\")\n",
    "    else:\n",
    "        image_paths_no_augm.append(im_path)\n",
    "\n",
    "len(image_paths_no_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a95a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before', len(data['annotations']), 'after', len(ground_truth_masks.values()), len(bbox_coords.values()), len(classes.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f99e6c",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "This algorithm performs augmentations and updates the negative masks in the case of a geometric transformations. Otherwise, it masks the result of a noise/blur transformation given the mask of the initial image. \n",
    "\n",
    "**!! For augmentation, the bboxes are expected to be in the XYWH format, not XYXY format (used by SAM). However, the SAM AMG generated results are in the XYWH format (converted from XYXY).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob(f'{input_dir}/*augm*')\n",
    "\n",
    "for file in files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e02efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/examples/showcase/\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.color import label2rgb\n",
    "import albumentations as A\n",
    "import random\n",
    "import math\n",
    "\n",
    "# load utils\n",
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "def enlarge_bbox(bbox, delta, image_size):\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min = max(0, math.floor(x_min))\n",
    "    y_min = max(0,  math.floor(y_min))\n",
    "    \n",
    "    w = min(image_size[1], math.ceil(w))\n",
    "    h = min(image_size[0], math.ceil(h))\n",
    "    return np.array([x_min, y_min, w, h])\n",
    "\n",
    "geometrical_augmentations = A.Compose([\n",
    "    A.Flip(),\n",
    "    A.RandomRotate90(),\n",
    "    A.RandomSizedCrop((512 - 50, 512 - 50), 512, 512),\n",
    "], bbox_params={'format':'coco', 'min_area': 0.1, 'min_visibility': 0.3, 'label_fields': ['category_id']}, p=1)\n",
    "\n",
    "noise_blur_augmentations = A.Compose([\n",
    "    A.GaussianBlur(blur_limit=(3, 3), p=1),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
    "    A.ISONoise(p=0.8),\n",
    "], bbox_params={'format':'coco', 'min_area': 0.1, 'min_visibility': 0.3, 'label_fields': ['category_id']}, p=1)\n",
    "\n",
    "    \n",
    "image_paths = []\n",
    "for image_path in image_paths_no_augm:\n",
    "    image_paths.append(image_path)\n",
    "    image_ = cv2.imread(image_path)\n",
    "    image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "    image_size = (image_.shape[0], image_.shape[1])\n",
    "    masks = [value_i for key_i, value_i in ground_truth_masks.items() if image_path.split('/')[-1] in key_i]\n",
    "    bboxes_ = [enlarge_bbox(np.array([value_i[0], math.floor(value_i[1]),  math.floor(value_i[2] - value_i[0]), \\\n",
    "                                    math.floor(value_i[3] - value_i[1])]), 2, image_size) \\\n",
    "                                    for key_i, value_i in bbox_coords.items() if image_path.split('/')[-1] in key_i]   \n",
    "    label_ids = [classes[key_i] for key_i, value_i in bbox_coords.items() if image_path.split('/')[-1] in key_i]\n",
    "\n",
    "    image_size = (image_.shape[0], image_.shape[1])\n",
    "    for bbox in bboxes_:\n",
    "        if bbox[2] <= 1 or bbox[3] <= 1:\n",
    "            print(\"Invalid bbox detected:\", bbox)\n",
    "        \n",
    "    if len(bboxes_) != len(masks):\n",
    "        print('len(bboxes_) != len(masks)', len(bboxes_), len(masks))\n",
    "        continue\n",
    "\n",
    "    if len(bboxes_) != len(label_ids):\n",
    "        print('len(bboxes_) != len(label_ids)', len(bboxes_), len(label_ids))\n",
    "        continue\n",
    "\n",
    "    if len(bboxes_) == 0:\n",
    "        print(image_path)\n",
    "        \n",
    "    img_negative_mask = (image_>0).astype(int)\n",
    "    \n",
    "    # the geometrical augm doesn't change the shape of the image\n",
    "    augmented1 = augment_and_show(geometrical_augmentations, image_, masks, bboxes_, label_ids, class_categories, show_=False)\n",
    "    new_image_negative_mask = (augmented1['image']>0).astype(int) # to mask the transform which is derived from the geometric transform\n",
    "    augmented3 = augment_and_show(noise_blur_augmentations, image_, masks, bboxes_, label_ids, class_categories, show_=False)\n",
    "\n",
    "    # mask the transform using the image negative mask\n",
    "    augmented3['image'] = augmented3['image'] * img_negative_mask\n",
    "        \n",
    "    new_filename1 = image_path.replace('.'+image_path.split('.')[-1], '_augm1.jpg')\n",
    "    new_filename3 = image_path.replace('.'+image_path.split('.')[-1], '_augm3.jpg')\n",
    "\n",
    "    update_dataset_with_augms(augmented1, new_filename1, bbox_coords, ground_truth_masks, image_paths, classes)\n",
    "    update_dataset_with_augms(augmented3, new_filename3, bbox_coords, ground_truth_masks, image_paths, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c020999",
   "metadata": {},
   "outputs": [],
   "source": [
    "for im_path in image_paths:\n",
    "    has_annots = 0\n",
    "    for k, v in bbox_coords.items():\n",
    "        if im_path.split('/')[-1] in k:\n",
    "            has_annots = 1\n",
    "            \n",
    "    if has_annots==0:\n",
    "           image_paths.remove(im_path)\n",
    "\n",
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_i, value_i in bbox_coords.items():\n",
    "     if image_path.split('/')[-1] in key_i:\n",
    "        x1, y1, x2, y2 = value_i\n",
    "        if x2 - x1<2 or y2-y1 <2:\n",
    "            print(value_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in image_paths:\n",
    "    image_ = cv2.imread(image_path)\n",
    "    image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "    if image_.shape [1]< 512:\n",
    "        print(image_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bede8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6576c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    image_array = np.array(image) / 255.0\n",
    "\n",
    "    mean = image_array.mean()\n",
    "    std = image_array.std()\n",
    "\n",
    "    means.append(mean)\n",
    "    stds.append(std)\n",
    "\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c21abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(means), np.std(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(stds),np.std(stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b099d96",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import math\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "def split_list(input_list, percentages):\n",
    "    size = len(input_list)\n",
    "    idx = 0\n",
    "    output = []\n",
    "    for percentage in percentages:\n",
    "        chunk_size = round(percentage * size)\n",
    "        chunk = input_list[idx : idx + chunk_size]\n",
    "        output.append(chunk)\n",
    "        idx += chunk_size\n",
    "    return output\n",
    "\n",
    "def create_dataset(image_paths, ground_truth_masks, bbox_coords):\n",
    "        \n",
    "    d_gt_masks, d_bboxes = {}, {}\n",
    "    for img_path in image_paths:\n",
    "        id = img_path.split('/')[-1]\n",
    "\n",
    "        # binary mask to RLE \n",
    "        d_gt_masks.update({mask_id:maskUtils.encode(np.asfortranarray(mask_array)) for mask_id, mask_array in ground_truth_masks.items() if mask_id.startswith(id)})\n",
    "        d_bboxes.update({bbox_id:bbox for bbox_id, bbox in bbox_coords.items() if bbox_id.startswith(id)}) \n",
    "\n",
    "    return d_gt_masks, d_bboxes\n",
    "    \n",
    "training_size, val_size, test_size = (0.7, 0.2, 0.1)\n",
    "splits = split_list(image_paths, [training_size, val_size, test_size])\n",
    "training_image_paths, val_image_paths, test_image_paths = splits[0], splits[1], splits[2]\n",
    "\n",
    "train_gt_masks, train_bboxes = create_dataset(training_image_paths, ground_truth_masks, bbox_coords)\n",
    "test_gt_masks, test_bboxes = create_dataset(test_image_paths, ground_truth_masks, bbox_coords)\n",
    "val_gt_masks, val_bboxes = create_dataset(val_image_paths, ground_truth_masks, bbox_coords)\n",
    "\n",
    "del ground_truth_masks, bbox_coords, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502967eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train:', len(training_image_paths), 'test:', len(test_image_paths), 'val:', len(val_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7d309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for one_path in training_image_paths[:4]:\n",
    "    image_id2 = one_path.split('/')[-1]\n",
    "    print(image_id2)\n",
    "    image_masks_ids = [key for key in train_gt_masks.keys() if key.startswith(image_id2)]\n",
    "    image_ = cv2.imread(one_path)\n",
    "    image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(image_)\n",
    "    for name in image_masks_ids:\n",
    "            show_box(train_bboxes[name], plt.gca())\n",
    "            show_mask(maskUtils.decode(train_gt_masks[name]), plt.gca())\n",
    "    plt.axis('off')\n",
    "    # plt.savefig(f'example{image_id2}.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81c73f",
   "metadata": {},
   "source": [
    "## üöÄ Prepare Mobile SAM Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ea05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9462692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/')\n",
    "from ft_mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/weights/mobile_sam.pt\"\n",
    "# mobile_sam_checkpoint = \"./ft_mobile_sam_final.pth\"\n",
    "device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "\n",
    "if use_wandb:\n",
    "    from datetime import datetime\n",
    "    # !pip install wandb\n",
    "    # !wandb login --relogin\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=\"OM_AI_v1\", name=f\"ft_MobileSAM {datetime.now()}\")\n",
    "\n",
    "    wandb.watch(mobile_sam_model, log='all', log_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6844a",
   "metadata": {},
   "source": [
    "## Convert the input images into a format SAM's internal functions expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3831979",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import segment_anything\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "from importlib import reload\n",
    "import astronomy_utils\n",
    "reload(astronomy_utils)\n",
    "from astronomy_utils import *\n",
    "\n",
    "transform = ResizeLongestSide(mobile_sam_model.image_encoder.img_size)\n",
    "\n",
    "def transform_image(image, k):\n",
    "       \n",
    "        # sets a specific mean for each image\n",
    "        image_T = np.transpose(image, (2, 1, 0))\n",
    "        mean_ = np.mean(image_T[image_T>0])\n",
    "        std_ = np.std(image_T[image_T>0]) \n",
    "        pixel_mean = torch.as_tensor([mean_, mean_, mean_], dtype=torch.float, device=device)\n",
    "        pixel_std = torch.as_tensor([std_, std_, std_], dtype=torch.float, device=device)\n",
    "\n",
    "        mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).unsqueeze(-1).unsqueeze(-1), False) # not in SAM\n",
    "        mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).unsqueeze(-1).unsqueeze(-1), False) # not in SAM\n",
    "\n",
    "        transformed_data = {}\n",
    "        negative_mask = np.where(image > 0, True, False)\n",
    "        negative_mask = torch.from_numpy(negative_mask)  \n",
    "        negative_mask = negative_mask.permute(2, 0, 1)\n",
    "        negative_mask = resize(negative_mask, [1024, 1024], antialias=True) \n",
    "        negative_mask = negative_mask.unsqueeze(0)\n",
    "        # scales the image to 1024x1024 by longest side \n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "        \n",
    "        # normalization and padding\n",
    "        input_image = mobile_sam_model.preprocess(transformed_image)\n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(transformed_image.shape[-2:])\n",
    "        input_image[~negative_mask] = 0\n",
    "        transformed_data['image'] = input_image.clone() \n",
    "        transformed_data['input_size'] = input_size \n",
    "        transformed_data['image_id'] = k\n",
    "        transformed_data['original_image_size'] = original_image_size\n",
    "    \n",
    "        return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, bbox_coords, gt_masks, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_paths[idx].split(\"/\")[-1]\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            return self.transform(image, img_id)\n",
    "        else:\n",
    "            print(\"‚ùóÔ∏èNo transform‚ùóÔ∏è\")\n",
    "            return image\n",
    "        \n",
    "batch_size = 12\n",
    "\n",
    "train_set = ImageDataset(training_image_paths[:12], train_bboxes, train_gt_masks, transform_image) \n",
    "val_set = ImageDataset(val_image_paths[:12], val_bboxes, val_gt_masks, transform_image) \n",
    "test_set = ImageDataset(test_image_paths, test_bboxes, test_gt_masks, transform_image) \n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7eece7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "lr=4e-3\n",
    "wd=0.0\n",
    "\n",
    "parameters_to_optimize = [param for param in mobile_sam_model.mask_decoder.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(parameters_to_optimize, lr=lr, weight_decay=wd) #betas=(0.9, 0.999))\n",
    "\n",
    "def dice_loss_per_mask_pair(pred, target, mask_areas, negative_mask=None, smooth=1):\n",
    "    \n",
    "    assert pred.size() == target.size(), \"Prediction and target must have the same shape\"\n",
    "    batch_size, height, width = pred.size()\n",
    "    total_masks_area = target.sum()\n",
    "    dice_loss = 0.0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        pred_mask = pred[i].contiguous()\n",
    "        target_mask = target[i].contiguous()\n",
    "        # fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "        # # Assuming masks are 2D tensors, convert them to numpy for plotting\n",
    "        # pred_mask_np = pred_mask.detach().cpu().numpy() if pred_mask.is_cuda else pred_mask.numpy()\n",
    "        # target_mask_np = target_mask.detach().cpu().numpy() if target_mask.is_cuda else target_mask.numpy()\n",
    "        \n",
    "        # ax[0].imshow(pred_mask_np, cmap='gray')\n",
    "        # ax[0].set_title('Predicted Mask')\n",
    "        # ax[0].axis('off')\n",
    "        \n",
    "        # ax[1].imshow(target_mask_np, cmap='gray')\n",
    "        # ax[1].set_title('Target Mask')\n",
    "        # ax[1].axis('off')\n",
    "        \n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        \n",
    "\n",
    "        # if negAative_mask is not None:\n",
    "        #     neg_mask = negative_mask[i].bool()\n",
    "        #     pred_mask = pred_mask * neg_mask\n",
    "        #     target_mask = target_mask * neg_mask\n",
    "        \n",
    "        intersection = (pred_mask * target_mask).sum()\n",
    "        total = pred_mask.sum() + target_mask.sum()\n",
    "        mask_loss = (2. * intersection + smooth) / (total + smooth)\n",
    "        # dice_loss += ((1 - mask_loss) * mask_areas[i] / total_masks_area) # weighted loss given mask size\n",
    "        dice_loss += (1 - mask_loss)\n",
    "    \n",
    "    dice_loss /= batch_size\n",
    "    \n",
    "    return dice_loss\n",
    "\n",
    "def focal_loss_per_mask_pair(inputs, targets, mask_areas, alpha=0.8, gamma=2):\n",
    "    \n",
    "    assert inputs.size() == targets.size(), \"Inputs and targets must have the same shape\"\n",
    "    batch_size, height, width = inputs.size()\n",
    "    total_masks_area = targets.sum()\n",
    "    focal_loss = 0.0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        input_mask = inputs[i].unsqueeze(0)\n",
    "        target_mask = targets[i].unsqueeze(0)\n",
    "        BCE = F.binary_cross_entropy_with_logits(input_mask, target_mask, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        mask_focal_loss = alpha * (1 - BCE_EXP)**gamma * BCE # * mask_areas[i]/total_masks_area # weighted loss given mask size\n",
    "        focal_loss += mask_focal_loss\n",
    "    \n",
    "    focal_loss /= batch_size\n",
    "    \n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0efe1",
   "metadata": {},
   "source": [
    "## Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1c0ea",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    if 'mask_decoder' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303be42",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_requires_grad(model, show=True):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and show:\n",
    "            print(\"‚úÖ Param\", name, \" requires grad.\")\n",
    "        elif param.requires_grad == False:\n",
    "            print(\"‚ùå Param\", name, \" doesn't require grad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f929b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"üöÄ The model has {sum(p.numel() for p in mobile_sam_model.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "check_requires_grad(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef22e7",
   "metadata": {},
   "source": [
    "## Run fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d038c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_gt_masks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196bb95",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import predictor_utils\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "\n",
    "def validate_model_AMG(ft_mobile_sam_model, mobile_sam_model_orig):\n",
    "    validation_loss = []\n",
    "    ft_validation_loss = []\n",
    "    for val_img_path in val_image_paths[:5]:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            print('Original MobileSAM')\n",
    "            annotated_image_orig_mobile_sam, annotated_image_orig_mobile_sam_loss = amg_predict(mobile_sam_model_orig, orig_mobile_SamAutomaticMaskGenerator, \\\n",
    "                                                       val_gt_masks, 'ft_MobileSAM', val_img_path, mask_on_negative=False, show_plot=True)\n",
    "\n",
    "            print('Fine-tuned MobileSAM')\n",
    "            annotated_img, loss = amg_predict(mobile_sam_model, SamAutomaticMaskGenerator, val_gt_masks, 'ft_MobileSAM', \\\n",
    "                                              val_img_path, mask_on_negative=True, show_plot=True)\n",
    "            \n",
    "            ft_validation_loss.append(annotated_image_orig_mobile_sam_loss)\n",
    "            validation_loss.append(loss)\n",
    "\n",
    "    return np.mean(ft_validation_loss), np.mean(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a552a9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "loss_scaling_factor = 1e5\n",
    "\n",
    "def one_image_predict(image_masks, gt_masks, gt_bboxes, image_embedding, original_image_size, input_size, negative_mask, input_image, model):\n",
    "    image_loss=[]\n",
    "    ce_loss = 0.0\n",
    "    boxes, masks, coords, coords_labels = [], [], [], []\n",
    "    gt_rle_to_masks, mask_areas = [], []\n",
    "\n",
    "    for k in image_masks: \n",
    "        prompt_box = np.array(gt_bboxes[k])\n",
    "        box = predictor.transform.apply_boxes(prompt_box, original_image_size)\n",
    "        box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "        boxes.append(box_torch)\n",
    "\n",
    "        # process masks\n",
    "        rle_to_mask = maskUtils.decode(gt_masks[k])\n",
    "        gt_rle_to_masks.append(torch.from_numpy(rle_to_mask).to(device))\n",
    "        mask_input_torch = torch.as_tensor(rle_to_mask, dtype=torch.float, device=predictor.device).unsqueeze(0)#.unsqueeze(0)\n",
    "        mask_input_torch = F.interpolate(mask_input_torch.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False)\n",
    "        masks.append(mask_input_torch.squeeze(0))\n",
    "        mask_areas.append(np.sum(rle_to_mask))\n",
    "\n",
    "        # process coords and labels\n",
    "        x_min, y_min, x_max, y_max = gt_bboxes[k]\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "        point_coords = np.array([(gt_bboxes[k][2]+gt_bboxes[k][0])/2.0, (gt_bboxes[k][3]+gt_bboxes[k][1])/2.0])\n",
    "        point_labels = np.array([1])\n",
    "        point_coords = predictor.transform.apply_coords(point_coords, original_image_size)\n",
    "        coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=predictor.device).unsqueeze(0)\n",
    "        labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=predictor.device)\n",
    "        coords.append(coords_torch)\n",
    "        coords_labels.append(labels_torch)\n",
    "\n",
    "    boxes = torch.stack(boxes, dim=0)\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "    coords = torch.stack(coords, dim=0)\n",
    "    coords_labels = torch.stack(coords_labels, dim=0)\n",
    "    points = (coords, coords_labels)\n",
    "    gt_rle_to_masks = torch.stack(gt_rle_to_masks, dim=0)\n",
    "    \n",
    "    sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "      points=None,\n",
    "      boxes=boxes, #None, \n",
    "      masks=None, #None,\n",
    "    )\n",
    "\n",
    "    del box_torch, coords_torch, labels_torch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    low_res_masks, iou_predictions = model.mask_decoder(\n",
    "    image_embeddings=image_embedding,\n",
    "    image_pe=model.prompt_encoder.get_dense_pe(),\n",
    "    sparse_prompt_embeddings=sparse_embeddings,\n",
    "    dense_prompt_embeddings=dense_embeddings,\n",
    "    multimask_output=False, # True value works better for ambiguous prompts (single points)\n",
    "    )\n",
    "\n",
    "    # plt.imshow(low_res_masks[0][0].detach().cpu().numpy())\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "    \n",
    "    pred_masks = model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "    threshold_mask = torch.sigmoid(pred_masks - model.mask_threshold)\n",
    "    gt_threshold_mask = torch.as_tensor(gt_rle_to_masks>0.5, dtype=torch.float32) \n",
    "    numpy_gt_threshold_mask = gt_threshold_mask.contiguous().detach().cpu().numpy()\n",
    "\n",
    "    mask_result = torch.squeeze(threshold_mask, dim=1).detach().cpu().numpy()\n",
    "\n",
    "    # compute weighted dice loss (smaller weights on smaller objects)\n",
    "    focal = focal_loss_per_mask_pair(torch.squeeze(pred_masks, dim=1), gt_threshold_mask, mask_areas)\n",
    "    dice = dice_loss_per_mask_pair(torch.squeeze(threshold_mask, dim=1), gt_threshold_mask, mask_areas, negative_mask) \n",
    "    image_loss.append(20 * focal + dice ) # used in SAM paper\n",
    "\n",
    "    del threshold_mask \n",
    "    del numpy_gt_threshold_mask \n",
    "    del low_res_masks, iou_predictions \n",
    "    del pred_masks, gt_threshold_mask\n",
    "    del rle_to_mask\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    image_loss = torch.stack(image_loss)\n",
    "    image_loss = torch.mean(image_loss) * loss_scaling_factor\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(40, 20))\n",
    "    axs[0].imshow(input_image) \n",
    "    axs[0].set_title(f'{k.split(\".\")[0]}', fontsize=40)\n",
    "    \n",
    "    axs[1].imshow(input_image)\n",
    "    show_masks(mask_result, axs[1], random_color=False)\n",
    "    axs[1].set_title('Predicted masks', fontsize=40)\n",
    "    \n",
    "    axs[2].imshow(input_image) \n",
    "    show_masks(gt_rle_to_masks.detach().cpu().numpy(), axs[2], random_color=False)\n",
    "    axs[2].set_title('Ground truth masks', fontsize=40)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return image_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b5613",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_validate_step(model, dataloader, gt_masks, gt_bboxes, device, optimizer=None, mode='train'):\n",
    "    assert mode in ['train', 'validate'], \"Mode must be 'train' or 'validate'\"\n",
    "    \n",
    "    losses = []\n",
    "    for inputs in tqdm(dataloader):\n",
    "        batch_loss = 0.0\n",
    "        batch_size = len(inputs['image'])\n",
    "        for i in range(batch_size):\n",
    "            image_masks = [k for k in gt_masks.keys() if k.startswith(inputs['image_id'][i])]\n",
    "            idd = inputs['image_id'][i]\n",
    "            input_image = torch.as_tensor(inputs['image'][i], dtype=torch.float, device=predictor.device) # (B, C, 1024, 1024)\n",
    "         \n",
    "            image = cv2.imread(input_dir+inputs['image_id'][i])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            original_image_size = image.shape[:-1]\n",
    "            input_size = (1024, 1024)\n",
    "\n",
    "            # IMAGE ENCODER\n",
    "            image_embedding = model.image_encoder(input_image)\n",
    "               \n",
    "            # negative_mask has the size of the image\n",
    "            negative_mask = np.where(image>0, True, False)\n",
    "            negative_mask = torch.from_numpy(negative_mask)  \n",
    "            negative_mask = negative_mask.permute(2, 0, 1)\n",
    "            negative_mask = negative_mask[0]\n",
    "            negative_mask = negative_mask.unsqueeze(0).unsqueeze(0)\n",
    "            negative_mask = negative_mask.to(device)\n",
    "            \n",
    "            # RUN PREDICTION ON IMAGE\n",
    "            if len(image_masks)>0:\n",
    "                batch_loss += (one_image_predict(image_masks, gt_masks, gt_bboxes, image_embedding, \n",
    "                                                 original_image_size, input_size, negative_mask, image, model)) \n",
    "            else:\n",
    "                print(\"Image with no annotations‚ùóÔ∏è\", idd)\n",
    "        if mode == 'train':\n",
    "            # for name, parameter in model.named_parameters():\n",
    "            #     if parameter.grad is not None: \n",
    "            #         grad_norm = parameter.grad.norm()\n",
    "            #         if grad_norm < 1e-8: \n",
    "            #             print(f'‚ùóÔ∏èLayer {name} has vanishing gradients: {grad_norm}')\n",
    "            # for name, parameter in model.named_parameters():\n",
    "            # \tif parameter.grad is not None:\n",
    "            # \t\tprint(name, parameter.grad.abs().mean())\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            del image_embedding, negative_mask, input_image, idd, image\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        losses.append(batch_loss.item()/batch_size/loss_scaling_factor)\n",
    "            \n",
    "    return np.mean(losses), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d627591",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee3ef2",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "num_epochs = 10\n",
    "best_valid_loss = float('inf')\n",
    "n_epochs_stop = 5 + num_epochs/10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # train\n",
    "    mobile_sam_model.train()\n",
    "    epoch_loss, model = train_validate_step(mobile_sam_model, train_dataloader, train_gt_masks, train_bboxes, device, optimizer, mode='train')\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # validate\n",
    "    mobile_sam_model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss, model = train_validate_step(mobile_sam_model, val_dataloader, val_gt_masks, val_bboxes, device, optimizer, mode='validate')\n",
    "        valid_losses.append(epoch_val_loss)\n",
    "        \n",
    "        if epoch_val_loss < best_valid_loss:\n",
    "            best_valid_loss = epoch_val_loss\n",
    "            best_model = model\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print(\"Early stopping initiated.\")\n",
    "                early_stop = True\n",
    "                break\n",
    "\n",
    "    # # # Logging\n",
    "    # if use_wandb:\n",
    "    #     wandb.log({'epoch training loss': epoch_loss, 'epoch validation loss': epoch_val_loss})\n",
    "\n",
    "    print(f'EPOCH: {epoch}. Training loss: {epoch_loss}')\n",
    "    print(f'EPOCH: {epoch}. Validation loss: {epoch_val_loss}.')\n",
    "\n",
    "torch.save(best_model.state_dict(), f'ft_mobile_sam_final.pth')\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.run.summary[\"batch_size\"] = batch_size\n",
    "    wandb.run.summary[\"num_epochs\"] = num_epochs\n",
    "    wandb.run.summary[\"learning rate\"] = lr\n",
    "    wandb.run.summary[\"weight_decay\"] = wd\n",
    "    wandb.run.summary[\"loss_scaling_factor\"] = loss_scaling_factor\n",
    "    wandb.run.summary[\"# train images\"] = len(train_dataloader.dataset)\n",
    "    wandb.run.summary[\"# validation images\"] = len(val_dataloader.dataset)\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('epoch train losses:', train_losses)\n",
    "print('epoch val losses:', valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c35856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "check_orig = False\n",
    "if check_orig:\n",
    "    sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/')\n",
    "    import mobile_sam\n",
    "    from mobile_sam import sam_model_registry as orig_mobile_sam_model_registry, \\\n",
    "                           SamAutomaticMaskGenerator as orig_mobile_SamAutomaticMaskGenerator, \\\n",
    "                           SamPredictor as orig_mobile_SamPredictor\n",
    "    \n",
    "    # orig_mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/ft_mobile_sam_final.pth\"\n",
    "    orig_mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-master/weights/mobile_sam.pt\"\n",
    "    orig_mobile_sam_model = orig_mobile_sam_model_registry[\"vit_t\"](checkpoint=orig_mobile_sam_checkpoint)\n",
    "    orig_mobile_sam_model.to(device);\n",
    "    orig_mobile_sam_model.eval();\n",
    "    \n",
    "    mobile_sam_model.eval();\n",
    "    predictor = orig_mobile_SamPredictor(mobile_sam_model)\n",
    "    \n",
    "    valid_losses = []\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss, _ = train_validate_step(orig_mobile_sam_model, val_dataloader, val_gt_masks, val_bboxes, device, optimizer, mode='validate')\n",
    "        valid_losses.append(epoch_val_loss)\n",
    "    print(f'EPOCH: {epoch}. Validation loss: {epoch_val_loss}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "# display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d048d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import graphviz\n",
    "\n",
    "# json_file_path = './graph_0_summary_598a07b5e588b04c9de6.graph.json'\n",
    "\n",
    "# with open(json_file_path, 'r') as file:\n",
    "#     model_data = json.load(file)\n",
    "\n",
    "# dot = graphviz.Digraph(comment='Model Visualization')\n",
    "\n",
    "# for node in model_data['nodes']:\n",
    "#     label = f\"{node['name']}\\n{node['class_name']}\"\n",
    "#     dot.node(str(node['id']), label=label)\n",
    "# dot.render('output', view=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63fd80",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# plt.plot(list(range(len(losses))), losses)\n",
    "# plt.title('Mean epoch loss \\n mask with sigmoid')\n",
    "# plt.xlabel('Epoch Number')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.savefig('loss_mask_sigmoid_training.png')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
