{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47421b3-f94e-4367-a703-1b50b6cdfee1",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29c832",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "import json\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "# Ensure deterministic behavior (cannot control everything though)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.cuda.set_device(6) # ‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è\n",
    "\n",
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "import predictor_utils\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de14702-8b4c-4de0-8ae6-e33dc88481b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "# project = rf.workspace(\"orij\").project(\"xmm_om_images_v4-contrast-512-5\")\n",
    "# dataset = project.version(2).download(\"coco-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34cba8",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images-contrast-512-v5-3/train/'\n",
    "# json_file_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images-contrast-512-v5-3/train/_annotations.coco.json'\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/-xmm_om_images_v4-contrast-512-5-2/train/'\n",
    "json_file_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/-xmm_om_images_v4-contrast-512-5-2/train/_annotations.coco.json'\n",
    "\n",
    "# COCO segmentation bboxes are in XYWH format\n",
    "with open(json_file_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "ground_truth_masks, bbox_coords, classes, class_categories = get_coords_and_masks_from_json(input_dir, data) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c08dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_images = [input_dir+img_data['file_name'] for img_data in data['images']]\n",
    "\n",
    "image_paths_no_augm = []\n",
    "\n",
    "for im_path in raw_images:\n",
    "    has_annots = 0\n",
    "    for k, v in bbox_coords.items():\n",
    "        if im_path.split('/')[-1] in k:\n",
    "            has_annots = 1\n",
    "            \n",
    "    if has_annots==0:\n",
    "                print(\"Img doesn't have annotations after filtering.\")\n",
    "    else:\n",
    "        image_paths_no_augm.append(im_path)\n",
    "\n",
    "len(image_paths_no_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b0ef9-9916-44c8-a1b6-521e0d96630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'before', len(data['annotations']), 'after', len(ground_truth_masks.values()), len(bbox_coords.values()), len(classes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ca004-2fe6-4bcf-8b12-59faa0141d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(data['annotations'])):\n",
    "#     points = data['annotations'][i]['segmentation'][0]\n",
    "#     binary_m = create_mask(points, (512, 512)) # COCO segmentations are polygon points, and must be converted to masks\n",
    "#     bbox = mask_to_bbox(binary_m)  #XYXY\n",
    "#     if bbox[2] - bbox[0]<4 and bbox[3] - bbox[1] <4:\n",
    "#         print(bbox[2],bbox[3])\n",
    "#         print(data['annotations'][i]['image_id'])\n",
    "#         plt.imshow(binary_m)\n",
    "#         plt.show()\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452fed4-6263-4b69-abef-17a93f8db341",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "This algorithm performs augmentations and updates the negative masks in the case of a geometric transformations. Otherwise, it masks the result of a noise/blur transformation given the mask of the initial image. \n",
    "\n",
    "**!! For augmentation, the bboxes are expected to be in the XYWH format, not XYXY format (used by SAM). However, the SAM AMG generated results are in the XYWH format (converted from XYXY).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa47349",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob(f'{input_dir}/*augm*')\n",
    "\n",
    "for file in files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa24f5a",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/examples/showcase/\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.color import label2rgb\n",
    "import albumentations as A\n",
    "import random\n",
    "import math\n",
    "\n",
    "# load utils\n",
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "\n",
    "def enlarge_bbox(bbox, delta, image_size):\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min = max(0, math.floor(x_min))\n",
    "    y_min = max(0,  math.floor(y_min))\n",
    "    \n",
    "    w = min(image_size[1], math.ceil(w))\n",
    "    h = min(image_size[0], math.ceil(h))\n",
    "    return np.array([x_min, y_min, w, h])\n",
    "\n",
    "geometrical_augmentations = A.Compose([\n",
    "    A.Flip(),\n",
    "    A.RandomRotate90(),\n",
    "    A.RandomSizedCrop((512 - 50, 512 - 50), 512, 512),\n",
    "], bbox_params={'format':'coco', 'min_area': 0.1, 'min_visibility': 0.3, 'label_fields': ['category_id']}, p=1)\n",
    "\n",
    "noise_blur_augmentations = A.Compose([\n",
    "    A.GaussianBlur(blur_limit=(3, 3), p=1),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
    "    A.ISONoise(p=0.8),\n",
    "], bbox_params={'format':'coco', 'min_area': 0.1, 'min_visibility': 0.3, 'label_fields': ['category_id']}, p=1)\n",
    "\n",
    "    \n",
    "image_paths = []\n",
    "for image_path in image_paths_no_augm:\n",
    "    image_paths.append(image_path)\n",
    "    image_ = cv2.imread(image_path)\n",
    "    image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "    image_size = (image_.shape[0], image_.shape[1])\n",
    "    masks = [value_i for key_i, value_i in ground_truth_masks.items() if image_path.split('/')[-1] in key_i]\n",
    "    bboxes_ = [enlarge_bbox(np.array([value_i[0], math.floor(value_i[1]),  math.floor(value_i[2] - value_i[0]), \\\n",
    "                                    math.floor(value_i[3] - value_i[1])]), 2, image_size) \\\n",
    "                                    for key_i, value_i in bbox_coords.items() if image_path.split('/')[-1] in key_i]   \n",
    "    label_ids = [classes[key_i] for key_i, value_i in bbox_coords.items() if image_path.split('/')[-1] in key_i]\n",
    "\n",
    "    image_size = (image_.shape[0], image_.shape[1])\n",
    "    for bbox in bboxes_:\n",
    "        if bbox[2] <= 1 or bbox[3] <= 1:\n",
    "            print(\"Invalid bbox detected:\", bbox)\n",
    "        \n",
    "    if len(bboxes_) != len(masks):\n",
    "        print('len(bboxes_) != len(masks)', len(bboxes_), len(masks))\n",
    "        continue\n",
    "\n",
    "    if len(bboxes_) != len(label_ids):\n",
    "        print('len(bboxes_) != len(label_ids)', len(bboxes_), len(label_ids))\n",
    "        continue\n",
    "\n",
    "    if len(bboxes_) == 0:\n",
    "        print(image_path)\n",
    "        \n",
    "    img_negative_mask = (image_>0).astype(int)\n",
    "    \n",
    "    # the geometrical augm doesn't change the shape of the image\n",
    "    augmented1 = augment_and_show(geometrical_augmentations, image_, masks, bboxes_, label_ids, class_categories, show_=False)\n",
    "    new_image_negative_mask = (augmented1['image']>0).astype(int) # to mask the transform which is derived from the geometric transform\n",
    "    augmented3 = augment_and_show(noise_blur_augmentations, image_, masks, bboxes_, label_ids, class_categories, show_=False)\n",
    "\n",
    "    # mask the transform using the image negative mask\n",
    "    augmented3['image'] = augmented3['image'] * img_negative_mask\n",
    "        \n",
    "    new_filename1 = image_path.replace('.'+image_path.split('.')[-1], '_augm1.jpg')\n",
    "    new_filename3 = image_path.replace('.'+image_path.split('.')[-1], '_augm3.jpg')\n",
    "\n",
    "    # print('aug masks1:', len(augmented1['masks']), 'bboxes:', len(augmented1['bboxes']))\n",
    "    # print('aug masks3:',len(augmented3['masks']), 'bboxes:', len(augmented3['bboxes']))\n",
    "\n",
    "    update_dataset_with_augms(augmented1, new_filename1, bbox_coords, ground_truth_masks, image_paths, classes)\n",
    "    update_dataset_with_augms(augmented3, new_filename3, bbox_coords, ground_truth_masks, image_paths, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0d944-e097-4a6b-bd29-8be45fbd88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for im_path in image_paths:\n",
    "    has_annots = 0\n",
    "    for k, v in bbox_coords.items():\n",
    "        if im_path.split('/')[-1] in k:\n",
    "            has_annots = 1\n",
    "            \n",
    "    if has_annots==0:\n",
    "           image_paths.remove(im_path)\n",
    "\n",
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3ab2a-7841-4d44-90d3-817e2347d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_i, value_i in bbox_coords.items():\n",
    "     if image_path.split('/')[-1] in key_i:\n",
    "        x1, y1, x2, y2 = value_i\n",
    "        if x2 - x1<2 or y2-y1 <2:\n",
    "            print(value_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5febcea-ac38-4ace-8011-b869336eb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in image_paths:\n",
    "    image_ = cv2.imread(image_path)\n",
    "    image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "    if image_.shape [1]< 512:\n",
    "        print(image_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cde032",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90482e51",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# augmented3['image'].shape, len(augmented3['bboxes']), len(augmented3['masks']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae50f7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    image_array = np.array(image) / 255.0\n",
    "\n",
    "    mean = image_array.mean()\n",
    "    std = image_array.std()\n",
    "\n",
    "    means.append(mean)\n",
    "    stds.append(std)\n",
    "\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920e435",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "np.mean(means), np.std(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ca4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(stds),np.std(stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f05a5e-9a88-4245-9ad7-c25fee0d9971",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bebae7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import math\n",
    "\n",
    "def split_list(input_list, percentages):\n",
    "    size = len(input_list)\n",
    "    idx = 0\n",
    "    output = []\n",
    "    for percentage in percentages:\n",
    "        chunk_size = round(percentage * size)\n",
    "        chunk = input_list[idx : idx + chunk_size]\n",
    "        output.append(chunk)\n",
    "        idx += chunk_size\n",
    "    return output\n",
    "\n",
    "def create_dataset(image_paths, ground_truth_masks, bbox_coords):\n",
    "        \n",
    "    d_gt_masks, d_bboxes = {}, {}\n",
    "    for img_path in image_paths:\n",
    "        id = img_path.split('/')[-1]\n",
    "        d_gt_masks.update({mask_id:mask_array for mask_id, mask_array in ground_truth_masks.items() if mask_id.startswith(id)})\n",
    "        d_bboxes.update({bbox_id:bbox for bbox_id, bbox in bbox_coords.items() if bbox_id.startswith(id)}) \n",
    "\n",
    "    return d_gt_masks, d_bboxes\n",
    "    \n",
    "training_size, val_size, test_size = (0.7, 0.2, 0.1)\n",
    "splits = split_list(image_paths, [training_size, val_size, test_size])\n",
    "training_image_paths, val_image_paths, test_image_paths = splits[0], splits[1], splits[2]\n",
    "\n",
    "train_gt_masks, train_bboxes = create_dataset(training_image_paths, ground_truth_masks, bbox_coords)\n",
    "test_gt_masks, test_bboxes = create_dataset(test_image_paths, ground_truth_masks, bbox_coords)\n",
    "val_gt_masks, val_bboxes = create_dataset(val_image_paths, ground_truth_masks, bbox_coords)\n",
    "\n",
    "del ground_truth_masks, bbox_coords, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d708a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "len(training_image_paths), len(test_image_paths), len(val_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba6afe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for one_path in training_image_paths[:4]:\n",
    "    image_id2 = one_path.split('/')[-1]\n",
    "    print(image_id2)\n",
    "    image_masks_ids = [key for key in train_gt_masks.keys() if key.startswith(image_id2)]\n",
    "    image_ = cv2.imread(one_path)\n",
    "    image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(image_)\n",
    "    for name in image_masks_ids:\n",
    "            show_box(train_bboxes[name], plt.gca())\n",
    "            show_mask(train_gt_masks[name], plt.gca())\n",
    "    plt.axis('off')\n",
    "    # plt.savefig(f'example{image_id2}.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fb7c3-b24f-428e-b335-7d6942afbfba",
   "metadata": {},
   "source": [
    "## üöÄ Prepare Mobile SAM Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb250bdf",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/')\n",
    "import ft_mobile_sam\n",
    "from ft_mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/weights/mobile_sam.pt\"\n",
    "# mobile_sam_checkpoint = \"./mobile_sam_model_checkpoint0.pth\"\n",
    "device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae6411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "\n",
    "if use_wandb:\n",
    "    from datetime import datetime\n",
    "    # !pip install wandb\n",
    "    # !wandb login --relogin\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=\"OM_AI_v1\", name=f\"ft_MobileSAM {datetime.now()}\")\n",
    "\n",
    "    wandb.watch(mobile_sam_model, log='all', log_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79627c-55c2-4049-acf4-d7a2ed6fc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in mobile_sam_model.named_parameters():\n",
    "#     if 'decoder' in name and 'mlps.0.' in name:\n",
    "#         print(f\"Layer: {name}, Weights: {param.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5b948-8325-438c-979e-de49ede31b70",
   "metadata": {},
   "source": [
    "# Convert the input images into a format SAM's internal functions expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0d674",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import segment_anything\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "from importlib import reload\n",
    "import astronomy_utils\n",
    "reload(astronomy_utils)\n",
    "from astronomy_utils import *\n",
    "\n",
    "transform = ResizeLongestSide(mobile_sam_model.image_encoder.img_size)\n",
    "\n",
    "def transform_image(image, k):\n",
    "       \n",
    "        # sets a specific mean for each image\n",
    "        image_T = np.transpose(image, (2, 1, 0))\n",
    "        mean_ = np.mean(image_T[image_T>0])\n",
    "        std_ = np.std(image_T[image_T>0]) \n",
    "        pixel_mean = torch.as_tensor([mean_, mean_, mean_], dtype=torch.float, device=device)\n",
    "        pixel_std = torch.as_tensor([std_, std_, std_], dtype=torch.float, device=device)\n",
    "\n",
    "        mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).unsqueeze(-1).unsqueeze(-1), False) # not in SAM\n",
    "        mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).unsqueeze(-1).unsqueeze(-1), False) # not in SAM\n",
    "\n",
    "        # mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        # mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
    "\n",
    "        transformed_data = {}\n",
    "        negative_mask = np.where(image > 0, True, False)\n",
    "        negative_mask = torch.from_numpy(negative_mask)  \n",
    "        negative_mask = negative_mask.permute(2, 0, 1)\n",
    "        negative_mask = resize(negative_mask, [1024, 1024], antialias=True) \n",
    "        negative_mask = negative_mask.unsqueeze(0)\n",
    "        # scales the image to 1024x1024 by longest side \n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "        \n",
    "        # normalization and padding\n",
    "        input_image = mobile_sam_model.preprocess(transformed_image)\n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(transformed_image.shape[-2:])\n",
    "        input_image[~negative_mask] = 0\n",
    "        transformed_data['image'] = input_image.clone() \n",
    "        transformed_data['input_size'] = input_size \n",
    "        transformed_data['image_id'] = k\n",
    "        transformed_data['original_image_size'] = original_image_size\n",
    "    \n",
    "        return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60b93e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, bbox_coords, gt_masks, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_paths[idx].split(\"/\")[-1]\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            return self.transform(image, img_id)\n",
    "        else:\n",
    "            print(\"‚ùóÔ∏èNo transform‚ùóÔ∏è\")\n",
    "            return image\n",
    "        \n",
    "batch_size =10\n",
    "\n",
    "train_set = ImageDataset(training_image_paths, train_bboxes, train_gt_masks, transform_image) \n",
    "val_set = ImageDataset(val_image_paths, val_bboxes, val_gt_masks, transform_image) \n",
    "test_set = ImageDataset(test_image_paths, test_bboxes, test_gt_masks, transform_image) \n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab59ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_paths[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd9d65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "lr=1e-4\n",
    "wd=0.0\n",
    "\n",
    "parameters_to_optimize = [param for param in mobile_sam_model.mask_decoder.parameters() if param.requires_grad]\n",
    "\n",
    "# Initialize the optimizer with those parameters\n",
    "optimizer = torch.optim.AdamW(parameters_to_optimize, lr=lr, weight_decay=wd) #betas=(0.9, 0.999))\n",
    "\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30, eta_min=1.0) # not very helpful\n",
    "\n",
    "def dice_loss(pred, target, negative_mask = None, smooth = 1): \n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    \n",
    "    if negative_mask is not None: # masked loss\n",
    "        negative_mask = negative_mask.bool()\n",
    "        pred = pred * negative_mask\n",
    "        target = target * negative_mask \n",
    "        \n",
    "    # fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    # axs[0].imshow(pred[0, 0].cpu().detach().numpy(), cmap='gray')\n",
    "    # axs[0].set_title('Pred')\n",
    "    # axs[1].imshow(target[0, 0].cpu().detach().numpy(), cmap='gray')\n",
    "    # axs[1].set_title('Target')\n",
    "    # axs[2].imshow(negative_mask[0, 0].cpu().detach().numpy(), cmap='gray')\n",
    "    # axs[2].set_title('Negative Mask')\n",
    "    # plt.show()\n",
    "\n",
    "    intersection = (pred * target).sum()\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)))\n",
    "        \n",
    "    return loss.mean()\n",
    "\n",
    "def focal_loss(inputs, targets):\n",
    "\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1 - BCE_EXP)**gamma * BCE\n",
    "\n",
    "        return focal_loss\n",
    "\n",
    "# class DiceLoss(nn.Module):\n",
    "#     \"\"\" Computes the Dice loss. \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, inputs, targets, smooth=1):\n",
    "#         inputs = F.sigmoid(inputs)\n",
    "#         inputs = inputs.flatten(0,2)\n",
    "#         intersection = (inputs * targets).sum()\n",
    "#         dice = (2. * intersection + smooth) / \\\n",
    "#             (inputs.sum() + targets.sum() + smooth)\n",
    "#         return 1 - dice\n",
    "\n",
    "# def criterion(x, y):\n",
    "#     \"\"\" Combined dice and focal loss.\n",
    "#     ARGS:\n",
    "#         x: (torch.Tensor) the model output\n",
    "#         y: (torch.Tensor) the target\n",
    "#     RETURNS:\n",
    "#         (torch.Tensor) the combined loss\n",
    "\n",
    "#     \"\"\"\n",
    "#     focal, dice = FocalLoss(), DiceLoss()\n",
    "#     y = y.to(DEVICE)\n",
    "#     x = x.to(DEVICE)\n",
    "#     return 20 * focal(x, y) + dice(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf128acc-595e-4618-9f6a-33936e0b08c6",
   "metadata": {},
   "source": [
    "## Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516038e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    if \"mask_decoder\" in name:\n",
    "    # if False:\n",
    "    # layers_to_fine_tune = ['mask_decoder.output_hypernetworks_mlps.1', 'mask_decoder.output_hypernetworks_mlps.2', 'mask_decoder.output_hypernetworks_mlps.3', \\\n",
    "    #                       'mask_decoder.iou_prediction_head.layers', 'mask_decoder.output_hypernetworks_mlps.0.layers', 'output_upscaling']\n",
    "    # if any(s in name for s in layers_to_fine_tune):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        # if 'norm' in name:\n",
    "        #     param.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e53781",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def check_requires_grad(model, show=True):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and show:\n",
    "            print(\"‚úÖ Param\", name, \" requires grad.\")\n",
    "        elif param.requires_grad == False:\n",
    "            print(\"‚ùå Param\", name, \" doesn't require grad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb84f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"üöÄ The model has {sum(p.numel() for p in mobile_sam_model.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "check_requires_grad(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ade316-c813-48df-895f-1f4988444685",
   "metadata": {},
   "source": [
    "## The classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b9bbf-f058-4d69-b445-6ed36fe47eb5",
   "metadata": {},
   "source": [
    "## Run fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37018f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "len(train_gt_masks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c07950",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# **idea** can use mixed-precision training:\n",
    "# This is a technique that involves using a mix of float16 and float32 tensors \n",
    "# to make the model use less memory and run faster. \n",
    "# PyTorch provides the torch.cuda.amp module for automatic mixed-precision training.\n",
    "# from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bccbe5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import predictor_utils\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "\n",
    "def validate_model_AMG(ft_mobile_sam_model, mobile_sam_model_orig):\n",
    "    validation_loss = []\n",
    "    ft_validation_loss = []\n",
    "    for val_img_path in val_image_paths[:5]:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            print('Original MobileSAM')\n",
    "            annotated_image_orig_mobile_sam, annotated_image_orig_mobile_sam_loss = amg_predict(mobile_sam_model_orig, orig_mobile_SamAutomaticMaskGenerator, \\\n",
    "                                                       val_gt_masks, 'ft_MobileSAM', val_img_path, mask_on_negative=False, show_plot=True)\n",
    "\n",
    "            print('Fine-tuned MobileSAM')\n",
    "            annotated_img, loss = amg_predict(mobile_sam_model, SamAutomaticMaskGenerator, val_gt_masks, 'ft_MobileSAM', \\\n",
    "                                              val_img_path, mask_on_negative=True, show_plot=True)\n",
    "            \n",
    "            ft_validation_loss.append(annotated_image_orig_mobile_sam_loss)\n",
    "            validation_loss.append(loss)\n",
    "\n",
    "    return np.mean(ft_validation_loss), np.mean(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee186320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_image_predict(image_masks, gt_masks, gt_bboxes, image_embedding, original_image_size, input_size, negative_mask, image, model):\n",
    "    image_loss=[]\n",
    "    mask_result = []\n",
    "    total_area = 0.0\n",
    "    ce_loss = 0.0\n",
    "    for k in image_masks:   \n",
    "\n",
    "                # process bboxes\n",
    "                prompt_box = np.array(gt_bboxes[k])\n",
    "                box = predictor.transform.apply_boxes(prompt_box, original_image_size)\n",
    "                box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "                box_torch = box_torch[None, :]\n",
    "\n",
    "                # process masks\n",
    "                mask_input_torch = torch.as_tensor(gt_masks[k], dtype=torch.float, device=predictor.device).unsqueeze(0).unsqueeze(0)\n",
    "                mask_input_torch = F.interpolate(mask_input_torch, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "                # '''\n",
    "                # this is not in SAM. I added this, because the values in the mask are not only 0/1\n",
    "                # not sure if it's correct ‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è\n",
    "                # '''\n",
    "                # mask_input_torch = (mask_input_torch>0) * 1.0\n",
    "\n",
    "                # process coords and labels\n",
    "                x_min, y_min, x_max, y_max = gt_bboxes[k]\n",
    "                x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "        \n",
    "                point_coords = np.array([(gt_bboxes[k][2]+gt_bboxes[k][0])/2.0, (gt_bboxes[k][3]+gt_bboxes[k][1])/2.0])\n",
    "                point_labels = np.array([1])\n",
    "                point_coords = predictor.transform.apply_coords(point_coords, original_image_size)\n",
    "                coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=predictor.device).unsqueeze(0)\n",
    "                labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=predictor.device)\n",
    "                coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n",
    "\n",
    "                # mask_copy = gt_masks[k].copy()\n",
    "                # mask_copy = cv2.cvtColor(mask_copy, cv2.COLOR_GRAY2BGR)\n",
    "                # mask_copy = mask_copy * 255\n",
    "                # print(x_min, y_min, x_max, y_max)\n",
    "                # cv2.rectangle(mask_copy, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                # plt.imshow(mask_copy)\n",
    "                # plt.plot((gt_bboxes[k][2]+gt_bboxes[k][0])/2.0, (gt_bboxes[k][3]+gt_bboxes[k][1])/2.0, 'ro')\n",
    "                # plt.show()\n",
    "\n",
    "                sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                  points=(coords_torch, labels_torch),\n",
    "                  boxes=box_torch, #None,\n",
    "                  masks=None, #mask_input_torch,\n",
    "                )\n",
    "                # print(box_torch.shape, mask_input_torch.shape)\n",
    "\n",
    "                del box_torch, coords_torch, labels_torch\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # print(image_embedding.shape, model.prompt_encoder.get_dense_pe().shape, sparse_embeddings.shape, dense_embeddings.shape)\n",
    "        \n",
    "                low_res_masks, iou_predictions = model.mask_decoder(\n",
    "                image_embeddings=image_embedding,\n",
    "                image_pe=model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=True, # True value works better for ambiguous prompts (single points)\n",
    "                    # Also, multimask_output= False will set some gradients on 0\n",
    "                )\n",
    "        \n",
    "                # print(low_res_masks.shape)\n",
    "\n",
    "                downscaled_masks = model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "        \n",
    "                # focal_loss = focal_loss(downscaled_masks, gt_binary_mask)\n",
    "        \n",
    "                # print(downscaled_masks.shape)\n",
    "                # binary_mask = normalize(threshold(downscaled_masks, 0.0, 0))\n",
    "                binary_mask = torch.sigmoid(downscaled_masks - model.mask_threshold)\n",
    "                gt_mask_resized = torch.from_numpy(np.resize(gt_masks[k], (1, 1, gt_masks[k].shape[0], gt_masks[k].shape[1]))).to(device)\n",
    "                gt_binary_mask = torch.as_tensor(gt_mask_resized>0, dtype=torch.float32) \n",
    "                # print(gt_binary_mask.dtype, gt_binary_mask.shape)\n",
    "                numpy_gt_binary_mask = gt_binary_mask.contiguous().detach().cpu().numpy()\n",
    "\n",
    "                # # mask = np.expand_dims(gt_binary_mask, axis=-1)\n",
    "                # # mask_input_torch = torch.as_tensor(mask, dtype=torch.float, device=device)\n",
    "                # # mask_input_torch = mask_input_torch / mask_input_torch.max()\n",
    "                # input_image = torch.as_tensor(image, dtype=torch.float, device=device)\n",
    "                # input_image = input_image / input_image.max()\n",
    "\n",
    "                # # CE loss over mask\n",
    "                # class_output = extended_model(mask_input_torch[0].permute(1,2,0) * input_image).unsqueeze(0) # also take the image pixels into account\n",
    "\n",
    "                # # class_output = extended_model(binary_mask, iou_predictions).unsqueeze(0)\n",
    "                # # print('class_output', class_output)\n",
    "                # gt_class = torch.as_tensor([classes[k]], dtype=torch.long, device=device)\n",
    "                # # find the index where the max value is in the class_output\n",
    "                # # max_index = torch.argmax(class_output)\n",
    "                # # print('class_output shape:', class_output, 'gt class', gt_class)\n",
    "                # ce_loss += ce_loss_fn(class_output, gt_class)\n",
    "                # # print('CE loss:', ce_loss)\n",
    "\n",
    "                # print(iou_predictions)\n",
    "                mask_result.append(binary_mask[0][0].detach().cpu())\n",
    "\n",
    "                # plt.imshow(numpy_gt_binary_mask[0][0])\n",
    "                # plt.show()\n",
    "        \n",
    "                # compute weighted dice loss\n",
    "                # print(gt_masks[k].shape,np.sum(gt_masks[k]))\n",
    "                mask_area = np.sum(gt_masks[k])\n",
    "\n",
    "                if mask_area > 0:\n",
    "                  image_loss.append(dice_loss(binary_mask, gt_binary_mask, negative_mask, mask_area) * mask_area)\n",
    "                  total_area += mask_area\n",
    "\n",
    "                del binary_mask \n",
    "                del gt_mask_resized, numpy_gt_binary_mask \n",
    "                del low_res_masks, iou_predictions \n",
    "        \n",
    "                torch.cuda.empty_cache()\n",
    "                # print('one mask', k, 'done')\n",
    "                # print(f'one image{k}: Allocated memory:', torch.cuda.memory_allocated()/(1024**2), 'MB. Reserved memory:', torch.cuda.memory_reserved()/(1024**2), 'MB')\n",
    "\n",
    "    image_loss = torch.stack(image_loss) # list to tensor\n",
    "    image_loss = torch.mean(image_loss)\n",
    "    # image_loss /= total_area\n",
    "    # if len(image_masks) > 0:\n",
    "    #     ce_loss /= len(image_masks)\n",
    "    # image_loss = (image_loss + ce_loss)/2.0\n",
    "    \n",
    "    # fig, axs = plt.subplots(1, 3, figsize=(40, 20))\n",
    "\n",
    "    # axs[0].imshow(image)\n",
    "    # # show_masks(mask_result, axs[0], random_color=False)\n",
    "    # # axs[0].set_title(f'{idd.split(\".\")[0]}', fontsize=10)\n",
    "    # axs[0].set_title(f'{k.split(\".\")[0]}', fontsize=70)\n",
    "    # axs[0].axis('off')\n",
    "\n",
    "    # axs[1].imshow(image)\n",
    "    # show_masks(mask_result, axs[1], random_color=False)\n",
    "    # axs[1].set_title('Predicted masks', fontsize=70)\n",
    "    # axs[1].axis('off')\n",
    "    \n",
    "    # axs[2].imshow(image)\n",
    "    # show_masks([gt_masks[k] for k in image_masks], axs[2], random_color=False)\n",
    "    # axs[2].set_title('Ground truth masks', fontsize=70)\n",
    "    # axs[2].axis('off')\n",
    "    # plt.savefig(f'./plots/pred_{k}.png', dpi=300)\n",
    "\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "\n",
    "    return image_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3a569-80a4-4ea2-9f99-0cc03107dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_validate_step(model, dataloader, gt_masks, gt_bboxes, device, optimizer=None, mode='train'):\n",
    "    assert mode in ['train', 'validate'], \"Mode must be 'train' or 'validate'\"\n",
    "    \n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    for inputs in tqdm(dataloader):\n",
    "        batch_loss = 0.0\n",
    "        batch_size = len(inputs['image'])\n",
    "        for i in range(batch_size):\n",
    "            image_masks = {k for k in gt_masks.keys() if k.startswith(inputs['image_id'][i])}\n",
    "            idd = inputs['image_id'][i]\n",
    "            input_image = inputs['image'][i].to(device)\n",
    "            # print(input_image.mean(), input_image.min(), input_image.max())\n",
    "            \n",
    "            image = cv2.imread(input_dir+inputs['image_id'][i])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            original_image_size = image.shape[:-1]\n",
    "            input_size = (1024, 1024)\n",
    "            # np_image = np.transpose(input_image[0].detach().cpu().numpy(), (1, 2, 0))\n",
    "            # print(np.min(np_image), np.max(np_image))\n",
    "            # plt.imshow(input_image[0][0].detach().cpu(), cmap='viridis')\n",
    "            # plt.title(f'{inputs[\"image_id\"][i]}.png')\n",
    "            # plt.show()\n",
    "            \n",
    "            # IMAGE ENCODER\n",
    "            image_embedding = model.image_encoder(input_image)\n",
    "               \n",
    "            # negative_mask has the size of the image\n",
    "            negative_mask = np.where(image>0, True, False)\n",
    "            negative_mask = torch.from_numpy(negative_mask)  \n",
    "            negative_mask = negative_mask.permute(2, 0, 1)\n",
    "            negative_mask = negative_mask[0]\n",
    "            negative_mask = negative_mask.unsqueeze(0).unsqueeze(0)\n",
    "            negative_mask = negative_mask.to(device)\n",
    "            \n",
    "            # RUN PREDICTION ON IMAGE\n",
    "            if len(image_masks)>0:\n",
    "                batch_loss += (one_image_predict(image_masks, gt_masks, gt_bboxes, image_embedding, original_image_size, input_size, negative_mask, image, model)/len(image_masks)) \n",
    "            else:\n",
    "                print(\"‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è Image with no annotations.\", idd)\n",
    "        if mode == 'train':\n",
    "            for name, parameter in model.named_parameters():\n",
    "                if parameter.grad is not None: \n",
    "                    grad_norm = parameter.grad.norm()\n",
    "                    if grad_norm < 1e-8: \n",
    "                        print(f'‚ùóÔ∏èLayer {name} has vanishing gradients: {grad_norm}')\n",
    "            # for name, parameter in model.named_parameters():\n",
    "            # \tif parameter.grad is not None:\n",
    "            # \t\tprint(name, parameter.grad.abs().mean())\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            del image_embedding, negative_mask, input_image, idd, image\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        losses.append(batch_loss.item() / batch_size)\n",
    "            \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e17bd-54ef-4126-a249-43addea772a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "predictor = SamPredictor(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d871e9-4d0f-4dd9-9bff-b994ac97af4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # train\n",
    "    epoch_loss = train_validate_step(mobile_sam_model, train_dataloader, train_gt_masks, train_bboxes, device, optimizer, mode='train')\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # validate\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss = train_validate_step(mobile_sam_model, val_dataloader, val_gt_masks, val_bboxes, device, optimizer, mode='validate')\n",
    "        valid_losses.append(epoch_val_loss)\n",
    "\n",
    "    # # Logging\n",
    "    # if use_wandb:\n",
    "    #     wandb.log({'epoch training loss': epoch_loss, 'epoch validation loss': epoch_val_loss})\n",
    "\n",
    "    print(f'EPOCH: {epoch}. Training loss: {epoch_loss}')\n",
    "    print(f'EPOCH: {epoch}. Validation loss: {epoch_val_loss}.')\n",
    "\n",
    "    # if epoch % 20 == 0:\n",
    "    #     torch.save(mobile_sam_model.state_dict(), f'mobile_sam_model_checkpoint{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6b90d-fa05-4925-8be0-f6c90682ebaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46557d-4d4f-40d9-a8f2-fd493d81f345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e59fd1-c27c-4b12-9bd6-97b062a8c2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ed5b7-a758-407a-91dc-7698e7430c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4520f-103b-46b2-9495-04e611899f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc14c2-53be-4445-84c4-8ea1b63d2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/')\n",
    "import mobile_sam\n",
    "from mobile_sam import sam_model_registry as orig_mobile_sam_model_registry, \\\n",
    "                       SamAutomaticMaskGenerator as orig_mobile_SamAutomaticMaskGenerator, \\\n",
    "                       SamPredictor as orig_mobile_SamPredictor\n",
    "\n",
    "orig_mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-master/weights/mobile_sam.pt\"\n",
    "\n",
    "orig_mobile_sam_model = orig_mobile_sam_model_registry[\"vit_t\"](checkpoint=orig_mobile_sam_checkpoint)\n",
    "orig_mobile_sam_model.to(device);\n",
    "orig_mobile_sam_model.eval();\n",
    "\n",
    "mobile_sam_model.eval();\n",
    "predictor = orig_mobile_SamPredictor(mobile_sam_model)\n",
    "\n",
    "valid_losses = []\n",
    "for epoch in range(1):\n",
    "    # epoch_loss = train_step(mobile_sam_model, dataloader, optimizer, device, mode='train')\n",
    "    # train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation step\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss = train_validate_step(orig_mobile_sam_model, val_dataloader, val_gt_masks, val_bboxes, device, optimizer, mode='validate')\n",
    "        valid_losses.append(epoch_val_loss)\n",
    "\n",
    "    # # Logging\n",
    "    # if use_wandb:\n",
    "    #     wandb.log({'epoch training loss': epoch_loss, 'epoch validation loss': epoch_val_loss})\n",
    "\n",
    "    # print(f'EPOCH: {epoch}. Training loss: {epoch_loss}')\n",
    "    print(f'EPOCH: {epoch}. Validation loss: {epoch_val_loss}.')\n",
    "\n",
    "    # if epoch % 20 == 0:\n",
    "    #     torch.save(mobile_sam_model.state_dict(), f'mobile_sam_model_checkpoint{epoch}.pth')\n",
    "\n",
    "# validate_model_AMG(mobile_sam_model, orig_mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdd493-18fd-425b-a855-c1f41bdfd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498fb62-0044-4524-a170-77caf4c9ac77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# last run\n",
    "losses = [52.27958413326379, 40.61563884850705, 34.62209655299331, 32.76141236045144, \n",
    "          33.59805725560044, 29.96991842443293, 28.418979529178504, 27.279344472018156, \n",
    "          27.800395011901855, 28.727664427323774]\n",
    "\n",
    "epochs = list(range(1, len(losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, losses, label='Training loss\\nbboxes + coords', marker='o', linestyle='-', color='royalblue', linewidth=2, markersize=8)\n",
    "scatter_epochs = [8, 8]\n",
    "scatter_values = [41.602, 57.308]\n",
    "k=0\n",
    "scatter_labels = ['fine_tuned_MobileSAM', 'original_MobileSAM'] \n",
    "for epoch, value, label in zip(scatter_epochs, scatter_values, scatter_labels):\n",
    "    if k<1:\n",
    "        plt.scatter(epoch, value, color='magenta', s=100, label='Validation loss',  marker='o',linestyle='-', zorder=5)\n",
    "        k+=1\n",
    "    else:\n",
    "        plt.scatter(epoch, value, color='magenta', s=100, zorder=5)\n",
    "    plt.text(epoch, value, f'  {label}', verticalalignment='bottom', color='magenta')\n",
    "\n",
    "plt.title('Loss Over 10 Epochs \\nbatch_size=10, lr=1e-4')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Saving the plot with scatter points and labels to a file\n",
    "plt.savefig('loss_plot_with_scatter_and_labels.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ab8b3-67ab-4063-96d1-1be79d59b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "# display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371097d",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(losses))), losses, label='Training Loss')\n",
    "plt.plot(list(range(len(valid_bboxes_losses))), valid_bboxes_losses, label='Validation Loss')\n",
    "plt.title('Mean epoch loss \\n mask with sigmoid')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/loss_mask_valid_points_prompt.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(range(len(losses))), losses)\n",
    "# plt.title('Mean epoch loss \\n mask with sigmoid')\n",
    "# plt.xlabel('Epoch Number')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.savefig('loss_mask_sigmoid_training.png')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56320d4-afbb-4e36-9634-969163689589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c7008-f153-4d07-bb98-c5b505562fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import graphviz\n",
    "\n",
    "json_file_path = './graph_0_summary_598a07b5e588b04c9de6.graph.json'\n",
    "\n",
    "with open(json_file_path, 'r') as file:\n",
    "    model_data = json.load(file)\n",
    "\n",
    "dot = graphviz.Digraph(comment='Model Visualization')\n",
    "\n",
    "for node in model_data['nodes']:\n",
    "    label = f\"{node['name']}\\n{node['class_name']}\"\n",
    "    dot.node(str(node['id']), label=label)\n",
    "dot.render('output', view=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1766f-f1f2-4033-9842-59b727481742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a205be1-5c8c-46b9-9e0b-1de13110e645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef1450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from statistics import mean\n",
    "# from tqdm import tqdm\n",
    "# from torch.nn.functional import threshold, normalize\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# import matplotlib.patches as patches\n",
    "# import copy\n",
    "# import time\n",
    "# import torchviz\n",
    "# import torch.autograd.profiler as profiler\n",
    "\n",
    "# num_epochs = 5\n",
    "# losses = []\n",
    "# valid_losses = []\n",
    "# valid_bboxes_losses = []\n",
    "# predictor = SamPredictor(mobile_sam_model)\n",
    "# mobile_sam_model.train();\n",
    "\n",
    "# idd = 0\n",
    "# for epoch in range(num_epochs):\n",
    "\t\n",
    "#     epoch_losses = []\n",
    "#     for inputs in tqdm(dataloader): # take image ids\n",
    "#         # with profiler.profile(profile_memory=True, use_cuda=True, record_shapes=True) as prof:\n",
    "#         if True:\n",
    "#             batch_loss = 0.0\n",
    "#             batch_size = min(len(inputs['image']), batch_size) # at the end, there's less than batch_size imgs in a batch (sometimes)\n",
    "            \n",
    "#             for i in range(batch_size): \n",
    "                  \n",
    "#                 image_masks = {k for k in train_gt_masks.keys() if k.startswith(inputs['image_id'][i])}\n",
    "#                 idd = inputs['image_id'][i]\n",
    "#                 input_image = inputs['image'][i].to(device)\n",
    "#                 # print(input_image.mean(), input_image.min(), input_image.max())\n",
    "                \n",
    "#                 image = cv2.imread(input_dir+inputs['image_id'][i])\n",
    "#                 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "#                 original_image_size = image.shape[:-1]\n",
    "#                 input_size = inputs['input_size'][i]\n",
    "#                 # np_image = np.transpose(input_image[0].detach().cpu().numpy(), (1, 2, 0))\n",
    "#                 # print(np.min(np_image), np.max(np_image))\n",
    "#                 # plt.imshow(input_image[0][0].detach().cpu(), cmap='viridis')\n",
    "#                 # plt.title(f'{inputs[\"image_id\"][i]}.png')\n",
    "#                 # plt.show()\n",
    "                \n",
    "#                 # IMAGE ENCODER\n",
    "#                 image_embedding = mobile_sam_model.image_encoder(input_image)\n",
    "                   \n",
    "#                 # negative_mask has the size of the image\n",
    "#                 negative_mask = np.where(image>0, True, False)\n",
    "#                 negative_mask = torch.from_numpy(negative_mask)  \n",
    "#                 negative_mask = negative_mask.permute(2, 0, 1)\n",
    "#                 negative_mask = negative_mask[0]\n",
    "#                 negative_mask = negative_mask.unsqueeze(0).unsqueeze(0)\n",
    "#                 negative_mask = negative_mask.to(device)\n",
    "#                 # del image\n",
    "                \n",
    "#                 # RUN PREDICTION ON IMAGE\n",
    "#                 if len(image_masks)>0:\n",
    "#                     batch_loss += (one_image_predict(image_masks)/len(image_masks)) # here should be divided by len(image_masks)\n",
    "#                     # print('BATCH LOSS:', batch_loss)\n",
    "#                 else:\n",
    "#                     print(\"‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è Image with no annotations.\", idd)\n",
    "#                 del image_embedding, negative_mask, input_image, idd, image\n",
    "#                 torch.cuda.empty_cache()\n",
    "\n",
    "#         batch_loss = batch_loss * 1.0/batch_size\n",
    "#         # dot = torchviz.make_dot(batch_loss, params=dict(mobile_sam_model.named_parameters()))\n",
    "        \n",
    "#         epoch_losses.append(batch_loss.item())\n",
    "#         optimizer.zero_grad()\n",
    "#         batch_loss.backward()\n",
    "        \n",
    "#         for name, parameter in mobile_sam_model.named_parameters():\n",
    "#             if parameter.grad is not None: \n",
    "#                 grad_norm = parameter.grad.norm()\n",
    "#                 if grad_norm < 1e-8:  # threshold\n",
    "#                     print(f'‚ùóÔ∏èLayer {name} has vanishing gradients: {grad_norm}')\n",
    "#         # for name, parameter in mobile_sam_model.named_parameters():\n",
    "#         # \tif parameter.grad is not None:\n",
    "#         # \t\tprint(name, parameter.grad.abs().mean())\n",
    "                \n",
    "#         nn.utils.clip_grad_norm_(mobile_sam_model.parameters(), 1.0) # gradient clipping‚ùóÔ∏è\n",
    "#         optimizer.step() \n",
    "        \n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     # validation loss\n",
    "#     losses.append(np.mean(epoch_losses))\n",
    "#     # mobile_sam_model.eval();\n",
    "#     # # valid_losses.append(validate_model_AMG())\n",
    "#     # # valid_bboxes_losses.append(validate_model())\n",
    "#     # mobile_sam_model.train();\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     if use_wandb:\n",
    "#         wandb.log({'epoch training loss': np.mean(epoch_losses)})\n",
    "#         wandb.log({'epoch validation loss': np.mean(valid_bboxes_losses)})\n",
    "#     # print(f'EPOCH: {epoch}. Training loss: {np.mean(epoch_losses)}. Validation AMG loss: {np.mean(valid_losses)}. Validation bboxes loss: {np.mean(valid_bboxes_losses)} ')\n",
    "#     # print(f'EPOCH: {epoch}. Training loss: {np.mean(epoch_losses)}.Validation bboxes loss: {np.mean(valid_bboxes_losses)} ')\n",
    "#     print(f'EPOCH: {epoch}. Training loss: {np.mean(epoch_losses)}.')\n",
    "#     if epoch%20==0:\n",
    "#         torch.save(mobile_sam_model.state_dict(), f'mobile_sam_model_checkpoint{epoch}.pth')\n",
    "\n",
    "# print('Training losses:', losses)\n",
    "# print('valid_bboxes_losses losses:', valid_bboxes_losses)\n",
    "# # print('Val losses:', valid_losses)\n",
    "\n",
    "# if use_wandb:\n",
    "#     wandb.run.summary[\"batch_size\"] = batch_size\n",
    "#     wandb.run.summary[\"num_epochs\"] = num_epochs\n",
    "#     wandb.run.summary[\"learning rate\"] = lr\n",
    "#     wandb.run.summary[\"used area for DICE loss\"] = 'YES'\n",
    "#     run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0337c7-9f95-431f-beaf-78281a74ccb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
