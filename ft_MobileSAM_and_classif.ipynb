{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47421b3-f94e-4367-a703-1b50b6cdfee1",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29c832",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "import json\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "# Ensure deterministic behavior (cannot control everything though)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "import predictor_utils\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a170d-fe7d-4816-a67f-e520d6b3b3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "# project = rf.workspace(\"orij\").project(\"xmm_om_images-contrast-512-v5\")\n",
    "# dataset = project.version(3).download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34cba8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images-contrast-512-v5-3/train/'\n",
    "json_file_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images-contrast-512-v5-3/train/_annotations.coco.json'\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ground_truth_masks, bbox_coords, classes, class_categories = get_coords_and_masks_from_json(input_dir, data) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_categories, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c08dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_no_augm = [input_dir+img_data['file_name'] for img_data in data['images']]\n",
    "len(image_paths_no_augm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452fed4-6263-4b69-abef-17a93f8db341",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "This algorithm performs augmentations and updates the negative masks in the case of a geometric transformations. Otherwise, it masks the result of a contrastive transformation given the mask of the initial image. Usually, the geometrical and contrastive transformations are not used simultaneously, as the notion of $<0$ pixels (negative mask) is lost.\n",
    "\n",
    "**!! For augmentation, the bboxes are expected to be in the XYHW format, not XYXY format (used by SAM). However, the SAM AMG generated results are in the XYHW format (converted from XYXY).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa47349",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob(f'{input_dir}/*augm*')\n",
    "\n",
    "for file in files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa24f5a",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/examples/showcase/\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.color import label2rgb\n",
    "import albumentations as A\n",
    "import random\n",
    "import math\n",
    "\n",
    "# load utils\n",
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "\n",
    "def enlarge_bbox(bbox, delta, image_size):\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min = max(0, math.floor(x_min))\n",
    "    y_min = max(0,  math.floor(y_min))\n",
    "    \n",
    "    w = min(image_size[1], math.ceil(w))\n",
    "    h = min(image_size[0], math.ceil(h))\n",
    "    return np.array([x_min, y_min, w, h])\n",
    "\n",
    "# define some augmentations\n",
    "geometrical_augmentations = A.Compose([\n",
    "    A.Flip(),\n",
    "    A.RandomRotate90(),\n",
    "    A.RandomSizedCrop((512 - 150, 512 - 50), 512, 512),\n",
    "], bbox_params={'format':'coco', 'min_area': 0.1, 'min_visibility': 0.3, 'label_fields': ['category_id']}, p=1)\n",
    "\n",
    "intensity_color_augmentations = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.0, contrast_limit=0.1, p=1),\n",
    "    A.Equalize(p=1)\n",
    "], bbox_params={'format':'coco', 'min_area': 0.1, 'min_visibility': 0.3, 'label_fields': ['category_id']}, p=1)\n",
    "\n",
    "noise_blur_augmentations = A.Compose([\n",
    "    A.GaussianBlur(blur_limit=(3, 3), p=1),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
    "    A.ISONoise(p=0.8),\n",
    "    # A.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2, p=1) \n",
    "], bbox_params={'format':'coco', 'min_area': 0.1, 'min_visibility': 0.3, 'label_fields': ['category_id']}, p=1)\n",
    "\n",
    "def align_masks_and_bboxes(augmented_set):\n",
    "    '''\n",
    "    Sometimes, the masks and bboxes returned by the Augmentation process do not have the same size, and (thus) they are not aligned by index. \n",
    "    This function will keep the non-empty masks and generate another bboxes given those masks.\n",
    "    '''\n",
    "    \n",
    "    bboxes_augm = []\n",
    "    masks_augm = []\n",
    "    \n",
    "    for mask_i in augmented_set['masks']:\n",
    "        if np.any(mask_i):\n",
    "            bbox = cv2.boundingRect(mask_i)\n",
    "            bboxes_augm.append(bbox)\n",
    "            masks_augm.append(mask_i)\n",
    "    \n",
    "    augmented_set['masks'] = masks_augm\n",
    "    augmented_set['bboxes'] = bboxes_augm\n",
    "    augmented_set['category_id'] = [1] * len(masks_augm)\n",
    "    return augmented_set\n",
    "    \n",
    "image_paths = []\n",
    "for image_path in image_paths_no_augm:\n",
    "    image_paths.append(image_path)\n",
    "    # print(image_path)\n",
    "    # image_ = cv2.imread(image_path)\n",
    "    # image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # masks = [value_i for key_i, value_i in ground_truth_masks.items() if image_path.split('/')[-1] in key_i]\n",
    "\n",
    "    # image_size = (image_.shape[0], image_.shape[1])\n",
    "    \n",
    "    # # Enlarge the bounding boxes\n",
    "    # bboxes_ = [enlarge_bbox(np.array([value_i[0], math.floor(value_i[1]),  math.floor(value_i[2] - value_i[0]), math.floor(value_i[3] - value_i[1])]), 5, image_size) \\\n",
    "    #            for key_i, value_i in bbox_coords.items() if image_path.split('/')[-1] in key_i]   \n",
    "\n",
    "    # label_ids = [1] * len(masks) # change this\n",
    "    # label_names = ['star'] # change this\n",
    "    \n",
    "    # img_negative_mask = (image_>0).astype(int)\n",
    "\n",
    "    # augmented1 = augment_and_show(geometrical_augmentations, image_, masks, bboxes_, label_ids, label_names, show_title=False)\n",
    "    # new_image_negative_mask = (augmented1['image']>0).astype(int)\n",
    "    # print(len(augmented1['masks']), len(augmented1['bboxes']))\n",
    "    \n",
    "    # # augmented2 = augment_and_show(intensity_color_augmentations, augmented1['image'],  augmented1['masks'],  augmented1['bboxes'], augmented1['category_id'], label_names, show_title=False)\n",
    "    \n",
    "    # # mask the transform which is derived from the geometric transform\n",
    "    # # augmented2['image'] = augmented2['image'] * new_image_negative_mask\n",
    "    \n",
    "    # augmented3 = augment_and_show(noise_blur_augmentations, image_, masks, bboxes_, label_ids, label_names, show_title=False)\n",
    "    \n",
    "    # # mask the transform using the image negative mask\n",
    "    # augmented3['image'] = augmented3['image'] * img_negative_mask\n",
    "        \n",
    "    # new_filename1 = image_path.replace('.'+image_path.split('.')[-1], '_augm1.jpg')\n",
    "    # new_filename2 = image_path.replace('.'+image_path.split('.')[-1], '_augm2.jpg')\n",
    "    # new_filename3 = image_path.replace('.'+image_path.split('.')[-1], '_augm3.jpg')\n",
    "\n",
    "    # augmented1 = align_masks_and_bboxes(augmented1)\n",
    "    # # augmented2 = align_masks_and_bboxes(augmented2)\n",
    "    # augmented3 = align_masks_and_bboxes(augmented3)\n",
    "\n",
    "    # update_dataset_with_augms(augmented1, new_filename1, bbox_coords, ground_truth_masks, image_paths)\n",
    "    # # update_dataset_with_augms(augmented2, new_filename2, bbox_coords, ground_truth_masks, image_paths)\n",
    "    # update_dataset_with_augms(augmented3, new_filename3, bbox_coords, ground_truth_masks, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cde032",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90482e51",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# augmented3['image'].shape, len(augmented3['bboxes']), len(augmented3['masks']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae50f7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    image_array = np.array(image) / 255.0\n",
    "\n",
    "    mean = image_array.mean()\n",
    "    std = image_array.std()\n",
    "\n",
    "    means.append(mean)\n",
    "    stds.append(std)\n",
    "\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920e435",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "np.mean(means), np.mean(stds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ca4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(means), np.std(stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f05a5e-9a88-4245-9ad7-c25fee0d9971",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bebae7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import math\n",
    "\n",
    "def split_list(input_list, percentages):\n",
    "    size = len(input_list)\n",
    "    idx = 0\n",
    "    output = []\n",
    "    for percentage in percentages:\n",
    "        chunk_size = round(percentage * size)\n",
    "        chunk = input_list[idx : idx + chunk_size]\n",
    "        output.append(chunk)\n",
    "        idx += chunk_size\n",
    "    return output\n",
    "\n",
    "def create_dataset(image_paths, ground_truth_masks, bbox_coords):\n",
    "        \n",
    "    d_gt_masks, d_bboxes = {}, {}\n",
    "    for img_path in image_paths:\n",
    "        id = img_path.split('/')[-1]\n",
    "        d_gt_masks.update({mask_id:mask_array for mask_id, mask_array in ground_truth_masks.items() if mask_id.startswith(id)})\n",
    "        d_bboxes.update({bbox_id:bbox for bbox_id, bbox in bbox_coords.items() if bbox_id.startswith(id)}) \n",
    "\n",
    "    return d_gt_masks, d_bboxes\n",
    "    \n",
    "training_size, test_size, val_size = (0.7, 0.2, 0.1)\n",
    "splits = split_list(image_paths, [training_size, test_size, val_size])\n",
    "training_image_paths, test_image_paths, val_image_paths = splits[0], splits[1], splits[2]\n",
    "\n",
    "train_gt_masks, train_bboxes = create_dataset(training_image_paths, ground_truth_masks, bbox_coords)\n",
    "test_gt_masks, test_bboxes = create_dataset(test_image_paths, ground_truth_masks, bbox_coords)\n",
    "val_gt_masks, val_bboxes = create_dataset(val_image_paths, ground_truth_masks, bbox_coords)\n",
    "\n",
    "\n",
    "del ground_truth_masks, bbox_coords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d708a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "len(training_image_paths), len(test_image_paths), len(val_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecc210-92df-4d7d-871e-90f010deaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba6afe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_id2 = image_paths[0].split('/')[-1]\n",
    "image_masks_ids = [key for key in train_gt_masks.keys() if key.startswith(image_id2)]\n",
    "image_ = cv2.imread(image_paths[0])\n",
    "image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(image_)\n",
    "for name in image_masks_ids:\n",
    "        show_box(train_bboxes[name], plt.gca())\n",
    "        show_mask(train_gt_masks[name], plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fb7c3-b24f-428e-b335-7d6942afbfba",
   "metadata": {},
   "source": [
    "## ðŸš€ Prepare Mobile SAM Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb250bdf",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/')\n",
    "import ft_mobile_sam\n",
    "from ft_mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/weights/mobile_sam.pt\"\n",
    "device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device);\n",
    "mobile_sam_model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae6411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "\n",
    "if use_wandb:\n",
    "    from datetime import datetime\n",
    "    # !pip install wandb\n",
    "    # !wandb login --relogin\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=\"OM_AI_v1\", name=f\"ft_MobileSAM {datetime.now()}\")\n",
    "\n",
    "    wandb.watch(mobile_sam_model, log='all', log_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5b948-8325-438c-979e-de49ede31b70",
   "metadata": {},
   "source": [
    "# Convert the input images into a format SAM's internal functions expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0d674",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import segment_anything\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "from importlib import reload\n",
    "import astronomy_utils\n",
    "reload(astronomy_utils)\n",
    "from astronomy_utils import *\n",
    "\n",
    "transform = ResizeLongestSide(mobile_sam_model.image_encoder.img_size)\n",
    "\n",
    "def transform_image(image, k):\n",
    "       \n",
    "        # sets a specific mean for each image\n",
    "        image_T = np.transpose(image, (2, 1, 0))\n",
    "        mean_ = np.mean(image_T[image_T>0])\n",
    "        std_ = np.std(image_T[image_T>0]) \n",
    "        pixel_mean = torch.as_tensor([mean_, mean_, mean_], dtype=torch.float, device=device)\n",
    "        pixel_std = torch.as_tensor([std_, std_, std_], dtype=torch.float, device=device)\n",
    "        \n",
    "        mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
    "\n",
    "        transformed_data = {}\n",
    "        negative_mask = np.where(image > 0, True, False)\n",
    "        negative_mask = torch.from_numpy(negative_mask)  \n",
    "        negative_mask = negative_mask.permute(2, 0, 1)\n",
    "        negative_mask = resize(negative_mask, [1024, 1024], antialias=True) \n",
    "        negative_mask = negative_mask.unsqueeze(0)\n",
    "        # scales the image to 1024x1024 by longest side \n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, dtype=torch.float32, device=device)\n",
    "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "        \n",
    "        # normalization and padding\n",
    "        input_image = mobile_sam_model.preprocess(transformed_image)\n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(transformed_image.shape[-2:])\n",
    "        input_image[~negative_mask] = 0\n",
    "        transformed_data['image'] = input_image.clone() \n",
    "        transformed_data['input_size'] = input_size \n",
    "        transformed_data['image_id'] = k\n",
    "        transformed_data['original_image_size'] = original_image_size\n",
    "    \n",
    "        return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60b93e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, bbox_coords, gt_masks, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.bbox_coords = bbox_coords\n",
    "        self.gt_masks = gt_masks\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_paths[idx].split(\"/\")[-1]\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "        # plt.imshow(image)\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        if self.transform is not None:\n",
    "            return self.transform(image, img_id)\n",
    "        else:\n",
    "            print(\"â—ï¸No transformâ—ï¸\")\n",
    "            return image\n",
    "        \n",
    "batch_size = 1\n",
    "dataset = ImageDataset(training_image_paths[1:2], train_bboxes, train_gt_masks, transform_image) \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab59ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_paths[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1300c",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() \n",
    "\n",
    "# print(torch.cuda.memory_summary(device=device, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd9d65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "lr=1e-4\n",
    "wd=0.0\n",
    "optimizer = torch.optim.AdamW(mobile_sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-8) # not very helpful\n",
    "\n",
    "def dice_loss(pred, target, negative_mask = None, area=None, smooth = 1): \n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    \n",
    "    if negative_mask is not None:\n",
    "        negative_mask = negative_mask.bool()\n",
    "        pred = pred * negative_mask\n",
    "        target = target * negative_mask \n",
    "        \n",
    "    # fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    # axs[0].imshow(pred[0, 0].cpu().detach().numpy(), cmap='gray')\n",
    "    # axs[0].set_title('Pred')\n",
    "    # axs[1].imshow(target[0, 0].cpu().detach().numpy(), cmap='gray')\n",
    "    # axs[1].set_title('Target')\n",
    "    # axs[2].imshow(negative_mask[0, 0].cpu().detach().numpy(), cmap='gray')\n",
    "    # axs[2].set_title('Negative Mask')\n",
    "    # plt.show()\n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "        \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "print(torch.cuda.memory_reserved()/(1024**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf128acc-595e-4618-9f6a-33936e0b08c6",
   "metadata": {},
   "source": [
    "## Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516038e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    if \"mask_decoder\" in name:\n",
    "    # if False:\n",
    "    # layers_to_fine_tune = ['mask_decoder.output_hypernetworks_mlps','mask_decoder.iou_prediction_head', 'mask_decoder.output_upscaling', \\\n",
    "    #                        'mask_decoder.mask_tokens', 'mask_decoder.iou_token']\n",
    "    # if any(s in name for s in layers_to_fine_tune): # or \"image_encoder.patch_embed\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e53781",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def check_requires_grad(model, show=True):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and show:\n",
    "            print(\"âœ… Param\", name, \" requires grad.\")\n",
    "        elif param.requires_grad == False:\n",
    "            print(\"âŒ Param\", name, \" doesn't require grad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb84f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"ðŸš€ The model has {sum(p.numel() for p in mobile_sam_model.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "check_requires_grad(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ade316-c813-48df-895f-1f4988444685",
   "metadata": {},
   "source": [
    "## The classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e99de-4f60-4f36-a65e-07c92af431fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ExtendedSAMModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 512), # TODO: use nn.AdaptiveAvgPool2d layer, as it is more robust to image size changes\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, mask):\n",
    "        # Flatten and concatenate mask and IOU predictions\n",
    "        flattened_mask = mask.view(-1)\n",
    "\n",
    "        # Pass through the classifier\n",
    "        class_logits = self.classifier(flattened_mask)\n",
    "        return class_logits\n",
    "\n",
    "num_classes = len(class_categories.values())\n",
    "extended_model = ExtendedSAMModel(512*512*3, num_classes).to(device) # TODO: chaneg this hard-coded value to work on other inputs\n",
    "\n",
    "\n",
    "# class ExtendedSAMModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super().__init__()\n",
    "      \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(0, 512),  # Placeholder, will be reset in forward (at runtime)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, mask, iou_predictions):\n",
    "\n",
    "#         flattened_mask = mask.squeeze(0).squeeze(0).view(-1)\n",
    "#         iou_predictions_flat = iou_predictions.view(-1)  \n",
    "\n",
    "#         # print(flattened_mask.shape, iou_predictions.shape)\n",
    "#         # Concatenate the flatten mask with IOU predictions\n",
    "#         combined_features = torch.cat((flattened_mask, iou_predictions_flat), dim=0).to(device)\n",
    "#         combined_features_size = combined_features.shape[0]\n",
    "\n",
    "#         # Reset the first layer of the classifier to match the input size\n",
    "#         self.classifier[0] = nn.Linear(combined_features_size, 512).to(device)\n",
    "\n",
    "#         class_logits = self.classifier(combined_features).to(device)\n",
    "#         return class_logits\n",
    "\n",
    "# num_classes = len(class_categories.values())\n",
    "# extended_model = ExtendedSAMModel(num_classes).to(device)\n",
    "# extended_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b9bbf-f058-4d69-b445-6ed36fe47eb5",
   "metadata": {},
   "source": [
    "## Run fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37018f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "len(train_gt_masks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c07950",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# **idea** can use mixed-precision training:\n",
    "# This is a technique that involves using a mix of float16 and float32 tensors \n",
    "# to make the model use less memory and run faster. \n",
    "# PyTorch provides the torch.cuda.amp module for automatic mixed-precision training.\n",
    "# from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bccbe5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def validate_model():\n",
    "    validation_loss = []\n",
    "    total_area= 0.0\n",
    "    with torch.no_grad():  \n",
    "        for k in val_gt_masks.keys():\n",
    "            prompt_box = np.array(val_bboxes[k])\n",
    "            box = predictor.transform.apply_boxes(prompt_box, original_image_size)\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "            box_torch = box_torch[None, :]\n",
    "            \n",
    "            mask_input_torch = torch.as_tensor(val_gt_masks[k], dtype=torch.float, device=device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            point_coords = np.array([(val_bboxes[k][2]+val_bboxes[k][0])/2.0, (val_bboxes[k][3]+val_bboxes[k][1])/2.0])\n",
    "            point_labels = np.array([1])\n",
    "            point_coords = predictor.transform.apply_coords(point_coords, original_image_size)\n",
    "            coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=device).unsqueeze(0)\n",
    "            labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=device)\n",
    "            coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n",
    "\n",
    "            sparse_embeddings, dense_embeddings = mobile_sam_model.prompt_encoder(\n",
    "              points=(coords_torch,labels_torch),\n",
    "              boxes=box_torch,\n",
    "              masks=None, \n",
    "            )\n",
    "\n",
    "            low_res_masks, iou_predictions = mobile_sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=mobile_sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "            )\n",
    "            \n",
    "            downscaled_masks = mobile_sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "            binary_mask = torch.sigmoid(downscaled_masks - mobile_sam_model.mask_threshold)\n",
    "            \n",
    "            numpy_binary_mask = binary_mask.detach().cpu().numpy()\n",
    "            gt_mask_resized = torch.from_numpy(np.resize(val_gt_masks[k], (1, 1, val_gt_masks[k].shape[0], val_gt_masks[k].shape[1]))).to(device)\n",
    "            gt_binary_mask = torch.as_tensor(gt_mask_resized >0, dtype=torch.float32) \n",
    "            numpy_gt_binary_mask = gt_binary_mask.contiguous().detach().cpu().numpy()\n",
    "            \n",
    "            mask_area = np.sum(val_gt_masks[k])\n",
    "            validation_loss.append(dice_loss(binary_mask, gt_binary_mask, negative_mask, mask_area) * mask_area)\n",
    "            total_area += mask_area\n",
    "\n",
    "            # print(f'val one image{k}: Allocated memory:', torch.cuda.memory_allocated()/(1024**2), 'MB. Reserved memory:', torch.cuda.memory_reserved()/(1024**2), 'MB')\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # # Ground truth mask\n",
    "            # axs[0].imshow(numpy_gt_binary_mask[0, 0], cmap='viridis')\n",
    "            # axs[0].set_title('Ground Truth Mask')\n",
    "            \n",
    "            # # Predicted mask\n",
    "            # axs[1].imshow(numpy_binary_mask[0, 0], cmap='viridis')\n",
    "            # axs[1].set_title('Predicted Mask')\n",
    "\n",
    "            # plt.show()\n",
    "            # plt.close()\n",
    "\n",
    "    validation_loss = torch.sum(validation_loss)\n",
    "    validation_loss /= total_area\n",
    "\n",
    "    return validation_loss\n",
    "\n",
    "import predictor_utils\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "\n",
    "def validate_model_AMG(ft_mobile_sam_model, mobile_sam_model_orig):\n",
    "    validation_loss = []\n",
    "\n",
    "    for val_img_path in val_image_paths:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            print('Original MobileSAM')\n",
    "            annotated_image_orig_mobile_sam, annotated_image_orig_mobile_sam_loss = amg_predict(mobile_sam_model_orig, orig_mobile_SamAutomaticMaskGenerator, \\\n",
    "                                                       ground_truth_masks, 'ft_MobileSAM', val_img_path, mask_on_negative=False, show_plot=True)\n",
    "\n",
    "            print('Fine-tuned MobileSAM')\n",
    "            annotated_img, loss = amg_predict(mobile_sam_model, SamAutomaticMaskGenerator, val_gt_masks, 'ft_MobileSAM', \\\n",
    "                                              val_img_path, mask_on_negative=True, show_plot=True)\n",
    "            # print('validation loss =', loss)\n",
    "            validation_loss.append(loss)\n",
    "\n",
    "    return np.mean(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_image_predict(image_masks):\n",
    "    image_loss=[]\n",
    "    mask_result = []\n",
    "    total_area = 0.0\n",
    "    ce_loss = 0.0\n",
    "    for k in image_masks:   \n",
    "\n",
    "                # process bboxes\n",
    "                prompt_box = np.array(train_bboxes[k])\n",
    "                box = predictor.transform.apply_boxes(prompt_box, original_image_size)\n",
    "                box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "                box_torch = box_torch[None, :]\n",
    "\n",
    "                # process masks\n",
    "                mask_input_torch = torch.as_tensor(train_gt_masks[k], dtype=torch.float, device=predictor.device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                # process coords and labels\n",
    "                x_min, y_min, x_max, y_max = train_bboxes[k]\n",
    "                x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "        \n",
    "                point_coords = np.array([(train_bboxes[k][2]+train_bboxes[k][0])/2.0, (train_bboxes[k][3]+train_bboxes[k][1])/2.0])\n",
    "                point_labels = np.array([1])\n",
    "                point_coords = predictor.transform.apply_coords(point_coords, original_image_size)\n",
    "                coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=predictor.device).unsqueeze(0)\n",
    "                labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=predictor.device)\n",
    "                coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n",
    "\n",
    "                # mask_copy = train_gt_masks[k].copy()\n",
    "                # mask_copy = cv2.cvtColor(mask_copy, cv2.COLOR_GRAY2BGR)\n",
    "                # mask_copy = mask_copy * 255\n",
    "                # print(x_min, y_min, x_max, y_max)\n",
    "                # cv2.rectangle(mask_copy, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                # plt.imshow(mask_copy)\n",
    "                # plt.plot((train_bboxes[k][2]+train_bboxes[k][0])/2.0, (train_bboxes[k][3]+train_bboxes[k][1])/2.0, 'ro')\n",
    "                # plt.show()\n",
    "\n",
    "                sparse_embeddings, dense_embeddings = mobile_sam_model.prompt_encoder(\n",
    "                  points=(coords_torch, labels_torch),\n",
    "                  boxes=box_torch, #None,\n",
    "                  masks=None,\n",
    "                )\n",
    "                # print(box_torch.shape, mask_input_torch.shape)\n",
    "\n",
    "                del box_torch, coords_torch, labels_torch\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # print(image_embedding.shape, mobile_sam_model.prompt_encoder.get_dense_pe().shape, sparse_embeddings.shape, dense_embeddings.shape)\n",
    "                low_res_masks, iou_predictions = mobile_sam_model.mask_decoder(\n",
    "                image_embeddings=image_embedding,\n",
    "                image_pe=mobile_sam_model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=True, # this works better for ambiguous prompts (single points)\n",
    "                )\n",
    "        \n",
    "                # print(low_res_masks.shape)\n",
    "\n",
    "                downscaled_masks = mobile_sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "                # binary_mask = normalize(threshold(downscaled_masks, 0.0, 0))\n",
    "                binary_mask = torch.sigmoid(downscaled_masks - mobile_sam_model.mask_threshold)\n",
    "                gt_mask_resized = torch.from_numpy(np.resize(train_gt_masks[k], (1, 1, train_gt_masks[k].shape[0], train_gt_masks[k].shape[1]))).to(device)\n",
    "                gt_binary_mask = torch.as_tensor(gt_mask_resized>0, dtype=torch.float32) \n",
    "                numpy_gt_binary_mask = gt_binary_mask.contiguous().detach().cpu().numpy()\n",
    "\n",
    "                # mask = np.expand_dims(gt_binary_mask, axis=-1)\n",
    "                # mask_input_torch = torch.as_tensor(mask, dtype=torch.float, device=device)\n",
    "                # mask_input_torch = mask_input_torch / mask_input_torch.max()\n",
    "                input_image = torch.as_tensor(image, dtype=torch.float, device=device)\n",
    "                input_image = input_image / input_image.max()\n",
    "\n",
    "                # CE loss over mask\n",
    "                class_output = extended_model(mask_input_torch[0].permute(1,2,0) * input_image).unsqueeze(0) # also take the image pixels into account\n",
    "\n",
    "                # class_output = extended_model(binary_mask, iou_predictions).unsqueeze(0)\n",
    "                # print('class_output', class_output)\n",
    "                gt_class = torch.as_tensor([classes[k]], dtype=torch.long, device=device)\n",
    "                # find the index where the max value is in the class_output\n",
    "                # max_index = torch.argmax(class_output)\n",
    "                # print('class_output shape:', class_output, 'gt class', gt_class)\n",
    "                ce_loss += ce_loss_fn(class_output, gt_class)\n",
    "                # print('CE loss:', ce_loss)\n",
    "\n",
    "                # print(iou_predictions)\n",
    "                mask_result.append(binary_mask[0][0].detach().cpu())\n",
    "\n",
    "                # plt.imshow(numpy_gt_binary_mask[0][0])\n",
    "                # plt.show()\n",
    "        \n",
    "                # compute weighted dice loss\n",
    "                # print(train_gt_masks[k].shape,np.sum(train_gt_masks[k]))\n",
    "                mask_area = np.sum(train_gt_masks[k])\n",
    "\n",
    "                if mask_area > 0:\n",
    "                  image_loss.append(dice_loss(binary_mask, gt_binary_mask, negative_mask, mask_area) * mask_area)\n",
    "                  total_area += mask_area\n",
    "\n",
    "                del binary_mask \n",
    "                del gt_mask_resized, numpy_gt_binary_mask \n",
    "                del low_res_masks, iou_predictions \n",
    "        \n",
    "                torch.cuda.empty_cache()\n",
    "                # print('one mask', k, 'done')\n",
    "                # print(f'one image{k}: Allocated memory:', torch.cuda.memory_allocated()/(1024**2), 'MB. Reserved memory:', torch.cuda.memory_reserved()/(1024**2), 'MB')\n",
    "\n",
    "    image_loss = torch.stack(image_loss)\n",
    "    image_loss = torch.sum(image_loss)\n",
    "    image_loss /= total_area\n",
    "    if len(image_masks) > 0:\n",
    "        ce_loss /= len(image_masks)\n",
    "    # image_loss = (image_loss + ce_loss)/2.0\n",
    "    \n",
    "    plt.figure(figsize=(40, 20))\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "    axs[0].imshow(255-image)\n",
    "    # show_masks(mask_result, axs[0], random_color=False)\n",
    "    axs[0].set_title(f'{idd.split(\".\")[0]}', fontsize=10)\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(255-image)\n",
    "    show_masks(mask_result, axs[0], random_color=False)\n",
    "    axs[1].set_title('Predicted masks', fontsize=10)\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    axs[2].imshow(255-image)\n",
    "    show_masks([train_gt_masks[k] for k in image_masks], axs[1], random_color=False)\n",
    "    axs[2].set_title('Ground truth masks', fontsize=10)\n",
    "    axs[2].axis('off')\n",
    "    plt.savefig('on_img_train.png', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return image_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as patches\n",
    "import copy\n",
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "losses = []\n",
    "valid_losses = []\n",
    "valid_bboxes_losses = []\n",
    "predictor = SamPredictor(mobile_sam_model)\n",
    "\n",
    "idd = 0\n",
    "for epoch in range(num_epochs):\n",
    "\t\n",
    "\tepoch_losses = []\n",
    "\tfor inputs in tqdm(dataloader): # take image ids\n",
    "\t\timage_loss = {}\n",
    "\t\tbatch_loss = 0.0\n",
    "\t\tbatch_size = min(len(inputs['image']), batch_size) # at the end, there's less than batch_size imgs in a batch (sometimes)\n",
    "\t\n",
    "\t\tfor i in range(batch_size): \n",
    "\t\t\timage_loss[i]=0.0\n",
    "\t\t\t  \n",
    "\t\t\timage_masks = {k for k in train_gt_masks.keys() if k.startswith(inputs['image_id'][i])}\n",
    "\t\t\tidd = inputs['image_id'][i]\n",
    "\t\t\tprint(inputs['image_id'][i])\n",
    "\t\t\tinput_image = inputs['image'][i].to(device)\n",
    "            \n",
    "\t\t\timage = cv2.imread(input_dir+inputs['image_id'][i])\n",
    "\t\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "\t\t\toriginal_image_size = image.shape[:-1]\n",
    "\t\t\tinput_size = (1024, 1024)\n",
    "\t\t\t# np_image = np.transpose(input_image[0].detach().cpu().numpy(), (1, 2, 0))\n",
    "\t\t\t# print(np.min(np_image), np.max(np_image))\n",
    "\t\t\t# plt.imshow(input_image[0][0].detach().cpu(), cmap='viridis')\n",
    "\t\t\t# plt.title(f'{inputs[\"image_id\"][i]}.png')\n",
    "\t\t\t# plt.show()\n",
    "\n",
    "            # IMAGE ENCODER\n",
    "\t\t\timage_embedding = mobile_sam_model.image_encoder(input_image)\n",
    "\t\t\t   \n",
    "\t\t\t# negative_mask has the size of the image\n",
    "\t\t\tnegative_mask = np.where(image>0, True, False)\n",
    "\t\t\tnegative_mask = torch.from_numpy(negative_mask)  \n",
    "\t\t\tnegative_mask = negative_mask.permute(2, 0, 1)\n",
    "\t\t\tnegative_mask = negative_mask[0]\n",
    "\t\t\tnegative_mask = negative_mask.unsqueeze(0).unsqueeze(0)\n",
    "\t\t\tnegative_mask = negative_mask.to(device)\n",
    "\t\t\t# del image\n",
    "\n",
    "            # RUN PREDICTION ON IMAGE\n",
    "\t\t\timage_loss[i] = one_image_predict(image_masks)\n",
    "\t\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t\t\tbatch_loss += image_loss[i]\n",
    "\t\t\tdel image_embedding, negative_mask, input_image\n",
    "        \n",
    "\t\tbatch_loss = batch_loss * 1.0/batch_size\n",
    "\t\tepoch_losses.append(batch_loss.item())\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tbatch_loss.backward()\n",
    "\t\toptimizer.step() \n",
    "\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t# validation loss\n",
    "\tlosses.append(np.mean(epoch_losses))\n",
    "\t# mobile_sam_model.eval();\n",
    "\t# # valid_losses.append(validate_model_AMG())\n",
    "\t# # valid_bboxes_losses.append(validate_model())\n",
    "\t# mobile_sam_model.train();\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "\tif use_wandb:\n",
    "\t\twandb.log({'epoch training loss': np.mean(epoch_losses)})\n",
    "\t\twandb.log({'epoch validation loss': np.mean(valid_bboxes_losses)})\n",
    "\t# print(f'EPOCH: {epoch}. Training loss: {np.mean(epoch_losses)}. Validation AMG loss: {np.mean(valid_losses)}. Validation bboxes loss: {np.mean(valid_bboxes_losses)} ')\n",
    "\tprint(f'EPOCH: {epoch}. Training loss: {np.mean(epoch_losses)}.Validation bboxes loss: {np.mean(valid_bboxes_losses)} ')\n",
    "\t# print(f'EPOCH: {epoch}. Training loss: {np.mean(epoch_losses)}.')\n",
    "\n",
    "# torch.save(mobile_sam_model.state_dict(), 'mobile_sam_model_checkpoint.pth')\n",
    "\n",
    "print('Training losses:', losses)\n",
    "print('valid_bboxes_losses losses:', valid_bboxes_losses)\n",
    "# print('Val losses:', valid_losses)\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.run.summary[\"batch_size\"] = batch_size\n",
    "    wandb.run.summary[\"num_epochs\"] = num_epochs\n",
    "    wandb.run.summary[\"learning rate\"] = lr\n",
    "    wandb.run.summary[\"used area for DICE loss\"] = 'YES'\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a81ab-fc36-4405-902b-e2cedcbad6ec",
   "metadata": {},
   "source": [
    "**original MobileSAM checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e915e-0965-4edc-ab37-f4c2dafcace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/')\n",
    "import mobile_sam\n",
    "from mobile_sam import sam_model_registry as orig_mobile_sam_registry, \\\n",
    "SamAutomaticMaskGenerator as orig_mobile_SamAutomaticMaskGenerator, \\\n",
    "SamPredictor as orig_mobile_SamPredictor\n",
    "\n",
    "orig_mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-master/weights/mobile_sam.pt\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model_orig = orig_mobile_sam_registry[\"vit_t\" ](checkpoint=orig_mobile_sam_checkpoint)\n",
    "mobile_sam_model_orig.to(device);\n",
    "mobile_sam_model_orig.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709069a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_model_AMG(mobile_sam_model, mobile_sam_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93b832-c31d-4fd5-b927-f69d6e2e12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('valid_losses', valid_losses)\n",
    "print('valid_bboxes_losses', valid_bboxes_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the GPU memory after training\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "print(torch.cuda.memory_reserved()/(1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371097d",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(losses))), losses, label='Training Loss')\n",
    "plt.plot(list(range(len(valid_bboxes_losses))), valid_bboxes_losses, label='Validation Loss')\n",
    "plt.title('Mean epoch loss \\n mask with sigmoid')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/loss_mask_valid_points_prompt.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(range(len(losses))), losses)\n",
    "# plt.title('Mean epoch loss \\n mask with sigmoid')\n",
    "# plt.xlabel('Epoch Number')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.savefig('loss_mask_sigmoid_training.png')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c7008-f153-4d07-bb98-c5b505562fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2aa372-205e-424a-b7cb-979d870c27dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1766f-f1f2-4033-9842-59b727481742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
