{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba08a5eb-cf5f-4b9c-bf97-94e34c5b061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "import json\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "# # Ensure deterministic behavior (cannot control everything though)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# torch.cuda.manual_seed_all(42)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4219539e-4dc4-4e78-8866-6a0c46f1ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = '/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6982136-6a41-43a1-908e-e692e3c04ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c6f101-eab2-4ffa-ae57-5b24d19ccfdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/tinyvit/tiny_vit.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with tinyvit.tiny_vit.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/tinyvit/tiny_vit.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with tinyvit.tiny_vit.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/tinyvit/tiny_vit.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with tinyvit.tiny_vit.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/tinyvit/tiny_vit.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with tinyvit.tiny_vit.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/tinyvit/tiny_vit.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with tinyvit.tiny_vit.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/')\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from mobilesamv2.promt_mobilesamv2 import ObjectAwareModel\n",
    "from mobilesamv2 import sam_model_registry, SamPredictor\n",
    "from typing import Any, Dict, Generator,List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_model():\n",
    "    Prompt_guided_path=dir_+'PromptGuidedDecoder/Prompt_guided_Mask_Decoder.pt'\n",
    "    obj_model_path=dir_+'weight/ObjectAwareModel.pt'\n",
    "    ObjAwareModel = ObjectAwareModel(obj_model_path)\n",
    "    PromptGuidedDecoder=sam_model_registry['PromptGuidedDecoder'](Prompt_guided_path)\n",
    "    mobilesamv2 = sam_model_registry['vit_h']()\n",
    "    mobilesamv2.prompt_encoder=PromptGuidedDecoder['PromtEncoder']\n",
    "    mobilesamv2.mask_decoder=PromptGuidedDecoder['MaskDecoder']\n",
    "    return mobilesamv2,ObjAwareModel\n",
    "    \n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((anns.shape[1], anns.shape[2], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in range(anns.shape[0]):\n",
    "        m = anns[ann].bool()\n",
    "        m=m.cpu().numpy()\n",
    "        color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def batch_iterator(batch_size: int, *args) -> Generator[List[Any], None, None]:\n",
    "    assert len(args) > 0 and all(\n",
    "        len(a) == len(args[0]) for a in args\n",
    "    ), \"Batched iteration must have inputs of all the same size.\"\n",
    "    n_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)\n",
    "    for b in range(n_batches):\n",
    "        yield [arg[b * batch_size : (b + 1) * batch_size] for arg in args]\n",
    "\n",
    "encoder_path={'efficientvit_l2':dir_+'weight/l2.pt',\n",
    "            'tiny_vit':dir_+'weight/mobile_sam.pt',\n",
    "            'sam_vit_h':dir_+'weight/sam_vit_h.pt',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e6c1c1f-c33d-4773-ace9-716037c58885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    ObjectAwareModel_path = dir_+'PromptGuidedDecoder/ObjectAwareModel.pt'\n",
    "    Prompt_guided_Mask_Decoder_path = dir_+'PromptGuidedDecoder/Prompt_guided_Mask_Decoder.pt'\n",
    "    encoder_path = dir_\n",
    "    img_path = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/zscaled_512_stretched/'\n",
    "    imgsz = 512\n",
    "    iou = 0.9\n",
    "    conf = 0.4\n",
    "    retina = False\n",
    "    output_dir = '/workspace/raid/OM_DeepLearning/new_temp/'\n",
    "    encoder_type = 'tiny_vit'  # choose from ['tiny_vit','sam_vit_h','mobile_sam','efficientvit_l2','efficientvit_l1','efficientvit_l0']\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# import pdb;pdb.set_trace()\n",
    "output_dir=args.output_dir  \n",
    "mobilesamv2, ObjAwareModel=create_model()\n",
    "image_encoder=sam_model_registry[args.encoder_type](encoder_path[args.encoder_type])\n",
    "mobilesamv2.image_encoder=image_encoder\n",
    "device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobilesamv2.to(device=device)\n",
    "# mobilesamv2.eval()\n",
    "predictor = SamPredictor(mobilesamv2)\n",
    "image_files= os.listdir(args.img_path)\n",
    "\n",
    "for name, param in mobilesamv2.named_parameters():\n",
    "    params_to_train = ['mask_tokens', 'output_upscaling', 'output_hypernetworks_mlps', 'iou_prediction_head']\n",
    "    # if 'mask_decoder' in name: # and any(s in name for s in params_to_train):\n",
    "    if True:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a770684-7f36-4310-b560-cb6e3a9129cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ The model has 10130092 trainable parameters.\n",
      "\n",
      "âœ… Param image_encoder.patch_embed.seq.0.c.weight  requires grad.\n",
      "âœ… Param image_encoder.patch_embed.seq.0.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.patch_embed.seq.0.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.patch_embed.seq.2.c.weight  requires grad.\n",
      "âœ… Param image_encoder.patch_embed.seq.2.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.patch_embed.seq.2.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv1.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv1.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv1.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv2.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv2.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv2.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv3.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv3.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.0.conv3.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv1.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv1.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv1.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv2.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv2.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv2.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv3.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv3.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.blocks.1.conv3.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv1.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv1.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv1.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv2.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv2.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv2.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv3.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv3.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.0.downsample.conv3.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.0.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.blocks.1.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv1.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv1.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv1.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv2.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv2.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv2.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv3.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv3.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.1.downsample.conv3.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.0.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.1.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.2.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.3.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.4.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.blocks.5.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv1.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv1.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv1.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv2.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv2.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv2.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv3.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv3.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.2.downsample.conv3.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.0.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.attn.attention_biases  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.attn.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.attn.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.attn.qkv.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.attn.qkv.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.attn.proj.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.attn.proj.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.mlp.norm.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.mlp.norm.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.mlp.fc1.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.mlp.fc1.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.mlp.fc2.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.mlp.fc2.bias  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.local_conv.c.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.local_conv.bn.weight  requires grad.\n",
      "âœ… Param image_encoder.layers.3.blocks.1.local_conv.bn.bias  requires grad.\n",
      "âœ… Param image_encoder.norm_head.weight  requires grad.\n",
      "âœ… Param image_encoder.norm_head.bias  requires grad.\n",
      "âœ… Param image_encoder.head.weight  requires grad.\n",
      "âœ… Param image_encoder.head.bias  requires grad.\n",
      "âœ… Param image_encoder.neck.0.weight  requires grad.\n",
      "âœ… Param image_encoder.neck.1.weight  requires grad.\n",
      "âœ… Param image_encoder.neck.1.bias  requires grad.\n",
      "âœ… Param image_encoder.neck.2.weight  requires grad.\n",
      "âœ… Param image_encoder.neck.3.weight  requires grad.\n",
      "âœ… Param image_encoder.neck.3.bias  requires grad.\n",
      "âœ… Param prompt_encoder.point_embeddings.0.weight  requires grad.\n",
      "âœ… Param prompt_encoder.point_embeddings.1.weight  requires grad.\n",
      "âœ… Param prompt_encoder.point_embeddings.2.weight  requires grad.\n",
      "âœ… Param prompt_encoder.point_embeddings.3.weight  requires grad.\n",
      "âœ… Param prompt_encoder.not_a_point_embed.weight  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.0.weight  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.0.bias  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.1.weight  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.1.bias  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.3.weight  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.3.bias  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.4.weight  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.4.bias  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.6.weight  requires grad.\n",
      "âœ… Param prompt_encoder.mask_downscaling.6.bias  requires grad.\n",
      "âœ… Param prompt_encoder.no_mask_embed.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.self_attn.q_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.self_attn.q_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.self_attn.k_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.self_attn.k_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.self_attn.v_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.self_attn.v_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.self_attn.out_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.self_attn.out_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.norm1.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.norm1.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.norm2.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.norm2.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.mlp.lin1.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.mlp.lin1.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.mlp.lin2.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.mlp.lin2.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.norm3.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.norm3.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.norm4.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.norm4.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.self_attn.q_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.self_attn.q_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.self_attn.k_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.self_attn.k_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.self_attn.v_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.self_attn.v_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.self_attn.out_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.self_attn.out_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.norm1.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.norm1.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.norm2.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.norm2.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.mlp.lin1.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.mlp.lin1.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.mlp.lin2.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.mlp.lin2.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.norm3.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.norm3.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.norm4.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.norm4.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.final_attn_token_to_image.q_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.final_attn_token_to_image.q_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.final_attn_token_to_image.k_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.final_attn_token_to_image.k_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.final_attn_token_to_image.v_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.final_attn_token_to_image.v_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.final_attn_token_to_image.out_proj.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.final_attn_token_to_image.out_proj.bias  requires grad.\n",
      "âœ… Param mask_decoder.transformer.norm_final_attn.weight  requires grad.\n",
      "âœ… Param mask_decoder.transformer.norm_final_attn.bias  requires grad.\n",
      "âœ… Param mask_decoder.iou_token.weight  requires grad.\n",
      "âœ… Param mask_decoder.mask_tokens.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_upscaling.0.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_upscaling.0.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_upscaling.1.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_upscaling.1.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_upscaling.3.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_upscaling.3.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.0.layers.0.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.0.layers.0.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.0.layers.1.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.0.layers.1.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.0.layers.2.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.0.layers.2.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.1.layers.0.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.1.layers.0.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.1.layers.1.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.1.layers.1.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.1.layers.2.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.1.layers.2.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.2.layers.0.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.2.layers.0.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.2.layers.1.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.2.layers.1.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.2.layers.2.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.2.layers.2.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.3.layers.0.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.3.layers.0.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.3.layers.1.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.3.layers.1.bias  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.3.layers.2.weight  requires grad.\n",
      "âœ… Param mask_decoder.output_hypernetworks_mlps.3.layers.2.bias  requires grad.\n",
      "âœ… Param mask_decoder.iou_prediction_head.layers.0.weight  requires grad.\n",
      "âœ… Param mask_decoder.iou_prediction_head.layers.0.bias  requires grad.\n",
      "âœ… Param mask_decoder.iou_prediction_head.layers.1.weight  requires grad.\n",
      "âœ… Param mask_decoder.iou_prediction_head.layers.1.bias  requires grad.\n",
      "âœ… Param mask_decoder.iou_prediction_head.layers.2.weight  requires grad.\n",
      "âœ… Param mask_decoder.iou_prediction_head.layers.2.bias  requires grad.\n"
     ]
    }
   ],
   "source": [
    "def check_requires_grad(model, show=True):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and show:\n",
    "            print(\"âœ… Param\", name, \" requires grad.\")\n",
    "        elif param.requires_grad == False:\n",
    "            print(\"âŒ Param\", name, \" doesn't require grad.\")\n",
    "\n",
    "print(f\"ðŸš€ The model has {sum(p.numel() for p in mobilesamv2.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "check_requires_grad(mobilesamv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f90dc26-6081-45b9-80b6-83f964f87449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from mobilesamv2.promt_mobilesamv2 import ObjectAwareModel\n",
    "from mobilesamv2 import sam_model_registry, SamPredictor\n",
    "from typing import Any, Dict, Generator,List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ObjectAwareModel_path\", type=str, default='./PromptGuidedDecoder/ObjectAwareModel.pt', help=\"ObjectAwareModel path\")\n",
    "    parser.add_argument(\"--Prompt_guided_Mask_Decoder_path\", type=str, default='./PromptGuidedDecoder/Prompt_guided_Mask_Decoder.pt', help=\"Prompt_guided_Mask_Decoder path\")\n",
    "    parser.add_argument(\"--encoder_path\", type=str, default=\"./\", help=\"select your own path\")\n",
    "    parser.add_argument(\"--img_path\", type=str, default=\"./test_images/\", help=\"path to image file\")\n",
    "    parser.add_argument(\"--imgsz\", type=int, default=1024, help=\"image size\")\n",
    "    parser.add_argument(\"--iou\",type=float,default=0.9,help=\"yolo iou\")\n",
    "    parser.add_argument(\"--conf\", type=float, default=0.4, help=\"yolo object confidence threshold\")\n",
    "    parser.add_argument(\"--retina\",type=bool,default=True,help=\"draw segmentation masks\",)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./\", help=\"image save path\")\n",
    "    parser.add_argument(\"--encoder_type\", choices=['tiny_vit','sam_vit_h','mobile_sam','efficientvit_l2','efficientvit_l1','efficientvit_l0'], help=\"choose the model type\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_model():\n",
    "    Prompt_guided_path=dir_+'PromptGuidedDecoder/Prompt_guided_Mask_Decoder.pt'\n",
    "    obj_model_path=dir_+'weight/ObjectAwareModel.pt'\n",
    "    ObjAwareModel = ObjectAwareModel(obj_model_path)\n",
    "    PromptGuidedDecoder=sam_model_registry['PromptGuidedDecoder'](Prompt_guided_path)\n",
    "    mobilesamv2 = sam_model_registry['vit_h']()\n",
    "    mobilesamv2.prompt_encoder=PromptGuidedDecoder['PromtEncoder']\n",
    "    mobilesamv2.mask_decoder=PromptGuidedDecoder['MaskDecoder']\n",
    "    return mobilesamv2,ObjAwareModel\n",
    "    \n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    # ax.set_autoscale_on(False)\n",
    "    img = np.ones((anns.shape[1], anns.shape[2], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in range(anns.shape[0]):\n",
    "        m = anns[ann].bool()\n",
    "        m=m.cpu().numpy()\n",
    "        color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def batch_iterator(batch_size: int, *args) -> Generator[List[Any], None, None]:\n",
    "    assert len(args) > 0 and all(\n",
    "        len(a) == len(args[0]) for a in args\n",
    "    ), \"Batched iteration must have inputs of all the same size.\"\n",
    "    n_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)\n",
    "    for b in range(n_batches):\n",
    "        yield [arg[b * batch_size : (b + 1) * batch_size] for arg in args]\n",
    "\n",
    "\n",
    "encoder_path={'efficientvit_l2':dir_+'weight/l2.pt',\n",
    "            'tiny_vit':dir_+'weight/mobile_sam.pt',\n",
    "            'sam_vit_h':dir_+'weight/sam_vit_h.pt',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdfbd199-59f6-4255-a23a-bfaf0c535510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** S0674810801_M.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x512 1 object, 8.9ms\n",
      "Speed: 5.5ms preprocess, 8.9ms inference, 6.8ms postprocess per image at shape (1, 3, 512, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.yolo.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.yolo.engine.results.Boxes object\n",
      "keypoints: None\n",
      "keys: ['boxes', 'masks']\n",
      "masks: ultralytics.yolo.engine.results.Masks object\n",
      "names: {0: 'object'}\n",
      "orig_img: array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8)\n",
      "orig_shape: (512, 512)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 5.461931228637695, 'inference': 8.927345275878906, 'postprocess': 6.816387176513672}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.81 GiB. GPU 3 has a total capacity of 39.39 GiB of which 2.44 GiB is free. Process 915220 has 36.95 GiB memory in use. Of the allocated memory 35.18 GiB is allocated by PyTorch, and 111.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m         plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     79\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[0;32m---> 80\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\u001b[39;00m\n\u001b[1;32m     47\u001b[0m sparse_embeddings, dense_embeddings \u001b[38;5;241m=\u001b[39m mobilesamv2\u001b[38;5;241m.\u001b[39mprompt_encoder(\n\u001b[1;32m     48\u001b[0m     points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m     boxes\u001b[38;5;241m=\u001b[39minput_boxes,\n\u001b[1;32m     50\u001b[0m     masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[0;32m---> 51\u001b[0m low_res_masks, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmobilesamv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimple_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m low_res_masks\u001b[38;5;241m=\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpostprocess_masks(low_res_masks, predictor\u001b[38;5;241m.\u001b[39minput_size, predictor\u001b[38;5;241m.\u001b[39moriginal_size)\n\u001b[1;32m     60\u001b[0m sam_mask_pre \u001b[38;5;241m=\u001b[39m (low_res_masks \u001b[38;5;241m>\u001b[39m mobilesamv2\u001b[38;5;241m.\u001b[39mmask_threshold)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/mobilesamv2/modeling/mask_decoder.py:103\u001b[0m, in \u001b[0;36mMaskDecoder.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output, simple_type)\u001b[0m\n\u001b[1;32m     96\u001b[0m     masks, iou_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_masks(\n\u001b[1;32m     97\u001b[0m         image_embeddings\u001b[38;5;241m=\u001b[39mimage_embeddings,\n\u001b[1;32m     98\u001b[0m         image_pe\u001b[38;5;241m=\u001b[39mimage_pe,\n\u001b[1;32m     99\u001b[0m         sparse_prompt_embeddings\u001b[38;5;241m=\u001b[39msparse_prompt_embeddings,\n\u001b[1;32m    100\u001b[0m         dense_prompt_embeddings\u001b[38;5;241m=\u001b[39mdense_prompt_embeddings,\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     masks, iou_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_masks_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multimask_output:\n\u001b[1;32m    110\u001b[0m     mask_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/mobilesamv2/modeling/mask_decoder.py:173\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks_simple\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    171\u001b[0m pos_src\u001b[38;5;241m=\u001b[39mimage_pe\n\u001b[1;32m    172\u001b[0m b, c, h, w \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 173\u001b[0m hs, src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m iou_token_out \u001b[38;5;241m=\u001b[39m hs[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    175\u001b[0m mask_tokens_out \u001b[38;5;241m=\u001b[39m hs[:, \u001b[38;5;241m1\u001b[39m : (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_mask_tokens), :]\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/mobilesamv2/modeling/transformer.py:92\u001b[0m, in \u001b[0;36mTwoWayTransformer.forward\u001b[0;34m(self, image_embedding, image_pe, point_embedding)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Apply transformer blocks and final layernorm\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 92\u001b[0m     queries, keys \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoint_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Apply the final attention layer from the points to the image\u001b[39;00m\n\u001b[1;32m    100\u001b[0m q \u001b[38;5;241m=\u001b[39m queries \u001b[38;5;241m+\u001b[39m point_embedding\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/MobileSAMv2/mobilesamv2/modeling/transformer.py:180\u001b[0m, in \u001b[0;36mTwoWayAttentionBlock.forward\u001b[0;34m(self, queries, keys, query_pe, key_pe)\u001b[0m\n\u001b[1;32m    178\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_image_to_token(q\u001b[38;5;241m=\u001b[39mk, k\u001b[38;5;241m=\u001b[39mq, v\u001b[38;5;241m=\u001b[39mqueries)\n\u001b[1;32m    179\u001b[0m keys \u001b[38;5;241m=\u001b[39m keys \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[0;32m--> 180\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m queries, keys\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/yolo_older_v/lib/python3.12/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.81 GiB. GPU 3 has a total capacity of 39.39 GiB of which 2.44 GiB is free. Process 915220 has 36.95 GiB memory in use. Of the allocated memory 35.18 GiB is allocated by PyTorch, and 111.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    # import pdb;pdb.set_trace()\n",
    "    output_dir=args.output_dir  \n",
    "    mobilesamv2, ObjAwareModel=create_model()\n",
    "    image_encoder=sam_model_registry[args.encoder_type](encoder_path[args.encoder_type])\n",
    "    mobilesamv2.image_encoder=image_encoder\n",
    "    device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "    mobilesamv2.to(device=device)\n",
    "    mobilesamv2.train()\n",
    "    predictor = SamPredictor(mobilesamv2)\n",
    "    image_files= os.listdir(args.img_path)\n",
    "    for image_name in image_files[:10]:\n",
    "        print('*****', image_name)\n",
    "        image = cv2.imread(args.img_path + image_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        obj_results = ObjAwareModel(image,device=device,retina_masks=args.retina,imgsz=args.imgsz,conf=args.conf,iou=args.iou)\n",
    "        if obj_results is None:\n",
    "            print(\"No mask here\")\n",
    "            continue\n",
    "        print(obj_results[0])\n",
    "        predictor.set_image(image)\n",
    "        input_boxes1 = obj_results[0].boxes.xyxy\n",
    "        input_boxes = input_boxes1.cpu().numpy()\n",
    "        input_boxes = predictor.transform.apply_boxes(input_boxes, predictor.original_size)\n",
    "        input_boxes = torch.from_numpy(input_boxes).to(device)\n",
    "        sam_mask=[]\n",
    "        image_embedding=predictor.features\n",
    "        image_embedding=torch.repeat_interleave(image_embedding, 720, dim=0)\n",
    "        prompt_embedding=mobilesamv2.prompt_encoder.get_dense_pe()\n",
    "        prompt_embedding=torch.repeat_interleave(prompt_embedding, 720, dim=0)\n",
    "        \n",
    "        # for (boxes,) in batch_iterator(320, input_boxes):\n",
    "            # with torch.no_grad():\n",
    "        image1 = np.zeros((1024, 1024, 3), dtype=np.uint8)\n",
    "        \n",
    "        for bbox in input_boxes:\n",
    "            x1, y1, x2, y2 = bbox.detach().cpu().numpy()\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv2.rectangle(image1, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green rectangle with thickness 2\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(image1)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\n",
    "        # prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\n",
    "        sparse_embeddings, dense_embeddings = mobilesamv2.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=input_boxes,\n",
    "            masks=None,)\n",
    "        low_res_masks, _ = mobilesamv2.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=prompt_embedding,\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "            simple_type=True,\n",
    "        )\n",
    "        low_res_masks=predictor.model.postprocess_masks(low_res_masks, predictor.input_size, predictor.original_size)\n",
    "        sam_mask_pre = (low_res_masks > mobilesamv2.mask_threshold)*1.0\n",
    "        sam_mask.append(sam_mask_pre.squeeze(1))\n",
    "        # print(sam_mask[0].shape)\n",
    "        sam_mask=torch.cat(sam_mask)\n",
    "        # print(sam_mask.shape)\n",
    "        annotation = sam_mask\n",
    "        areas = torch.sum(annotation, dim=(1, 2))\n",
    "        sorted_indices = torch.argsort(areas, descending=True)\n",
    "        show_img = annotation[sorted_indices]\n",
    "        plt.figure(figsize=(5,5))\n",
    "        background=np.ones_like(image)*255\n",
    "        plt.imshow(background)\n",
    "        # print('torch', show_img.shape)\n",
    "        show_anns(show_img)\n",
    "        plt.axis('off')\n",
    "        plt.show() \n",
    "        # plt.savefig(\"{}\".format(output_dir+image_name), bbox_inches='tight', pad_inches = 0.0) \n",
    "        plt.close()\n",
    "\n",
    "args = Args()\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
