{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xFZSDikKMsA"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwmQm0C3n_3D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qwPYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch import cuda\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# # wandb.login()\n",
    "# !wandb login --relogin\n",
    "# try:\n",
    "#     run = wandb.init(project=\"OM_AI_v1\", name=\"fine-tuning2\")\n",
    "# except wandb.CommError as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code/one_image_mask_test1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 1 image training process.\n",
    "\n",
    "The annotations are imported from Roboflow. Each mask is a dict of keys:\n",
    "\n",
    "* `id` - annotation id\n",
    "* `image_id` - corresponding image id\n",
    "* `category_id` - class id\n",
    "* `bbox` - XYHW coordinates\n",
    "* `area` - area of the bbox\n",
    "* `segmentation` `[List[float]]` - polygon coordinates of the mask\n",
    "* `iscrowd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import cv2\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from matplotlib.path import Path\n",
    "import math\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code/OM_sky_images-6/valid/'\n",
    "json_file_path = input_dir+'_annotations.coco.json'\n",
    "\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#take one image\n",
    "# image = cv2.imread(input_dir+data['images'][1]['file_name'], cv2.IMREAD_GRAYSCALE)\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# print(np.max(image))\n",
    "image = np.ones((256, 256, 3))\n",
    "image_id = data['images'][1]['id']\n",
    "bbox_coords = {}\n",
    "\n",
    "masks = [data['annotations'][a] for a in range(len(data['annotations'])) if data['annotations'][a]['image_id'] == image_id]\n",
    "ground_truth_masks = {}\n",
    "\n",
    "# Roboflow segmentations are polygon points, and should be converted to masks\n",
    "def create_mask(points, image_size):\n",
    "    polygon = [(points[i], points[i+1]) for i in range(0, len(points), 2)]\n",
    "    mask = np.zeros(image_size, dtype=np.uint8)\n",
    "    \n",
    "    cv2.fillPoly(mask, [np.array(polygon, dtype=np.int32)], 1)\n",
    "    return mask\n",
    "\n",
    "for i in range(len(masks)):\n",
    "    xyhw = masks[i]['bbox']\n",
    "    points = masks[i]['segmentation'][0]\n",
    "    mask = create_mask(points, image.shape[:2])\n",
    "    ground_truth_masks[masks[i]['id']] = np.ones((256, 256))\n",
    "    bbox_coords[masks[i]['id']] = [0, 0, 255,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_masks = {0:ground_truth_masks[0]}\n",
    "bbox_coords =  {0:bbox_coords[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "\n",
    "def display_masks(masks):\n",
    "    cmap = mcolors.ListedColormap(['lightblue']) \n",
    "    \n",
    "    for mask in masks.values():\n",
    "        masked_data = ma.masked_where(mask == 0, mask)\n",
    "        plt.imshow(masked_data, alpha=0.4, cmap=cmap)\n",
    "        plt.contour(mask, colors='darkviolet', linewidths=0.1)  # contour color\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image)\n",
    "display_masks(ground_truth_masks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if xyhw[2] >2 or xyhw[3] >2:\n",
    "#         fig = px.imshow(mask)\n",
    "#         fig.update_layout(\n",
    "#             title_text=f'{image_id}<br>bbox (x, y, h, w):({xyhw[0]}, {xyhw[1]}, {xyhw[2]}, {xyhw[3]})',\n",
    "#             title_x=0.5, \n",
    "#             autosize=False,\n",
    "#             width=700,\n",
    "#             height=500\n",
    "#         )        \n",
    "#         fig.show()\n",
    "#         # cv2.imwrite(masks_dir+f'{k}_mask{a}.png',  np.array(mask_).astype(int))\n",
    "#         # # x1, y1, x2, y2 = xyhw[0]-2, xyhw[1]-2, xyhw[2]+ xyhw[0]+2, xyhw[3]+xyhw[1]+2\n",
    "#         # x1, y1, x2, y2 = xyhw[0], xyhw[1], xyhw[2]+ xyhw[0], xyhw[3]+xyhw[1]\n",
    "        \n",
    "#         # bbox_coords[f'{k}_mask{a}'] = np.array([x1, y1, x2, y2])\n",
    "#         # keys.append(f'{k}_mask{a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNJGn1BsKUMS"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lz7B4NDoJRxJ"
   },
   "outputs": [],
   "source": [
    "# ground_truth_masks = {}\n",
    "# for k in bbox_coords.keys():\n",
    "#   gt_grayscale = cv2.imread(f'{masks_dir}{k}.png', cv2.IMREAD_GRAYSCALE)\n",
    "#   ground_truth_masks[k] = (gt_grayscale != 0) # was ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bbox_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique(ground_truth_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test the masks\n",
    "from PIL import Image\n",
    "# for keyy in ground_truth_masks.keys():\n",
    "#     img = Image.fromarray(ground_truth_masks[keyy])\n",
    "#     print(keyy)\n",
    "#     img.show() \n",
    "\n",
    "# masks 4,5,6,9 and 16 are bad. \n",
    "# out them on 0 and check the prediction.\n",
    "# bad_masks = [4,5,6,9,16]\n",
    "\n",
    "# for i in bad_masks:\n",
    "#     ground_truth_masks[f\"S0720251301_L_mask{i}\"][:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bz8C8QaxoT6N"
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "3csOAxFju_Pi",
    "outputId": "c7321fb2-d34d-45e9-b00a-f77d246c9a45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in range(len(ground_truth_masks)):\n",
    "        # print(f'{masks_dir}{name}.png')\n",
    "        # image = cv2.imread(f'{masks_dir}{name}.png')\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(image)\n",
    "        show_box(bbox_coords[name], plt.gca())\n",
    "        show_mask(ground_truth_masks[name], plt.gca())\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJIFDGaUKfQp"
   },
   "source": [
    "## ðŸš€ Prepare Mobile SAM Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjTIJtLxP8ZG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/XMM_OM_code/MobileSAM/')\n",
    "import mobile_sam\n",
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "model_type = \"vit_t\" # tiny version\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/XMM_OM_code/MobileSAM/weights/mobile_sam.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model = sam_model_registry[model_type](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device)\n",
    "mobile_sam_model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The mean and std should be changed, OM images don't have the same mean/std as normal images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_T = np.transpose(image, (2, 1, 0))\n",
    "pixel_mean = torch.as_tensor([np.mean(image_T[0]), np.mean(image_T[1]),np.mean(image_T[2])], dtype=torch.float, device=device)\n",
    "pixel_std = torch.as_tensor([np.std(image_T[0]), np.std(image_T[1]),np.std(image_T[2])], dtype=torch.float, device=device)\n",
    "\n",
    "mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(image_T[0]), np.std(image_T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_parameters = count_parameters(mobile_sam_model)\n",
    "print(f\"The model has {num_parameters} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.watch(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bounding_box(ground_truth_map):\n",
    "#   # get bounding box from mask\n",
    "#   y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "#   x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "#   y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "#   # add perturbation to bounding box coordinates\n",
    "#   H, W = ground_truth_map.shape\n",
    "#   x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "#   # x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "#   # y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "#   # y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "#   w = x_max - x_min\n",
    "#   h = y_max - y_min\n",
    "#   bbox = [x_min, y_min, w, h]\n",
    "\n",
    "#   return np.array(bbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu0MdejGylZY"
   },
   "source": [
    "Convert the input images into a format SAM's internal functions expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtPYpirbK3Wi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "images_dir = \"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/\"\n",
    "transformed_data = defaultdict(dict)\n",
    "image = image.astype(np.uint8)\n",
    "\n",
    "transform = ResizeLongestSide(mobile_sam_model.image_encoder.img_size)\n",
    "k = 0\n",
    "negative_mask = np.where(image > 0, True, False)\n",
    "negative_mask = torch.from_numpy(negative_mask)  \n",
    "negative_mask = negative_mask.permute(2, 0, 1)\n",
    "negative_mask = resize(negative_mask, [1024, 1024], antialias=True) \n",
    "negative_mask = negative_mask.unsqueeze(0)\n",
    "\n",
    "# scales the image to 1024x1024 by the longest side (it doesn't matter in my case because images are square)\n",
    "input_image = transform.apply_image(image)\n",
    "print(input_image.shape)\n",
    "input_image_torch = torch.as_tensor(input_image, dtype=torch.float32, device=device)\n",
    "transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "# normalization and padding\n",
    "input_image = mobile_sam_model.preprocess(transformed_image)\n",
    "input_image = torch.ones_like(input_image)\n",
    "print(input_image)\n",
    "\n",
    "original_image_size = image.shape[:2]\n",
    "input_size = tuple(transformed_image.shape[-2:])\n",
    "input_image[~negative_mask] = 0\n",
    "input_image[:] = 1\n",
    "plt.imshow(input_image[0][0].cpu(), cmap='gray')\n",
    "plt.show()\n",
    "transformed_data[k]['image'] = input_image\n",
    "transformed_data[k]['input_size'] = input_size\n",
    "transformed_data[k]['original_image_size'] = original_image_size\n",
    "\n",
    "# apparently, this doesn't free the memory of x since y still points to x\n",
    "# del input_image_torch\n",
    "# del transformed_image\n",
    "# del input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() \n",
    "\n",
    "print(torch.cuda.memory_summary(device=device, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxnY6TMGKjdc"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "lr = 3e-4\n",
    "wd = 0.0\n",
    "optimizer = torch.optim.Adam(mobile_sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-8) # not very helpful\n",
    "\n",
    "def dice_loss(pred, target, area, smooth = 1): # smooth is added to avoid division by 0\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "\n",
    "    # loss = loss - 1.0/area\n",
    "    # loss = loss.clamp(min=0.0)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "print(torch.cuda.memory_reserved()/(1024**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print model weights before tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_before = {}\n",
    "for name, param in mobile_sam_model.state_dict().items():\n",
    "    weights_before[name] = param.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    if \"image_encoder\" in name or \"prompt_encoder\" in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_requires_grad(model, show=False):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(\"âœ… Param\", name, \" requires grad.\")\n",
    "        elif param.requires_grad == False:\n",
    "            print(\"âŒ Param\", name, \" doesn't require grad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_requires_grad(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRHCNdzZy3dt"
   },
   "source": [
    "## Run fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRQ6yd_PM_B9",
    "outputId": "ba548c39-113c-4c59-a62e-9c8ce68c8cce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as patches\n",
    "import copy\n",
    "\n",
    "num_epochs = 45\n",
    "losses = []\n",
    "mask_epoch_losses = {}\n",
    "mask_loss = {}\n",
    "\n",
    "predictor = SamPredictor(mobile_sam_model)\n",
    "\n",
    "input_image = transformed_data[0]['image'].clone().to(device)\n",
    "# plt.imshow(input_image[0].permute(1,2,0).detach().cpu().numpy())\n",
    "# plt.show()\n",
    "input_size = transformed_data[0]['input_size']\n",
    "original_image_size = transformed_data[0]['original_image_size']\n",
    "negative_mask = np.where(image > 0, True, False)\n",
    "negative_mask = torch.from_numpy(negative_mask)  \n",
    "negative_mask = negative_mask.permute(2, 0, 1)\n",
    "negative_mask = negative_mask[0]\n",
    "negative_mask = negative_mask.unsqueeze(0).unsqueeze(0)\n",
    "negative_mask = negative_mask.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_losses = []\n",
    "  for k in tqdm(ground_truth_masks):\n",
    "\n",
    "        # with torch.no_grad(): # this doesn't seem to work. I set explicitly the params before training.\n",
    "        if True:\n",
    "              image_embedding = mobile_sam_model.image_encoder(input_image)\n",
    "              prompt_box = np.array(bbox_coords[k])\n",
    "                \n",
    "              box = predictor.transform.apply_boxes(prompt_box, original_image_size)\n",
    "              box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "              box_torch = box_torch[None, :]\n",
    "\n",
    "              mask_input_torch = torch.as_tensor(ground_truth_masks[k], dtype=torch.float, device=device).unsqueeze(0)\n",
    "              # print(mask_input_torch.shape)\n",
    "              # mask_input_torch = mask_input_torch[None, :, :, :]\n",
    "\n",
    "              sparse_embeddings, dense_embeddings = mobile_sam_model.prompt_encoder(\n",
    "                  points=None,\n",
    "                  boxes=box_torch,\n",
    "                  masks=mask_input_torch,\n",
    "              )\n",
    "            \n",
    "        low_res_masks, iou_predictions = mobile_sam_model.mask_decoder(\n",
    "          image_embeddings=image_embedding,\n",
    "          image_pe=mobile_sam_model.prompt_encoder.get_dense_pe(), #  Returns the positional encoding used to encode point prompts,\n",
    "                                                                   #  applied to a dense set of points the shape of the image encoding.\n",
    "          sparse_prompt_embeddings=sparse_embeddings,\n",
    "          dense_prompt_embeddings=dense_embeddings,\n",
    "          multimask_output=True,\n",
    "        )\n",
    "        \n",
    "        # for mask_ in low_res_masks[0]:\n",
    "        #     mask = (mask_ > mobile_sam_model.mask_threshold).int()\n",
    "        #     plt.figure(figsize=(10,10))\n",
    "        #     plt.imshow(image)\n",
    "        #     show_mask(mask.detach().cpu().numpy(), plt.gca())\n",
    "        #     show_box(prompt_box, plt.gca())\n",
    "        #     plt.axis('off')\n",
    "        #     plt.show()  \n",
    "  \n",
    "        downscaled_masks = mobile_sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "        '''          \n",
    "        return_logits (bool): If true, returns un-thresholded masks logits instead of a binary mask.\n",
    "        '''\n",
    "        # return_logits = False\n",
    "        # if not return_logits:\n",
    "        #     binary_mask = downscaled_masks > mobile_sam_model.mask_threshold\n",
    "        #     binary_mask = binary_mask.int()\n",
    "            \n",
    "        # binary_mask = normalize(threshold(downscaled_masks, 0.0, 0))\n",
    "        binary_mask = torch.sigmoid(downscaled_masks - mobile_sam_model.mask_threshold)\n",
    "        # binary_mask[~negative_mask]= 0\n",
    "\n",
    "        numpy_binary_mask = binary_mask.detach().cpu().numpy()\n",
    "        # plt.imshow(numpy_binary_mask[0][0], cmap='viridis')\n",
    "        # plt.show()\n",
    "        gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masks[k], (1, 1, ground_truth_masks[k].shape[0], ground_truth_masks[k].shape[1]))).to(device)\n",
    "        gt_binary_mask = torch.as_tensor(gt_mask_resized >0, dtype=torch.float32) #was >0\n",
    "        numpy_gt_binary_mask = gt_binary_mask.contiguous().detach().cpu().numpy()\n",
    "        area = (prompt_box[2]-prompt_box[0])*(prompt_box[3]-prompt_box[1])\n",
    "        loss = dice_loss(binary_mask, gt_binary_mask, area)\n",
    "        if k in mask_loss.keys():\n",
    "            mask_loss[k].append(loss.item())\n",
    "        else:\n",
    "            mask_loss[k] = [loss.item()]\n",
    "            \n",
    "        epoch_losses.append(loss)\n",
    "        torch.cuda.empty_cache()\n",
    "      \n",
    "  one_loss = sum(epoch_losses)*1.0/len(epoch_losses)\n",
    "  losses.append(one_loss.item())\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  one_loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  print(f'EPOCH: {epoch}. Mean loss: {np.mean(losses)}')\n",
    "  torch.cuda.empty_cache()\n",
    "  print(\"Torch cuda memory allocated:\" , torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "  print(\"Torch cuda memory reserved:\" , torch.cuda.memory_reserved()/(1024**2))\n",
    "  print(\"_________________________\")\n",
    "    \n",
    "  #or print(f'EPOCH: {epoch}. Mean loss: {losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_mask[0][0].min(), binary_mask[0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all values should  be 1\n",
    "np.unique(binary_mask.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the GPU memory after training\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "print(torch.cuda.memory_reserved()/(1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model to checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': mobile_sam_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'mobile_sam_model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del mobile_sam_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print model weights after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run.finish()\n",
    "\n",
    "# After fine-tuning\n",
    "# print(\"After fine-tuning:\")\n",
    "# for name, param in mobile_sam_model.state_dict().items():\n",
    "#     if not torch.all(torch.eq(weights_before[name], param)):\n",
    "#         print(f'{name} has changed')\n",
    "#         print('Old weights:', weights_before[name])\n",
    "#         print('New weights:', param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_masks = []\n",
    "\n",
    "# for keyy in mask_loss.keys():\n",
    "#     bad_masks.append(mask_loss[keyy])\n",
    "\n",
    "# items = list(mask_loss.items())\n",
    "\n",
    "# fig, axes = plt.subplots(5, 4, figsize=(30, 30))\n",
    "\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#     x_values = np.arange(1, len(bad_masks[i]) + 1)\n",
    "    \n",
    "#     ax.plot(x_values, bad_masks[i])\n",
    "#     # ax.set_title(f'{items[i][0].split(\"_\")[2]}')\n",
    "#     ax.set_xlabel('Index')\n",
    "#     ax.set_ylabel('Value')\n",
    "#     # ax.set_ylim(0, 1)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "UKqIxUgAOTzp",
    "outputId": "27921a45-aa39-4f01-da87-b551ab6cc66e"
   },
   "outputs": [],
   "source": [
    "# mean_losses = mean(x) for x in losses]\n",
    "# mean_losses\n",
    "\n",
    "plt.plot(list(range(len(losses))), losses)\n",
    "plt.title('Mean epoch loss \\n mask with sigmoid')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('loss_mask_sigmoid.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuDlIiRjmitT"
   },
   "source": [
    "## Compare the tuned model to the Mobile SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9fZiPoIKXYW"
   },
   "outputs": [],
   "source": [
    "# Load up the model with default weights\n",
    "sam_model_orig = sam_model_registry[model_type](checkpoint=mobile_sam_checkpoint)\n",
    "sam_model_orig.to(device);\n",
    "sam_model_orig.eval();\n",
    "\n",
    "print(torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "print(torch.cuda.memory_reserved()/(1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dIKKKHOn_7R"
   },
   "outputs": [],
   "source": [
    "# Set up predictors for both tuned and original models\n",
    "# from segment_anything import sam_model_registry, SamPredictor\n",
    "predictor_tuned = SamPredictor(mobile_sam_model)\n",
    "predictor_original = SamPredictor(sam_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "print(torch.cuda.memory_reserved()/(1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove gradients requirement on the model\n",
    "\n",
    "# for name, param in mobile_sam_model.named_parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# mobile_sam_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhNHx-6kpEWu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dict and append tuned/original masks them on each image.\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('Before inference:')\n",
    "print(torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "print(torch.cuda.memory_reserved()/(1024**2))\n",
    "\n",
    "images_from_bbox_tuned = []\n",
    "images_from_bbox_orig = []\n",
    "k=0\n",
    "\n",
    "if True:\n",
    "# for k in bbox_coords.keys():\n",
    "\n",
    "    predictor_tuned.set_image(image)\n",
    "    predictor_original.set_image(image)\n",
    "    \n",
    "    input_bbox = np.array(bbox_coords[k])\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        masks_tuned, _, _ = predictor_tuned.predict(\n",
    "            point_coords=None,\n",
    "            box=input_bbox,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        \n",
    "        masks_orig, _, _ = predictor_original.predict(\n",
    "            point_coords=None,\n",
    "            box=input_bbox,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "    \n",
    "    masks_tuned = torch.sigmoid(masks_tuned - mobile_sam_model.mask_threshold)\n",
    "    masks_orig = torch.sigmoid(masks_orig - mobile_sam_model.mask_threshold)\n",
    "\n",
    "    print('In loop:')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_allocated()/(1024**2)) #MB\n",
    "    print(torch.cuda.memory_reserved()/(1024**2))\n",
    "    \n",
    "    images_from_bbox_tuned.append(masks_tuned.detach().cpu().numpy())\n",
    "    images_from_bbox_orig.append(masks_orig.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_from_bbox_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bbox_coords.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting\n",
    "# image = 255 - image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy.ma as ma\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def show_mask(masks, ax, random_color=False):\n",
    "    for mask in masks:\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        else:\n",
    "            color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)\n",
    "    \n",
    "_, axs = plt.subplots(1, 3, figsize=(25, 25))\n",
    "\n",
    "negative_mask_img = np.transpose((image>0).astype(int), (2, 0, 1))\n",
    "masks_tuned = images_from_bbox_tuned\n",
    "masks_orig = images_from_bbox_orig\n",
    "axs[0].imshow(image)\n",
    "show_mask(masks_tuned, axs[0])\n",
    "axs[0].set_title(f'Mask with Mobile Tuned Model\\n #epochs = {num_epochs}, lr={lr}, loss=dice_loss\\n mask with sigmoid', fontsize=18)\n",
    "\n",
    "axs[1].imshow(image)\n",
    "show_mask(masks_orig, axs[1])\n",
    "axs[1].set_title('Mask with Mobile Untuned Model', fontsize=18)\n",
    "\n",
    "axs[2].imshow(negative_mask_img[0], cmap='gray')\n",
    "axs[2].set_title('Negative pixels map', fontsize=18)\n",
    "# plt.subplots_adjust(left=2, right=1, bottom=0, top=1)  # remove padding and distance to corners\n",
    "\n",
    "plt.savefig('MobileSAM_output_one_image_sigmoid.png', dpi=400)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# import matplotlib.backends.backend_pdf\n",
    "\n",
    "# pdf = matplotlib.backends.backend_pdf.PdfPages(\"fine_tuned_mobile_sam_images.pdf\")\n",
    "\n",
    "# i=0\n",
    "# for k in images_from_bbox_tuned:\n",
    "#     if i<50:\n",
    "#         i+=1\n",
    "#     else:\n",
    "#         break\n",
    "#     _, axs = plt.subplots(1, 2, figsize=(25, 25))\n",
    "    \n",
    "#     image = cv2.imread(f'{images_dir}{k}.png')\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     masks_tuned = images_from_bbox_tuned[k]\n",
    "#     masks_orig = images_from_bbox_orig[k]\n",
    "    \n",
    "#     axs[0].imshow(image)\n",
    "#     display_masks(axs[0], masks_tuned)\n",
    "#     # show_mask(mask, axs[0])\n",
    "#     # show_box(input_bbox, axs[0])\n",
    "#     axs[0].set_title('Mask with Tuned Model', fontsize=26)\n",
    "#     axs[0].axis('off')\n",
    "\n",
    "#     axs[1].imshow(image)\n",
    "#     display_masks(axs[1],masks_orig)\n",
    "#     # show_mask(mask, axs[1])\n",
    "#     # show_box(input_bbox, axs[1])\n",
    "#     axs[1].set_title('Mask with Untuned Model', fontsize=26)\n",
    "#     axs[1].axis('off')\n",
    "#     pdf.savefig(_)\n",
    "#     plt.close(_)\n",
    "# pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the tuned model to the original SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "# !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "\n",
    "HOME = os.getcwd()\n",
    "import os\n",
    "\n",
    "origSAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(origSAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(origSAM_CHECKPOINT_PATH))\n",
    "\n",
    "origMODEL_TYPE = \"vit_h\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[origMODEL_TYPE](checkpoint=origSAM_CHECKPOINT_PATH).to(device=device)\n",
    "sam.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_origSAM = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images_from_bbox_orig_SAM = {}\n",
    "\n",
    "# k=74\n",
    "if True:\n",
    "# for k in keys:\n",
    "    # image_from_key = f'{k.split(\"_\")[0]+\"_\"+k.split(\"_\")[1]}'\n",
    "    \n",
    "    if k not in images_from_bbox_orig_SAM:\n",
    "        images_from_bbox_orig_SAM[k] = []\n",
    "\n",
    "    # image = cv2.imread(f'{images_dir}{k.split(\"_\")[0]+\"_\"+k.split(\"_\")[1]}.png')\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    input_bbox = np.array(bbox_coords[k])\n",
    "    predictor_tuned.set_image(image)\n",
    "    predictor_origSAM.set_image(image)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        masks_orig, _, _ = predictor_origSAM.predict(\n",
    "            point_coords=None,\n",
    "            box=input_bbox,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "    images_from_bbox_orig_SAM[k].append(masks_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### for k in images_from_bbox_tuned:\n",
    "if True:\n",
    "    _, axs = plt.subplots(1, 3, figsize=(25, 25))\n",
    "    \n",
    "    # image = cv2.imread(f'{images_dir}{k}.png')\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    negative_mask_img = np.transpose(image>0, (2, 0, 1))\n",
    "    # image = 255 - image\n",
    "    \n",
    "    # masks_tuned = images_from_bbox_tuned[k]\n",
    "    masks_orig = images_from_bbox_orig_SAM[k]\n",
    "    axs[0].imshow(image)\n",
    "    show_mask(masks_tuned, axs[0])\n",
    "    axs[0].set_title(f'Mask with Mobile Tuned Model\\n #epochs = {num_epochs}, lr={lr}, loss=dice_loss\\n mask with sigmoid', fontsize=18)\n",
    "    \n",
    "    axs[1].imshow(image)\n",
    "    show_mask(masks_orig, axs[1])\n",
    "    axs[1].set_title('Mask with SAM Untuned Model', fontsize=26)\n",
    "    \n",
    "    axs[2].imshow(negative_mask_img[0], cmap='viridis')\n",
    "    axs[2].set_title('Negative pixels map', fontsize=26)\n",
    "    # plt.subplots_adjust(left=0, right=1, bottom=0, top=1)  # remove padding and distance to corners\n",
    "\n",
    "    plt.savefig('origSAM_output_one_image.png')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
