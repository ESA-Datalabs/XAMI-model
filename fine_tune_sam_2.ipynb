{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3c6e91",
   "metadata": {},
   "source": [
    "### 00 Install segment anything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37897d42",
   "metadata": {},
   "source": [
    "source: https://maxjoas.medium.com/finetune-segment-anything-sam-for-images-with-multiple-masks-34514ee811bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segment_anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if theer are duplicates in the dataset\n",
    "# import json\n",
    "\n",
    "# with open('./xmm_om_images_v4-contrast-512-5-7/train/_annotations.coco.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# filenames = [img['file_name'] for img in data['images']]\n",
    "# duplicates = {}\n",
    "# second_duplicates = {}\n",
    "\n",
    "# for filename in filenames:\n",
    "# \tid = '_'.join(filename.split('_')[:2])\n",
    "# \tif id in duplicates:\n",
    "# \t\tsecond_duplicates[id] = 1\n",
    "# \telse:\n",
    "# \t\tduplicates[id] = 1\n",
    "\n",
    "# second_duplicates.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767e7cf",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import json\n",
    "import toml\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e1430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.types import Device\n",
    "#data\n",
    "global train\n",
    "global test\n",
    "global annot\n",
    "\n",
    "# TODO: Put your path here !!!!\n",
    "# train_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/train\"\n",
    "# test = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/test\"\n",
    "\n",
    "train_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images_v4-contrast-512-5-7/train/\"\n",
    "test = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images_v4-contrast-512-5-7/train/\"\n",
    "\n",
    "annot_train = \"_annotations.coco.json\"\n",
    "annot_test = \"_annotations.coco.json\"\n",
    "\n",
    "#model\n",
    "global batch_size\n",
    "global epochs\n",
    "global lr\n",
    "global weight_decay\n",
    "global DEVICE\n",
    "\n",
    "# TODOD adjust, if needed!!!!\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "lr = 6e-4\n",
    "weight_decay = 0.0\n",
    "loss_scaling_factor = 1e4  # to save some very small gradients (on some attention params) from extinction \n",
    "\n",
    "device_id = 1\n",
    "torch.cuda.set_device(device_id)\n",
    "\n",
    "DEVICE = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# loss\n",
    "global alpha\n",
    "global gamma\n",
    "alpha = 0.8\n",
    "gamma = 2\n",
    "\n",
    "use_wandb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed75954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa651ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is used to extract annotations for images given filenames and json annotations file\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('./-xmm_om_images_v4-contrast-512-5-2/train/_annotations.coco.json', 'r') as f:\n",
    "#     coco_data = json.load(f)\n",
    "\n",
    "# # Filter the images to keep only those with filenames in image_filenames\n",
    "# filtered_images = [image for image in coco_data['images'] if image['file_name'] in iter_1_repr_images]\n",
    "# print(filtered_images)\n",
    "# # Collect IDs of the filtered images\n",
    "# img_ids = [image['id'] for image in filtered_images]\n",
    "\n",
    "# # Filter the annotations to keep only those associated with the filtered image IDs\n",
    "# filtered_annotations = [annotation for annotation in coco_data['annotations'] if annotation['image_id'] in img_ids]\n",
    "\n",
    "# # Update the original COCO data structure with filtered data\n",
    "# coco_data['images'] = filtered_images\n",
    "# coco_data['annotations'] = filtered_annotations\n",
    "\n",
    "# # Optionally, filter categories to keep only those used in the filtered annotations\n",
    "# used_category_ids = set(annotation['category_id'] for annotation in filtered_annotations)\n",
    "# filtered_categories = [category for category in coco_data['categories'] if category['id'] in used_category_ids]\n",
    "# coco_data['categories'] = filtered_categories\n",
    "\n",
    "# # Save the modified data to a new JSON file\n",
    "# with open('./repr_images_1/_annotations.coco.json', 'w') as f:\n",
    "#     json.dump(coco_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067a240",
   "metadata": {},
   "source": [
    "### Take a first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add your path\n",
    "sample_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images_v4-contrast-512-5-7/train/S0038541101_L_png.rf.03cac6eed3c7b1e15645a0a684f32e90.jpg\"\n",
    "sample_img = cv2.imread(sample_path)\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb37f0",
   "metadata": {},
   "source": [
    "## 01 Segment image with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305be618",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "def build_totalmask(pred: List[Dict[str, Any]]) -> np.ndarray:\n",
    "    \"\"\"Builds a total mask from a list of segmentations\n",
    "    ARGS:\n",
    "        pred (list): list of dicts with keys 'segmentation' and others\n",
    "    RETURNS:\n",
    "        total_mask (np.ndarray): total mask\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    total_mask = np.zeros(pred[0]['segmentation'].shape, dtype=np.uint8)\n",
    "    for seg in pred:\n",
    "        total_mask += seg['segmentation']\n",
    "    # use cv2 to make image black and white\n",
    "    _, total_mask = cv2.threshold(total_mask, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "    return total_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5602681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"./sam_vit_h_4b8939.pth\")\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "sam.to(DEVICE)\n",
    "masks = mask_generator.generate(sample_img)\n",
    "print(type(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80497670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(masks))\n",
    "print(type(masks[0]))\n",
    "print(f'keys of dict: {masks[0].keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of masks: {masks[0]['segmentation'].shape}\")\n",
    "print('Value counts in segmentation of first mask:')\n",
    "print(np.unique(masks[0]['segmentation'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c5ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mask = build_totalmask(masks)\n",
    "plt.imshow(total_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef217810",
   "metadata": {},
   "source": [
    "When you use the web app of SAM, you might notice, that you need to provide a prompt (i.e. point with your mouse where your object is) to get a result. The mask_generator does this for you, by providing a grid of points over the whole image and creating a mask for each point and then later removing duplicated and low-quality masks. See the point grid below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = mask_generator.point_grids[0]\n",
    "# plot image and lay points on it\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(sample_img)\n",
    "ax.scatter(x=points[:, 0] *512, y=points[:, 1] *512, c=\"r\", s=10)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc857ba",
   "metadata": {},
   "source": [
    "### Summary Segment Anything format:\n",
    "\n",
    "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
    "\n",
    "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
    "* `area` - `[int]` - the area of the mask in pixels\n",
    "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
    "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
    "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
    "* `stability_score` - `[float]` - an additional measure of mask quality\n",
    "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format\n",
    "\n",
    "- The mask generator uses a grid of points as prompts and generates masks for each point.[see here](https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py)\n",
    "\n",
    "- The coco annotations categories is a list of dictionaries with keys:\n",
    "  * `id` - the id of the class\n",
    "  * `name` - the name corresponding to the id\n",
    "  * `supercategory` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to load data from a json file in COCO format.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        the root directory containing the images and annotations\n",
    "    annotation_file : str\n",
    "        name of the json file containing the annotations (in root_dir)\n",
    "    transform : callable\n",
    "        a function/transform to apply to each image\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx)\n",
    "        returns the image, image path, and masks for the given index\n",
    "    buid_total_mask(masks)\n",
    "        combines the masks into a single mask\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, root_dir, annotation_file, transform=None, split=None, percentage=None):\n",
    "\n",
    "        assert (split is None and percentage is None) or (split is not None and percentage is not None), \"Either both split and percentage should be None or neither.\"\n",
    "        assert (split=='train') or (split=='val'), \"Specify the split by using 'train' or 'val' keywords.\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids_1 = list(self.coco.imgs.keys())\n",
    "        self.categories = self.coco.loadCats(self.coco.getCatIds()) \n",
    "        # Filter out image_ids without any annotations\n",
    "        self.image_ids_1 = [image_id for image_id in self.image_ids_1 if len(self.coco.getAnnIds(imgIds=image_id)) > 0]\n",
    "        self.categories = {self.categories[i]['id']: self.categories[i]['name'] for i in range(len(self.categories))} # id:name dict\n",
    "        self.image_ids = []\n",
    "\n",
    "        for idx in range(len(self.image_ids_1)):\n",
    "            image_id = self.image_ids_1[idx]\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            good_image = False\n",
    "            \n",
    "            for ann in anns:\n",
    "                x, y, w, h = ann['bbox']\n",
    "                if w > 2 or h > 2: # filter out very small masks\n",
    "                    # if not self.categories[ann['category_id']].endswith('star') and (not self.categories[ann['category_id']].endswith('streak')): # remove stars \n",
    "                        good_image = True\n",
    "                        \n",
    "            if good_image: # filter out images with no annotations again\n",
    "                self.image_ids.append(image_id)\n",
    "\n",
    "        if split == 'train':\n",
    "            percentage = int(len(self.image_ids) * percentage)\n",
    "            self.image_ids = self.image_ids[:percentage]\n",
    "        else:\n",
    "            percentage = int(len(self.image_ids) * (1-percentage))\n",
    "            self.image_ids = self.image_ids[percentage:]\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root_dir, image_info['file_name'])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # if not self.categories[ann['category_id']].endswith('star') and (not self.categories[ann['category_id']].endswith('streak')): # this is safe because images with only stars are already filtered out in __init__\n",
    "            bboxes.append([x, y, x + w, y + h])\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "      \n",
    "        if self.transform:\n",
    "            image, masks, bboxes = self.transform(image, masks, np.array(bboxes))\n",
    "\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        return image, image_path, torch.tensor(masks).float()\n",
    "\n",
    "    def get_totalmask(self, masks):\n",
    "        \"\"\"get all masks in to one image\n",
    "        ARGS:\n",
    "            masks (List[Tensor]): list of masks\n",
    "        RETURNS:\n",
    "            total_gt (Tensor): all masks in one image\n",
    "\n",
    "        \"\"\"\n",
    "        total_gt = torch.zeros_like(masks[0][0,:,:])\n",
    "        for k in range(len(masks[0])):\n",
    "            total_gt += masks[0][k,:,:]\n",
    "        return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class ResizeAndPad:\n",
    "    \"\"\"\n",
    "    Resize and pad images and masks to a target size.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_size : int\n",
    "        the target size of the image\n",
    "    transform : ResizeLongestSide\n",
    "        a transform to resize the image and masks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "        self.transform = ResizeLongestSide(target_size)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, image, masks, bboxes):\n",
    "        # Resize image and masks\n",
    "        og_h, og_w, _ = image.shape\n",
    "\n",
    "        negative_map = (np.where(image > 0, 1, 0)).astype(np.uint8)\n",
    "        negative_map = torch.from_numpy(negative_map)  \n",
    "        negative_map = negative_map.permute(2, 0, 1)\n",
    "        negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "        negative_map = negative_map.to(torch.uint8)\n",
    "        image = self.transform.apply_image(image)\n",
    "        masks = [torch.tensor(self.transform.apply_image(mask)) for mask in masks]\n",
    "        image = self.to_tensor(image)\n",
    "\n",
    "        # Pad image and masks to form a square\n",
    "        _, h, w = image.shape\n",
    "        max_dim = max(w, h)\n",
    "        pad_w = (max_dim - w) // 2\n",
    "        pad_h = (max_dim - h) // 2\n",
    "\n",
    "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n",
    "        image = transforms.Pad(padding)(image)\n",
    "        masks = [transforms.Pad(padding)(mask) for mask in masks]\n",
    "\n",
    "        # image = image * negative_map # mask -ve pixels\n",
    "        # print(negative_map.shape, image.shape, np.unique(negative_map.detach().cpu().numpy()))\n",
    "        # plt.imshow(image[0].detach().cpu().numpy())\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        # Adjust bounding boxes\n",
    "        bboxes = self.transform.apply_boxes(bboxes, (og_h, og_w))\n",
    "        bboxes = [[bbox[0] + pad_w, bbox[1] + pad_h, bbox[2] + pad_w, bbox[3] + pad_h] for bbox in bboxes]\n",
    "\n",
    "        return image, masks, bboxes\n",
    "\n",
    "\n",
    "def load_datasets(img_size):\n",
    "    \"\"\" load the training and validation datasets in PyTorch DataLoader objects\n",
    "    ARGS:\n",
    "        img_size (Tuple(int, int)): image size\n",
    "    RETURNS:\n",
    "        train_dataloader (DataLoader): training dataset\n",
    "        val_dataloader (DataLoader): validation dataset\n",
    "\n",
    "    \"\"\"\n",
    "    transform = ResizeAndPad(1024)\n",
    "    traindata = COCODataset(root_dir=train_path,\n",
    "                        annotation_file=os.path.join(train_path, annot_train),\n",
    "                        transform=transform,\n",
    "                        split='train',\n",
    "                        percentage=0.8)\n",
    "    valdata = COCODataset(root_dir=test,\n",
    "                      annotation_file=os.path.join(test, annot_test),\n",
    "                      transform=transform,\n",
    "                      split='val', \n",
    "                      percentage=0.2)\n",
    "    train_dataloader = DataLoader(traindata,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=1)\n",
    "    val_dataloader = DataLoader(valdata,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b4af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = load_datasets(1024)\n",
    "sample_img = trainloader.dataset[0][0]\n",
    "sample_mask = trainloader.dataset[0][2]\n",
    "\n",
    "print('#train images:', len(trainloader.dataset), '#validation images:', len(validloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape of sample_img: {sample_img.shape}')\n",
    "print(f'shape fo sample_mask: {sample_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617290fb",
   "metadata": {},
   "source": [
    "### Summary functions and classes:\n",
    "Now we have transformed our images and coco annotations to torch tensors, that we can use for training. For training (fine-tuning SAM) we need to define a Neural net with PyTorch first, we do this in the next class. It's pretty well documented, so I'll leave you with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c626b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ModelSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the sam model to to fine-tune the model on a new dataset\n",
    "\n",
    "    ...\n",
    "    Attributes:\n",
    "    -----------\n",
    "    freeze_encoder (bool): freeze the encoder weights\n",
    "    freeze_decoder (bool): freeze the decoder weights\n",
    "    freeze_prompt_encoder (bool): freeze the prompt encoder weights\n",
    "    transform (ResizeLongestSide): resize the images to the model input size\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    setup(): load the model and freeze the weights\n",
    "    forward(images, points): forward pass of the model, returns the masks and iou_predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, freeze_encoder=True, freeze_decoder=False, freeze_prompt_encoder=True):\n",
    "        super().__init__()\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.freeze_decoder = freeze_decoder\n",
    "        self.freeze_prompt_encoder = freeze_prompt_encoder\n",
    "        # we need this to make the input image size compatible with the model\n",
    "        self.transform = ResizeLongestSide(1024) #This is 1024, because sam was trained on 1024x1024 images\n",
    "\n",
    "    def setup(self, MODEL_TYPE, chekpoint):\n",
    "        self.model = sam_model_registry[MODEL_TYPE](chekpoint)\n",
    "        # to speed up training time, we normally freeze the encoder and decoder\n",
    "        if self.freeze_encoder:\n",
    "            for param in self.model.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_prompt_encoder:\n",
    "            for param in self.model.prompt_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_decoder:\n",
    "            for param in self.model.mask_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.transfrom = ResizeLongestSide(self.model.image_encoder.img_size)\n",
    "        \n",
    "    def forward(self, images, negative_maps):\n",
    "\n",
    "        # print('images shape:', images.shape)\n",
    "        _, _, H, W = images.shape # batch, channel, height, width - well, batch_size cannot be greater than 1 apparently\n",
    "        \n",
    "        image_embeddings = self.model.image_encoder(images) # shape: (1, 256, 64, 64)\n",
    "        # get prompt embeddings without acutally any prompts (uninformative)\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "\n",
    "        # get low resolution masks and iou predictions\n",
    "        # mulitmask_output=False means that we only get one mask per image,\n",
    "        # otherwise we would get three masks per image\n",
    "        low_res_masks, iou_predictions = self.model.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # sparse_embeddings shape: (1, 0, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # dense_embeddings shape: (1, 256, 256)\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        # print('low_res_masks', low_res_masks.shape)\n",
    "        # postprocess the masks to get the final masks and resize them to the original image size\n",
    "        masks = F.interpolate(\n",
    "            low_res_masks, # shape: (1, 1, 256, 256)\n",
    "            (H, W),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        \n",
    "        masks = masks * negative_maps\n",
    "\n",
    "        # print(masks.shape)\n",
    "        # shape masks after interpolate: torch.Size([1, 1, 1024, 1024])\n",
    "        return masks, iou_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10131450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ModelSimple()\n",
    "# model.setup('vit_h', './sam_vit_h_4b8939.pth')\n",
    "# img_size = model.model.image_encoder.img_size\n",
    "# print(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c924a",
   "metadata": {},
   "source": [
    "## Association algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def compute_iou_tensor(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) of two binary masks represented as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    intersection = torch.logical_and(mask1, mask2).sum().item()\n",
    "    union = torch.logical_or(mask1, mask2).sum().item()\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return intersection / union\n",
    "\n",
    "def associate_masks_tensor(predicted_masks, target_masks):\n",
    "    \"\"\"\n",
    "    Associate each predicted mask with a target mask using the Hungarian algorithm.\n",
    "    Assumes inputs are lists of PyTorch tensors.\n",
    "    \"\"\"\n",
    "    num_predicted = len(predicted_masks)\n",
    "    num_targets = len(target_masks)\n",
    "    \n",
    "    # Calculate the IoU between each pair of predicted and target mask\n",
    "    iou_matrix = torch.zeros(num_predicted, num_targets)\n",
    "    for i, pred_mask in enumerate(predicted_masks):\n",
    "        for j, target_mask in enumerate(target_masks):\n",
    "            iou_matrix[i, j] = compute_iou_tensor(pred_mask, target_mask)\n",
    "    \n",
    "    # Convert IoU matrix to NumPy for linear_sum_assignment\n",
    "    iou_matrix_np = iou_matrix.numpy()\n",
    "    row_ind, col_ind = linear_sum_assignment(1 - iou_matrix_np)\n",
    "    \n",
    "    # Filter out assignments with no overlap\n",
    "    matched_indices = [(r, c) for r, c in zip(row_ind, col_ind) if iou_matrix[r, c] > 0]\n",
    "    \n",
    "    return matched_indices, iou_matrix\n",
    "\n",
    "# # Example usage with tensor masks\n",
    "# predicted_masks = [torch.randint(0, 2, (100, 100), dtype=torch.bool) for _ in range(5)]\n",
    "# target_masks = [torch.randint(0, 2, (100, 100), dtype=torch.bool) for _ in range(4)]\n",
    "\n",
    "# matched_indices, iou_scores_tensor = associate_masks_tensor(predicted_masks, target_masks)\n",
    "\n",
    "# for pred_idx, target_idx in matched_indices:\n",
    "#     print(f\"Predicted Mask {pred_idx} is matched with Target Mask {target_idx} with IoU {iou_scores_tensor[pred_idx, target_idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb02b67",
   "metadata": {},
   "source": [
    "## Models, classes functions for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4336f8d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_totalmask(masks):\n",
    "    \"\"\"get all masks in to one image\n",
    "    ARGS:\n",
    "        masks (torch.Tensor): shape: (N, H, W) where N is the number of masks\n",
    "                              masks H,W is usually 1024,1024\n",
    "    RETURNS:\n",
    "        total_gt (torch.Tensor): all masks in one image\n",
    "\n",
    "    \"\"\"\n",
    "    total_gt = torch.zeros_like(masks[0,:,:])\n",
    "    for k in range(len(masks)):\n",
    "        total_gt += masks[k,:,:]\n",
    "    return total_gt\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Computes the Focal loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1 - BCE_EXP)**gamma * BCE\n",
    "\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\" Computes the Dice loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / \\\n",
    "            (inputs.sum() + targets.sum() + smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "\n",
    "def criterion(x, y):\n",
    "    \"\"\" Combined dice and focal loss.\n",
    "    ARGS:\n",
    "        x: (torch.Tensor) the model output\n",
    "        y: (torch.Tensor) the target\n",
    "    RETURNS:\n",
    "        (torch.Tensor) the combined loss\n",
    "\n",
    "    \"\"\"\n",
    "    focal, dice = FocalLoss(), DiceLoss()\n",
    "    y = y.to(DEVICE)\n",
    "    x = x.to(DEVICE)\n",
    "    return loss_scaling_factor * (20 * focal(x, y) + dice(x, y)) # ðŸ“ðŸ“ðŸ“\n",
    "\n",
    "def validate_step(model, validloader):\n",
    "    # trainloader, validloader = load_datasets(1024)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        running_vloss = 0.\n",
    "        with torch.no_grad():\n",
    "            for images, path, masks in validloader:\n",
    "                orig_image = cv2.imread(path[0])\n",
    "                orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                negative_map = (np.where(orig_image > 0, 1, 0)).astype(np.uint8)\n",
    "                negative_map = torch.from_numpy(negative_map)  \n",
    "                negative_map = negative_map.permute(2, 0, 1)\n",
    "                negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "                negative_map = negative_map.to(torch.uint8).squeeze(0).to(DEVICE)\n",
    "                \n",
    "                # model.to(DEVICE)\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks[0].to(DEVICE)\n",
    "                total_mask = get_totalmask(masks)\n",
    "                total_mask = total_mask.to(DEVICE)\n",
    "                preds, iou = model(images, negative_map)\n",
    "                preds = preds.to(DEVICE)\n",
    "                \n",
    "                # was criterion(preds, total_mask) before. i changed because the multimask_output was set on True. anyways, it treats both cases now\n",
    "                vloss = criterion(preds[0][0].unsqueeze(0).unsqueeze(0), total_mask) \n",
    "                running_vloss += vloss.item()\n",
    "                # np_image = images[0].permute(1,2,0).detach().cpu().numpy()\n",
    "                # np_mask = masks.detach().cpu().numpy()\n",
    "                # np_pred = preds[0].detach().cpu().numpy()\n",
    "        \n",
    "                # binary_pred_mask = ((F.sigmoid(preds[0]).cpu().numpy()[0]) > 0.5).astype(int) * 255\n",
    "                # binary_pred_mask = ((preds[0]).cpu().numpy()[0] > 0).astype(int) * 255\n",
    "                \n",
    "                # print(np.mean((F.sigmoid(preds[0]).cpu().numpy()[0])), np.min((F.sigmoid(preds[0]).cpu().numpy()[0])), np.max((F.sigmoid(preds[0]).cpu().numpy()[0])))\n",
    "                # print(np.unique((F.sigmoid(preds[0]).cpu().numpy()[0]) > 0.5).astype(int))\n",
    "                # print(\"validation loss: \", running_vloss)\n",
    "                \n",
    "                # fig, axs = plt.subplots(1, 3)\n",
    "                \n",
    "                # axs[0].imshow(np_image)\n",
    "                # axs[0].axis('off')\n",
    "            \n",
    "                # axs[1].imshow(get_totalmask(masks).detach().cpu().numpy())\n",
    "                # axs[1].set_title('Ground truth masks', fontsize=10)\n",
    "                # axs[1].axis('off')\n",
    "                \n",
    "                # axs[2].imshow(binary_pred_mask)\n",
    "                # axs[2].set_title('Predicted masks', fontsize=10)\n",
    "                # axs[2].axis('off')\n",
    "                # # plt.savefig('on_img_train.png', dpi=300)\n",
    "            \n",
    "                # plt.show()\n",
    "                # plt.close()\n",
    "    return running_vloss/len(validloader)\n",
    "    \n",
    "def train_one_epoch(model, trainloader, optimizer, epoch_idx):\n",
    "    \"\"\" Runs forward and backward pass for one epoch and returns the average\n",
    "    batch loss for the epoch.\n",
    "    ARGS:\n",
    "        model: (nn.Module) the model to train\n",
    "        trainloader: (torch.utils.data.DataLoader) the dataloader for training\n",
    "        optimizer: (torch.optim.Optimizer) the optimizer to use for training\n",
    "        epoch_idx: (int) the index of the current epoch\n",
    "        tb_writer: (torch.utils.tensorboard.writer.SummaryWriter) the tensorboard writer\n",
    "    RETURNS:\n",
    "        last_loss: (float) the average batch loss for the epoch\n",
    "\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    for i, (image, path, masks) in tqdm(enumerate(trainloader)):\n",
    "        orig_image = cv2.imread(path[0])\n",
    "        orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        negative_map = (np.where(orig_image > 0, 1, 0)).astype(np.uint8)\n",
    "        negative_map = torch.from_numpy(negative_map)  \n",
    "        negative_map = negative_map.permute(2, 0, 1)\n",
    "        negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "        negative_map = negative_map.to(torch.uint8).squeeze(0).to(DEVICE)\n",
    "        del orig_image\n",
    "        image = image.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model(image, negative_map) \n",
    "        # print('pred', pred.shape)\n",
    "        # pred, _ = model(255-image, negative_map) # ðŸ“ŒðŸ“ŒðŸ“Œ\n",
    "        masks = masks[0].to(DEVICE) # (number of masks, 1024, 1024)\n",
    "\n",
    "        # # match predicted with ground truth masks (using the Hungarian method)\n",
    "        # matched_indices, iou_scores_tensor = associate_masks_tensor(pred, masks)\n",
    "        # print(matched_indices, iou_scores_tensor)\n",
    "        total_mask = get_totalmask(masks)\n",
    "        a_mask_on_prediction = total_mask > 0.0 # the predicted total mask will ignore stars and other non-annotated objects from prediction\n",
    "        # pred[0][0] = pred[0][0] * a_mask_on_prediction\n",
    "        pred = pred.to(DEVICE)\n",
    "        loss = criterion(pred[0][0].unsqueeze(0).unsqueeze(0), total_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        binary_pred_mask = ((pred.detach().cpu().numpy()[0]) > 0.0).astype(int)\n",
    "\n",
    "        # for name, parameter in model.named_parameters():\n",
    "        #         if parameter.grad is not None: \n",
    "        #             grad_norm = parameter.grad.norm()\n",
    "        #             if grad_norm < 1e-8: \n",
    "        #                 print(f'â—ï¸Layer {name} has vanishing gradients: {grad_norm}')\n",
    "                        \n",
    "        # fig, axs = plt.subplots(1, 3)\n",
    "        \n",
    "        # axs[0].imshow(image[0].permute(1,2,0).detach().cpu().numpy())\n",
    "        # axs[0].axis('off')\n",
    "    \n",
    "        # axs[1].imshow(total_mask.detach().cpu().numpy())\n",
    "        # axs[1].set_title('Ground truth masks', fontsize=10)\n",
    "        # axs[1].axis('off')\n",
    "        \n",
    "        # # axs[2].imshow(pred.detach().cpu().numpy()[0][0])\n",
    "        # axs[2].imshow(binary_pred_mask[0])\n",
    "        # axs[2].set_title('Predicted masks', fontsize=10)\n",
    "        # axs[2].axis('off')\n",
    "        # # plt.savefig('on_img_train.png', dpi=300)\n",
    "    \n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "\n",
    "    i = len(trainloader)\n",
    "    last_loss = running_loss / i\n",
    "    # print(f'batch_loss for batch {i}: {last_loss}')\n",
    "    tb_x = epoch_idx * len(trainloader) + i + 1\n",
    "    # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    running_loss = 0.\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def train(n_epochs_stop):\n",
    "    \"\"\" Trains the model for the given number of epochs.\"\"\"\n",
    "    model = ModelSimple()\n",
    "    model.setup('vit_h', './sam_vit_h_4b8939.pth')\n",
    "\n",
    "    if use_wandb:\n",
    "        from datetime import datetime\n",
    "        import wandb\n",
    "        wandb.login()\n",
    "        run = wandb.init(project=\"OM_AI_all_masks\", name=f\"ft_MobileSAM {datetime.now()}\")\n",
    "        wandb.watch(model, log='all', log_graph=True)\n",
    "\n",
    "    # model.load_state_dict(torch.load('model_final.pth'))\n",
    "    model.to(DEVICE)\n",
    "    img_size = model.model.image_encoder.img_size\n",
    "    trainloader, validloader = load_datasets(img_size=img_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    train_losses, valid_losses = [], []\n",
    "    for epch in range(epochs):\n",
    "        #train\n",
    "        running_vloss = 0.\n",
    "        model.train()\n",
    "        avg_batchloss = train_one_epoch(model, trainloader, optimizer, epch)/loss_scaling_factor\n",
    "        train_losses.append(avg_batchloss)\n",
    "        \n",
    "        # validate\n",
    "        eval = True\n",
    "        if not eval: \n",
    "            continue\n",
    "        running_vloss = validate_step(model, validloader)/loss_scaling_factor\n",
    "        valid_losses.append(running_vloss)\n",
    "        \n",
    "        # save model\n",
    "        print(f'epoch: {epch}, train loss: {avg_batchloss}.')\n",
    "        print(f'epoch: {epch}, validloss: {running_vloss}')\n",
    "        if running_vloss < best_valid_loss:\n",
    "            best_valid_loss = running_vloss\n",
    "            best_model = model\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print(\"Early stopping initiated.\")\n",
    "                break\n",
    "\n",
    "        print(f'best valid loss: {best_valid_loss}')\n",
    "\n",
    "        # Logging\n",
    "        if use_wandb:\n",
    "            wandb.log({'epoch_train_loss': avg_batchloss, 'epoch_val_loss': running_vloss})\n",
    "            \n",
    "    return best_model, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f7ce76",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = True\n",
    "model, train_losses, val_losses = train(n_epochs_stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33616181",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wandb:\n",
    "    wandb.run.summary[\"batch_size\"] = batch_size\n",
    "    wandb.run.summary[\"num_epochs\"] = epochs\n",
    "    wandb.run.summary[\"learning rate\"] = lr\n",
    "    wandb.run.summary[\"weight_decay\"] = weight_decay\n",
    "    wandb.run.summary[\"loss_scaling_factor\"] = loss_scaling_factor\n",
    "    wandb.run.summary[\"alpha\"] = alpha\n",
    "    wandb.run.summary[\"gamma\"] = gamma\n",
    "    wandb.run.summary[\"#train images\"] = len(trainloader.dataset)\n",
    "    wandb.run.summary[\"#validation images\"] = len(validloader.dataset)\n",
    "\n",
    "    run.finish()\n",
    "    \n",
    "torch.save(model.state_dict(), f'model_final_all_masks.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train losses:', train_losses)\n",
    "print('val losses:', val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda1aa4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(train_losses))), train_losses, label='Training Loss')\n",
    "plt.plot(list(range(len(val_losses))), val_losses, label='Validation Loss')\n",
    "plt.title('Mean epoch loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/amg_losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25148f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sam = sam_model_registry[\"vit_h\"](checkpoint=\"./sam_vit_h_4b8939.pth\")\n",
    "# mask_generator = SamAutomaticMaskGenerator(model.model)\n",
    "# # sam.to(DEVICE)\n",
    "# predicted_masks = mask_generator.generate(sample_img) # this doesn't really work this way, but rather in the way the model was fine-tuned\n",
    "# print(len(predicted_masks))\n",
    "\n",
    "# total_mask = build_totalmask(predicted_masks)\n",
    "# plt.imshow(total_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cf379",
   "metadata": {},
   "source": [
    "## Predict trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05348ab0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# predict trainied model\n",
    "\n",
    "def evaluate_model(fine_tuned = True):\n",
    "    trainloader, validloader = load_datasets(1024)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = ModelSimple()\n",
    "        model.setup('vit_h', './sam_vit_h_4b8939.pth')\n",
    "\n",
    "        if fine_tuned:\n",
    "            model.load_state_dict(torch.load('model_final.pth', map_location=torch.device(\"cuda:1\")))\n",
    "            \n",
    "        model.eval()\n",
    "        img_size = model.model.image_encoder.img_size\n",
    "        running_vloss = 0.\n",
    "        for images, path, masks in validloader:\n",
    "            orig_image = cv2.imread(path[0])\n",
    "            orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            negative_map = (np.where(orig_image > 0, 1, 0)).astype(np.uint8)\n",
    "            negative_map = torch.from_numpy(negative_map)  \n",
    "            negative_map = negative_map.permute(2, 0, 1)\n",
    "            negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "            negative_map = negative_map.to(torch.uint8).squeeze(0).to(DEVICE)\n",
    "            \n",
    "            # print(images)\n",
    "            model.to(DEVICE)\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks[0].to(DEVICE)\n",
    "            total_mask = get_totalmask(masks)\n",
    "            total_mask = total_mask.to(DEVICE)\n",
    "            model.eval()\n",
    "            preds, iou = model(images, negative_map)\n",
    "            preds = preds.to(DEVICE)\n",
    "            vloss = criterion(preds[0][0].unsqueeze(0).unsqueeze(0), total_mask) # was criterion(preds, total_mask) before. i changed because the multimask_output is set on True\n",
    "            running_vloss += vloss.item()\n",
    "            np_image = images[0].permute(1,2,0).detach().cpu().numpy()\n",
    "            np_mask = masks.detach().cpu().numpy()\n",
    "            np_pred = preds[0].detach().cpu().numpy()\n",
    "    \n",
    "            binary_pred_mask = ((F.sigmoid(preds[0]).cpu().numpy()[0]) > 0.5).astype(int) * 255\n",
    "            # binary_pred_mask = ((preds[0]).cpu().numpy()[0] > 0).astype(int) * 255\n",
    "            \n",
    "            # print(np.mean((F.sigmoid(preds[0]).cpu().numpy()[0])), np.min((F.sigmoid(preds[0]).cpu().numpy()[0])), np.max((F.sigmoid(preds[0]).cpu().numpy()[0])))\n",
    "            # print(np.unique((F.sigmoid(preds[0]).cpu().numpy()[0]) > 0.5).astype(int))\n",
    "            # print(\"validation loss: \", running_vloss)\n",
    "            \n",
    "            # fig, axs = plt.subplots(1, 3)\n",
    "            \n",
    "            # axs[0].imshow(np_image)\n",
    "            # axs[0].axis('off')\n",
    "        \n",
    "            # axs[1].imshow(get_totalmask(masks).detach().cpu().numpy())\n",
    "            # axs[1].set_title('Ground truth masks', fontsize=10)\n",
    "            # axs[1].axis('off')\n",
    "            \n",
    "            # axs[2].imshow(binary_pred_mask)\n",
    "            # axs[2].set_title('Predicted masks', fontsize=10)\n",
    "            # axs[2].axis('off')\n",
    "            # # plt.savefig('on_img_train.png', dpi=300)\n",
    "        \n",
    "            # plt.show()\n",
    "            # plt.close()\n",
    "            \n",
    "    return running_vloss/len(validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43801d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = True\n",
    "\n",
    "if eval:\n",
    "    fine_tuned_loss = []\n",
    "    fine_tuned_loss.append(evaluate_model(fine_tuned = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval:\n",
    "    orig_loss = []\n",
    "    orig_loss.append(evaluate_model(fine_tuned = False))\n",
    "    print('Fine-tuned loss: ', np.mean(fine_tuned_loss), 'Original model loss:', np.mean(orig_loss))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "env_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00c3d159c6084b78aebdfaa0b4c648bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0147d1e2e64e4929b519ce6987dc1dee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "0546728f865145c0acb381ec0e8b37df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4cbf52d2359447aa9980d2329e5af9b0",
        "IPY_MODEL_f450302ca6c74f1c81ec785f2cab2d9b"
       ],
       "layout": "IPY_MODEL_9ae5f610de144230a1f1fd2df6907f07"
      }
     },
     "05aa885cdbe940d58c926e66f15a81fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0649348924104932a7bfe46c692dadf2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "08f13a1d8f66411cb155a1df43e87a7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2eb29e03791348f188840eee9cfc327d",
        "IPY_MODEL_60c02af9af8242dda35c0b62a062afd0"
       ],
       "layout": "IPY_MODEL_f4c3a5f3b22749deb804ca51e575b46f"
      }
     },
     "0a62506b9185432c9197b019f5cc49ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0f6465647c0e4d288495a0da872836f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1725efaf294846438566271b8debac7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_aba96c3009c344e8a68aefeec5ec306a",
       "style": "IPY_MODEL_0147d1e2e64e4929b519ce6987dc1dee",
       "value": "Waiting for wandb.init()...\r"
      }
     },
     "19bd433b774c409e8c5940a4399a2046": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "1e0ec329ad8c41789c10720691eb635b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2eb29e03791348f188840eee9cfc327d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_b377c8757b684ac8bec553aaaf0f52c7",
       "style": "IPY_MODEL_dd701143e8604a649809366eff6c00aa",
       "value": "Waiting for wandb.init()...\r"
      }
     },
     "2fbf271df4754683bd14a8b96413ba2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3704bcb3a1be4e328129ca05a63e4c79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1725efaf294846438566271b8debac7e",
        "IPY_MODEL_d4397bc6788a4e80a4fe85c979ab4175"
       ],
       "layout": "IPY_MODEL_f927483d66f246c1a419f141a1f6d4ef"
      }
     },
     "3797993d28f3450793572e3660e3ac5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "3c0fbf096b114aeb87c8f777b61f2af7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3cc5b4762f5449fc8902304473d9e5b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3ddada81e8904716b7affb381641cab1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_76e52efa5410482db78d64c871cd65e9",
        "IPY_MODEL_e2a61f2740804ad3b55d2fc823482348"
       ],
       "layout": "IPY_MODEL_0649348924104932a7bfe46c692dadf2"
      }
     },
     "3eebcfaa1d4f4ff28e6f9e00854a4719": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "409c60d1dd7042b98e48e99bc7608b71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "447dd1ab0c2b4c80ab1a0ca66319a04e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_49b9508b82a24d6e8521bd1b2245ac6e",
       "style": "IPY_MODEL_19bd433b774c409e8c5940a4399a2046",
       "value": "Waiting for wandb.init()...\r"
      }
     },
     "462abab75d18453ca0c30ddfb22e923f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_bdc1d55f002346d980c6ba8446edb5c3",
       "max": 1,
       "style": "IPY_MODEL_0a62506b9185432c9197b019f5cc49ae",
       "value": 1
      }
     },
     "46db35710d8945a9bc68ab7bd73a270d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "49b9508b82a24d6e8521bd1b2245ac6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b45ca67dd7340f29e59f2bd0d945d18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4cbf52d2359447aa9980d2329e5af9b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_8d21825ca1b74c8e8d2fb172756b2307",
       "style": "IPY_MODEL_d838014236574ef2aa0ab95e738f87b5"
      }
     },
     "564d6c168c0d43b6ac537b222d0b3a1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "58bda5083bd540d69627a27594bdb112": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "5df158d9d50c4203b5633c94feec5b28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_fe5cc483d4fa47c7bf9ab048d2d224cf",
       "max": 1,
       "style": "IPY_MODEL_61e0472dc8ce443fb8e445af8b17418e",
       "value": 1
      }
     },
     "5efc4b3fdaaa417ead9c7fd1c88774ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "60c02af9af8242dda35c0b62a062afd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_66838017e9d34a65ac5272cdc35f224d",
       "max": 1,
       "style": "IPY_MODEL_f4fa804146974921904a1d9c831cda0d",
       "value": 1
      }
     },
     "61e0472dc8ce443fb8e445af8b17418e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "65ba7c120a9a42a88b60e04030936765": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_fd64aed45fcd40a9940d1657fc9bb2a6",
       "max": 1,
       "style": "IPY_MODEL_c61565ada8214f4f827a7de4f8a69f83",
       "value": 1
      }
     },
     "66838017e9d34a65ac5272cdc35f224d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "72ebbc5f49634b5487beddaa4afcd558": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_447dd1ab0c2b4c80ab1a0ca66319a04e",
        "IPY_MODEL_9152f02dd0cc44b48744283ab71894be"
       ],
       "layout": "IPY_MODEL_c3bd2f04d11b403ea7fcd8b7952398b7"
      }
     },
     "755f6b12730349f38b5b8b701485e4d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75e136309acd4abbba43306730f05fe3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c07593cc8d594a1d81c20542dcd938da",
        "IPY_MODEL_5df158d9d50c4203b5633c94feec5b28"
       ],
       "layout": "IPY_MODEL_883fbf16fe0645d68960f5a04a306ded"
      }
     },
     "76e52efa5410482db78d64c871cd65e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_893f57fd87ce431aa67489087ab056a3",
       "style": "IPY_MODEL_564d6c168c0d43b6ac537b222d0b3a1a",
       "value": "Waiting for wandb.init()...\r"
      }
     },
     "7972b637c6ad42df9c93fb156de0c913": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7c23ec2c55a646e68333c7fb719fd8ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_b27fb878e3534f6dac5e370d62a2ff45",
       "style": "IPY_MODEL_58bda5083bd540d69627a27594bdb112",
       "value": "0.052 MB of 0.052 MB uploaded\r"
      }
     },
     "7cf4bfec54fb46e4b3fde590702a63a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "86fd5a23999d45f8abc83af392d70459": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_5efc4b3fdaaa417ead9c7fd1c88774ab",
       "style": "IPY_MODEL_9af1a92cd30645db95d7dfb0a07afedb",
       "value": "0.051 MB of 0.051 MB uploaded\r"
      }
     },
     "883fbf16fe0645d68960f5a04a306ded": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "893f57fd87ce431aa67489087ab056a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8d21825ca1b74c8e8d2fb172756b2307": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8e96ff8a805044df8308af8dd945329a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "8f1f5a4b57784af1afbaf10ff5748b9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8fecc66599274b329c04b133c1951631": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_3eebcfaa1d4f4ff28e6f9e00854a4719",
       "max": 1,
       "style": "IPY_MODEL_1e0ec329ad8c41789c10720691eb635b",
       "value": 1
      }
     },
     "9152f02dd0cc44b48744283ab71894be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_f2cf59cd927f40a3a95d5ae8c46a0bf1",
       "max": 1,
       "style": "IPY_MODEL_8f1f5a4b57784af1afbaf10ff5748b9e",
       "value": 1
      }
     },
     "9ae5f610de144230a1f1fd2df6907f07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9af1a92cd30645db95d7dfb0a07afedb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "9c14fbfad61146da8b3d6a54814ef307": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a734325d672d4733896cfe09fed38d2a",
        "IPY_MODEL_a88d3aca99b445dc98ce1dd16ef980bd"
       ],
       "layout": "IPY_MODEL_3c0fbf096b114aeb87c8f777b61f2af7"
      }
     },
     "9ce2176bf6e247e094d16d90f0a0cfaa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_daa29b9bc76f4d5884c6340475e2a832",
       "style": "IPY_MODEL_3797993d28f3450793572e3660e3ac5c",
       "value": "0.150 MB of 0.150 MB uploaded\r"
      }
     },
     "a5e9cb133b684d5b981de95e49931ff1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a734325d672d4733896cfe09fed38d2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_c526fb972eb74bd39b5f95b7ae81c321",
       "style": "IPY_MODEL_8e96ff8a805044df8308af8dd945329a"
      }
     },
     "a88d3aca99b445dc98ce1dd16ef980bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_7cf4bfec54fb46e4b3fde590702a63a5",
       "max": 1,
       "style": "IPY_MODEL_c826e3641a384f70a1fde66c295eba89"
      }
     },
     "aba96c3009c344e8a68aefeec5ec306a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "af214c31a993468d978a16701a115587": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b27fb878e3534f6dac5e370d62a2ff45": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b377c8757b684ac8bec553aaaf0f52c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b665731930184b9e809344a70284e6f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b69bcfa8e7704600bef9604b59211b55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_edcc5e89eaed425888cf522f438f8c6e",
       "max": 1,
       "style": "IPY_MODEL_4b45ca67dd7340f29e59f2bd0d945d18",
       "value": 1
      }
     },
     "bdc1d55f002346d980c6ba8446edb5c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c07593cc8d594a1d81c20542dcd938da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_f560dd404ae34b179227fb80908613e9",
       "style": "IPY_MODEL_e2bdf2c8e481490087b0cd4854ae6f8f",
       "value": "Waiting for wandb.init()...\r"
      }
     },
     "c3bd2f04d11b403ea7fcd8b7952398b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c526fb972eb74bd39b5f95b7ae81c321": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c61565ada8214f4f827a7de4f8a69f83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c826e3641a384f70a1fde66c295eba89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cd03cbb79d4f4dcf9d47133507502d3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_05aa885cdbe940d58c926e66f15a81fe",
       "style": "IPY_MODEL_f4109011fa9b42ac84c269c8baeef0e9",
       "value": "0.052 MB of 0.052 MB uploaded\r"
      }
     },
     "d13ca0a80ff54505861462856f036b34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_de616a80a9d64533b4aa13de2de581a2",
       "max": 1,
       "style": "IPY_MODEL_af214c31a993468d978a16701a115587",
       "value": 1
      }
     },
     "d4397bc6788a4e80a4fe85c979ab4175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_409c60d1dd7042b98e48e99bc7608b71",
       "max": 1,
       "style": "IPY_MODEL_46db35710d8945a9bc68ab7bd73a270d",
       "value": 1
      }
     },
     "d838014236574ef2aa0ab95e738f87b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "d9bc3637a70541989f8a8fd1c59fedfa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "daa29b9bc76f4d5884c6340475e2a832": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dd701143e8604a649809366eff6c00aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "de616a80a9d64533b4aa13de2de581a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dedd3f691af84d3e818d196cba9a2d23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e1b92aee7cd64e2b8ed777fa4780632d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_dedd3f691af84d3e818d196cba9a2d23",
       "style": "IPY_MODEL_e2f55a35d98f41d2b57e302053eeada1",
       "value": "0.052 MB of 0.052 MB uploaded\r"
      }
     },
     "e2a61f2740804ad3b55d2fc823482348": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_7972b637c6ad42df9c93fb156de0c913",
       "max": 1,
       "style": "IPY_MODEL_b665731930184b9e809344a70284e6f7",
       "value": 1
      }
     },
     "e2bdf2c8e481490087b0cd4854ae6f8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "e2f55a35d98f41d2b57e302053eeada1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "edcc5e89eaed425888cf522f438f8c6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f2cf59cd927f40a3a95d5ae8c46a0bf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4109011fa9b42ac84c269c8baeef0e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "f450302ca6c74f1c81ec785f2cab2d9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_3cc5b4762f5449fc8902304473d9e5b6",
       "max": 1,
       "style": "IPY_MODEL_a5e9cb133b684d5b981de95e49931ff1"
      }
     },
     "f4c3a5f3b22749deb804ca51e575b46f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4fa804146974921904a1d9c831cda0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f560dd404ae34b179227fb80908613e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f927483d66f246c1a419f141a1f6d4ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fd64aed45fcd40a9940d1657fc9bb2a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fe5cc483d4fa47c7bf9ab048d2d224cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
