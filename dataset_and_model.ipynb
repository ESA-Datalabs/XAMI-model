{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ESA-Datalabs/XAMI-model/blob/main/dataset_and_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MfC5nMEkhUa",
    "outputId": "cbbc24d4-ae20-4385-9a2a-8f38ba0d1b11"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q condacolab\n",
    "    import condacolab\n",
    "    condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZktHME-kuIY",
    "outputId": "ced95049-66f1-48da-db19-ebbcd551684c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'XAMI-model'...\n",
      "remote: Enumerating objects: 1139, done.\u001b[K\n",
      "remote: Counting objects: 100% (238/238), done.\u001b[K\n",
      "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
      "remote: Total 1139 (delta 139), reused 238 (delta 139), pack-reused 901\u001b[K\n",
      "Receiving objects: 100% (1139/1139), 109.78 MiB | 21.16 MiB/s, done.\n",
      "Resolving deltas: 100% (632/632), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ESA-Datalabs/XAMI-model.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Awltjo-1jmpS"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"XAMI-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJ2IW3TpkAQm",
    "outputId": "1f2b818a-e7ba-4522-8df0-3d18202fa0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      " - local\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): \\ "
     ]
    }
   ],
   "source": [
    "!conda env update -n base -f environment.yaml\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ThBe7tOa_D-1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'XAMI-dataset'...\n",
      "remote: Enumerating objects: 235, done.\u001b[K\n",
      "remote: Counting objects: 100% (235/235), done.\u001b[K\n",
      "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
      "remote: Total 235 (delta 112), reused 172 (delta 52), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (235/235), 2.30 MiB | 10.37 MiB/s, done.\n",
      "Resolving deltas: 100% (112/112), done.\n",
      "^C\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n",
      "\n",
      "Exiting because of \"interrupt\" signal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ESA-Datalabs/XAMI-dataset.git\n",
    "!pip install -e XAMI-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in colab, there is an issue with recognising these packages \n",
    "if 'google.colab' in sys.modules: \n",
    "  sys.path.append(os.getcwd())\n",
    "  sys.path.append(os.path.join(os.getcwd(), 'XAMI-dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193,
     "referenced_widgets": [
      "a20c0c1da8c146549d519118ce184559",
      "a6aaab6918ab4889ad2ebc14635ac0c1",
      "11e968e2c2874eb185b9e5e567e2cfdb",
      "c46c11990173470ab6882995823f23b0",
      "4610695017ee42e581ec857a74635953",
      "4d6c2afd6a134d4faa4665eb68572d57",
      "ede2bb577b3b49879eb7d63a247faac5",
      "b0665a869f0443eeb1c58f21cb094405",
      "34a70984f1b34a359c98c31ceac7fd07",
      "a6895b74fa4d42bfbb42b9ebaf5b7e32",
      "466462ad232840679380999a08736eca"
     ]
    },
    "id": "AGN65cHH_Am8",
    "outputId": "43e055a6-f886-4606-c78f-31cd88551e75"
   },
   "outputs": [],
   "source": [
    "from xami_dataset.loader import XAMIDataset\n",
    "\n",
    "# Download the dataset\n",
    "xami_dataset = XAMIDataset(\n",
    "    repo_id=\"iulia-elisa/XAMI-dataset\",\n",
    "    dataset_name=\"xami_dataset\",\n",
    "    dest_dir='./data_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nB3dUtiW_hpa",
    "outputId": "6ad32e69-e438-4856-8ef1-6a6719ab57ff"
   },
   "outputs": [],
   "source": [
    "from xami_dataset.xami_utils import coco_to_yolo_converter\n",
    "\n",
    "convert = True # if True, converts coco dataset to yolo format\n",
    "\n",
    "if convert: \n",
    "    coco_to_yolo_converter.convert_coco_to_yolo(\n",
    "    dir_absolute_path=os.getcwd(),\n",
    "    dataset_path='./data_2/xami_dataset/',\n",
    "    yolo_dataset_path=os.path.join(os.getcwd(), 'data_2/xami_dataset_YOLO')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "c4d37397b84b4b6bbdeeaf4971609bd9",
      "64e249bfa3fa4fdcaae84de6f31c8146",
      "ed2a745a776e4af09e8148d21f1a1826",
      "0bcd443fdd8546a8ab4185453f779d49",
      "fc4efffd528847e5b93851709a86cc14",
      "e7a0d29449444134b95f1acbc1e5535b",
      "36fc6696961c4f07911c9bacab6c7dab",
      "91ac0af6ac4544f58aca6bc10084796c",
      "90f3b9b6270b4030ab1efd7e99ccbc71",
      "6f914e9041874443a8e9786243056bf4",
      "9114b9fabeb049b9bfbf12ed81452e22",
      "e70558182c2341548bfc59a8ff95b272",
      "14d01e2ae15148eba4ca400dfafbee0f",
      "6b8247b5300e4c8fb2747bab098eb174",
      "083a0273c2c1462e9c011e962090e03f",
      "31b80aedc9bc49108378071e68da8b7c",
      "a23751b29b0044a49706c4734deeb43f",
      "295c14a6a1f0411193cffb1feb627160",
      "370bbfdd4ab14d3ab4499bf974fcea26",
      "0989d38d66db48a79557018264e520be",
      "124e4ef786d64b07b71d03efec327827",
      "7fc9a5a9b0f7408b804ebba025cc56c6",
      "1373f70385474c0791e3bdf6e0d56c77",
      "882364a5cf41461294cc5d5ef3e1801f",
      "4b7d8f6f8aef40218a4c14cec0643fbe",
      "e3eb63940fea439ca278ffaab8877652",
      "10f42fad439d4476a058da329d18065f",
      "083ce9da08974ef083db460a8b6676dd",
      "dfa31f6df4cf406c9d7585764b67bca8",
      "3e9a71798ea34e1796575ae79637803a",
      "1863587a7bad4e1c91c8777ffb061158",
      "79aae3a1ce4d4a9ca789d39c08bfaa9e",
      "7425989f119e425e95a650a821810c65"
     ]
    },
    "id": "btLdQ8eL_kLC",
    "outputId": "623d0e0b-4af2-477d-a5a0-c633cbfc6225"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "dest_dir = './xami_model/train/weights/'\n",
    "checkpoints = [\n",
    "    'yolo_weights/yolov8_detect_300e_best.pt', \n",
    "    'yolo_weights/yolov8_detect_300e_last.pt', \\\n",
    "    'sam_weights/sam_0_best.pth', \n",
    "    'yolo_sam_final.pth']\n",
    "\n",
    "# download checkpoints from HuggingFace\n",
    "for checkpoint in checkpoints:\n",
    "    hf_hub_download(\n",
    "    \t\t\trepo_id=\"iulia-elisa/XAMI-model\",\n",
    "    \t\t\trepo_type='model',\n",
    "    \t\t\tfilename=checkpoint,\n",
    "    \t\t\tlocal_dir=dest_dir\n",
    "    \t\t)\n",
    "print(f'Checkpoints saved in {dest_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBgIKSOC96Yn",
    "outputId": "8c30dbf5-0753-4b7c-ed9f-621084ab5d92",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xami_model.inference.xami_inference import InferXami\n",
    "\n",
    "detr_checkpoint = './xami_model/train/rt-detr-iter0/rtdetr-l4/weights/last.pt'\n",
    "# detr_checkpoint = './train/yolov8-segm-0/yolov8n-seg6/weights/best.pt'\n",
    "sam_checkpoint = './xami_model/train/output_sam_8/sam_0_2024-06-05 13:08:02.871454_best.pth'\n",
    "\n",
    "# the SAM checkpoint and model_type (vit_h, vit_t, etc.) must be compatible\n",
    "detr_sam_pipeline = InferXami(\n",
    "    device='cuda:0',\n",
    "    detr_checkpoint=detr_checkpoint,\n",
    "    sam_checkpoint=sam_checkpoint,\n",
    "    model_type='vit_t',\n",
    "    use_detr_masks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "ikqBGvsD-nJi",
    "outputId": "51cd3654-f8e2-4ffb-f73a-286a49800b77",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# masks = detr_sam_pipeline.run_predict('../XAMI-dataset/xami_dataset/valid/S0743200101_V_png.rf.dba4ec263252737ec92121f599a4ca8b.jpg', show_masks=True)\n",
    "# masks = detr_sam_pipeline.run_predict('./example_images/S0743200101_V.jpg', show_masks=True) \n",
    "masks = detr_sam_pipeline.run_predict('../XAMI-dataset/data/xami_dataset/valid/S0604980201_L_png.rf.50a2ef55be16b3e7f688869309e22052.jpg', show_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "Btbdsbqc-wRu",
    "outputId": "228d0295-879a-4b37-c2bf-b6958794bc01"
   },
   "outputs": [],
   "source": [
    "masks = detr_sam_pipeline.run_predict('./example_images/S0893811101_M.png', show_masks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SanhfVPp3Pxe"
   },
   "outputs": [],
   "source": [
    "# Restarting and running from this point would be better\n",
    "# Because after inference, there is a model already on GPU => OOM\n",
    "\n",
    "import os\n",
    "os.chdir(\"XAMI-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5dL_DT8AhMd"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0, 1, 2, 3\" # replace with the GPU IDs that are available\n",
    "\n",
    "# to help with reproducibility\n",
    "seed=0\n",
    "import torch.backends.cudnn as cudnn\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.benchmark, cudnn.deterministic = False, True\n",
    "\n",
    "from xami_model.model_predictor import predictor_utils, xami #, residualAttentionBlock\n",
    "from xami_model.losses import metrics_utils\n",
    "from xami_model.dataset import dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okH597tbA1Oj"
   },
   "outputs": [],
   "source": [
    "kfold_iter=0\n",
    "\n",
    "device_id=0\n",
    "batch_size=4\n",
    "lr=3e-5\n",
    "wd=0.0005\n",
    "wandb_track=False\n",
    "torch.cuda.set_device(device_id)\n",
    "datetime_now = datetime.now()\n",
    "\n",
    "if wandb_track:\n",
    "    # !pip install wandb\n",
    "    # !wandb login\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=\"yolo-sam\", name=f\"yolo-sam {datetime_now}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9af7X8_fA1Ss",
    "outputId": "e2b17b36-f3e7-42e1-d99f-aa7695ae70ac"
   },
   "outputs": [],
   "source": [
    "yolo_dataset_path = './data/xami_dataset_YOLO/' # replace with the path to the YOLO dataset\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(yolo_dataset_path+f\"data.yaml\", 'r') as stream:\n",
    "    yam_data = yaml.safe_load(stream) # dictionary with keys 'names', 'nc', 'train', 'val'\n",
    "\n",
    "classes = {i:name for i, name in enumerate(yam_data['names'])}\n",
    "train_path = yam_data['train']\n",
    "val_path = yam_data['val']\n",
    "print('Classes:', classes)\n",
    "\n",
    "coco_data_path = f'./data/xami_dataset/'\n",
    "annotations_file = '_annotations.coco.json'\n",
    "\n",
    "with open(coco_data_path+'train/'+annotations_file, 'r') as f1, open(coco_data_path+'valid/'+annotations_file, 'r') as f2:\n",
    "    train_coco_data = json.load(f1)\n",
    "    valid_coco_data = json.load(f2)\n",
    "\n",
    "train_dir = yolo_dataset_path+f'train/images/'\n",
    "valid_dir = yolo_dataset_path+f'valid/images/'\n",
    "\n",
    "train_image_files = os.listdir(train_dir)\n",
    "valid_image_files = os.listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), 'xami_model/train/weights/yolo_weights/last.pt')\n",
    "print(f\"Looking for YOLO model at: {file_path}\")\n",
    "\n",
    "assert os.path.exists(file_path), f\"File not found: {file_path}\"\n",
    "# raid/OM_DeepLearning/XAMI-model/xami_model/train/weights/yolo_weights/last.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GT9V5slDPhP",
    "outputId": "0ffe4959-43fc-4b21-a28e-37f88ff5720a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xami_model.mobile_sam.mobile_sam import sam_model_registry, SamPredictor\n",
    "\n",
    "device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobile_sam_checkpoint = './xami_model/train/weights/sam_weights/sam_0_best.pth'\n",
    "yolov8_pretrained_model = YOLO(os.path.join(os.getcwd(), 'xami_model/train/weights/yolo_weights/yolov8_detect_300e_last.pt'));\n",
    "yolov8_pretrained_model.to(f'cuda:{device_id}');\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device)\n",
    "predictor = SamPredictor(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqWxL29YDZro"
   },
   "outputs": [],
   "source": [
    "xami_model = xami.XAMI(mobile_sam_model, device, predictor)\n",
    "# xami_model = xami.XAMI(\n",
    "#     mobile_sam_model,\n",
    "#     device,\n",
    "#     predictor,\n",
    "#     use_yolo_masks=True,\n",
    "#     wt_threshold=0.6,\n",
    "#     wt_classes_ids = [1.0, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkExU_cDD1oD",
    "outputId": "7eb93485-c3a0-4ebc-a052-72c4d058cf9a"
   },
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    if 'mask_decoder' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "print(f\"ðŸš€ The model has {sum(p.numel() for p in xami_model.model.parameters() if p.requires_grad)} trainable parameters.\")\n",
    "# print(f\"ðŸš€ The residual attention block has {sum(p.numel() for p in xami_model.residualAttentionBlock.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "\n",
    "# predictor_utils.check_requires_grad(xami_model.model)\n",
    "# predictor_utils.check_requires_grad(xami_model.residualAttentionBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKw89tRID2Cv"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_num_batches = len(train_image_files) // batch_size\n",
    "valid_num_batches = len(valid_image_files) // batch_size\n",
    "parameters_to_optimize = [param for param in mobile_sam_model.mask_decoder.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(parameters_to_optimize, lr=lr, weight_decay=wd) if len(parameters_to_optimize) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BduaqR2D2FP"
   },
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "\n",
    "metric_thresholds = [[0.5], [0.75], [0.5, 0.9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twjsELVKD7Ut"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 1\n",
    "n_epochs_stop = num_epochs // 10 + 5\n",
    "all_metrics = defaultdict(dict)\n",
    "compute_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXkjuBQdD843",
    "outputId": "4f89f6b4-9096-4da4-a4d1-c4876457a9dc"
   },
   "outputs": [],
   "source": [
    "# Intro\n",
    "predictor_utils.print_training_intro(\n",
    "    train_image_files, valid_image_files, device, metric_thresholds, num_epochs,\n",
    "    batch_size, lr, wd, wandb_track, mobile_sam_model, 'AdamW')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Train\n",
    "    xami_model.model.train()\n",
    "\n",
    "    # xami_model.residualAttentionBlock.train()\n",
    "    train_results = xami_model.run_yolo_sam_epoch(\n",
    "        yolov8_pretrained_model,\n",
    "        phase='train',\n",
    "        batch_size=batch_size,\n",
    "        image_files=train_image_files,\n",
    "        images_dir=train_dir,\n",
    "        num_batches=train_num_batches,\n",
    "        optimizer=optimizer)\n",
    "\n",
    "    epoch_sam_loss_train, train_preds, train_gts = train_results[:3]\n",
    "    train_gt_classes, train_pred_classes, train_all_iou_scores, train_mask_areas, _ = train_results[3:]\n",
    "\n",
    "    # Validate\n",
    "    xami_model.model.eval()\n",
    "    # xami_model.residualAttentionBlock.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_results = xami_model.run_yolo_sam_epoch(\n",
    "            yolov8_pretrained_model,\n",
    "            phase='val',\n",
    "            batch_size=batch_size,\n",
    "            image_files=valid_image_files,\n",
    "            images_dir=valid_dir,\n",
    "            num_batches=valid_num_batches,\n",
    "            optimizer=None)\n",
    "\n",
    "    epoch_sam_loss_val, valid_preds, valid_gts = valid_results[:3]\n",
    "    valid_gt_classes, valid_pred_classes, valid_all_iou_scores, valid_mask_areas, pred_images = valid_results[3:]\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch_sam_loss_val < best_valid_loss:\n",
    "        best_valid_loss = epoch_sam_loss_val\n",
    "        best_model = xami_model.model\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print(\"Early stopping initiated.\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "    # Metrics\n",
    "    if compute_metrics:\n",
    "        for threshold in tqdm(metric_thresholds, desc=\"(Metrics) Processing thresholds\", bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}'):\n",
    "            metric = MeanAveragePrecision(\n",
    "            iou_type = \"segm\",\n",
    "            iou_thresholds = threshold,\n",
    "            max_detection_thresholds=[1, 10, 100],\n",
    "            class_metrics=True,\n",
    "            extended_summary=False)\n",
    "\n",
    "            train_metrics = metrics_utils.mAP_metrics(metric,\n",
    "            train_preds,\n",
    "            train_gts,\n",
    "            train_gt_classes,\n",
    "            train_pred_classes,\n",
    "            train_all_iou_scores,\n",
    "            train_mask_areas,\n",
    "            show_metrics=False)\n",
    "\n",
    "            valid_metrics = metrics_utils.mAP_metrics(metric,\n",
    "                    valid_preds,\n",
    "                    valid_gts,\n",
    "                    valid_gt_classes,\n",
    "                    valid_pred_classes,\n",
    "                    valid_all_iou_scores,\n",
    "                    valid_mask_areas,\n",
    "                    show_metrics=False)\n",
    "\n",
    "            all_metrics[tuple(threshold)] = {'train': train_metrics, 'valid': valid_metrics}\n",
    "\n",
    "        # Prints\n",
    "        if not wandb_track:\n",
    "            wandb = None\n",
    "        predictor_utils.prints_and_wandb(\n",
    "            epoch,\n",
    "            epoch_sam_loss_train,\n",
    "            epoch_sam_loss_val,\n",
    "            all_metrics,\n",
    "            metric_thresholds,\n",
    "            wandb)\n",
    "\n",
    "#     # Checkpoint save\n",
    "#     torch.save(best_model.state_dict(), f'yolo_sam_{epoch}.pth')\n",
    "\n",
    "# torch.save(best_model.state_dict(), f'yolo_sam_final.pth')\n",
    "\n",
    "# Finish wandb run\n",
    "if wandb_track:\n",
    "    run.finish()\n",
    "\n",
    "def convert_tensors(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_tensors(value) for key, value in data.items()}\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.tolist() if data.ndim > 0 else data.item()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "if compute_metrics:\n",
    "    metrics_dict = defaultdict(dict)\n",
    "    metrics_dict[f'{metric_thresholds[0][0]}'] = convert_tensors(all_metrics[tuple(metric_thresholds[0])])\n",
    "    metrics_dict[f'{metric_thresholds[1][0]}'] = convert_tensors(all_metrics[tuple(metric_thresholds[1])])\n",
    "    metrics_dict[f'{metric_thresholds[2][0]}-{metric_thresholds[2][1]}'] = convert_tensors(all_metrics[tuple(metric_thresholds[2])])\n",
    "    metrics_dict['best_epoch'] = best_epoch\n",
    "\n",
    "    with open(f'metrics_{datetime_now}.json', 'w') as json_file:\n",
    "        json.dump(metrics_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwitgN4AIEM3",
    "outputId": "e202bb93-5787-40dc-9f8e-b57f75489067",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_dict"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (xami_model_env_stable_3)",
   "language": "python",
   "name": "xami_model_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
