{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check current model prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from YoloSamPipeline import YoloSam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yolo_path = './yolov8-segm-ft_no_stars-n/200_epochs-no_stars-n-even_fewer_obj25/weights/best.pt' \n",
    "\n",
    "# yolo_sam_pipe = YoloSam(\n",
    "#     device='cuda:0', \n",
    "#     yolo_checkpoint=yolo_path, \n",
    "#     sam_checkpoint='./ft_mobile_sam_final.pth', \n",
    "#     model_type='vit_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo_sam_pipe.run_predict('../XMM_OM_dataset/zscaled_512_stretched/S0140160101_M.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the YOLOv8 pretrained model\n",
    "\n",
    "- The model is pretrained (in another notebook)  using a Roboflow dataset version on OM images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "id": "eotMLol5O5G0",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "import json\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from typing import Any, Dict, Generator, List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from dataset import dataset_utils\n",
    "from class_agnostic_sam_predictor import predictor_utils\n",
    "from losses import loss\n",
    "\n",
    "import torch.autograd.profiler as profiler\n",
    "device_id = 3\n",
    "torch.cuda.set_device(device_id) # â—ï¸â—ï¸â—ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "\n",
    "if use_wandb:\n",
    "    from datetime import datetime\n",
    "    # !pip install wandb\n",
    "    # !wandb login --relogin\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=\"yolo-sam\", name=f\"yolo-sam {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dataset (YOLOv8 format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yolo_dataset_path = './roboflow_datasets/xmm_om_artefacts_512-3-YOLO/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'central-ring', 1: 'smoke-ring', 2: 'star-loop'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(yolo_dataset_path+\"data.yaml\", 'r') as stream:\n",
    "    yam_data = yaml.safe_load(stream) # dict with keys 'names', 'nc', 'roboflow', 'test', 'train', 'val'\n",
    "yam_data['names']\n",
    "\n",
    "classes = {i:name for i, name in enumerate(yam_data['names'])}\n",
    "train_path = yam_data['train']\n",
    "val_path = yam_data['val']\n",
    "test_path = yam_data['test']\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get masks from dataset (in YOLOv8 format) given an image file\n",
    "\n",
    "def get_label_file_path(dataset_path, image_location):\n",
    "    dataset_path = '/'.join(dataset_path.split('/')[:-2])+'/'+'labels'+'/'\n",
    "    label_file_path = os.path.join(dataset_path, image_location)\n",
    "    label_loc = '.'.join(image_location.split('.')[:-1]) + '.txt'\n",
    "    label_file_path = dataset_path+label_loc\n",
    "    return label_file_path\n",
    "\n",
    "def read_annotations(label_file_path):\n",
    "    annotations = []\n",
    "    k = 0\n",
    "    with open(label_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            print(k+1)\n",
    "            k+=1\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            segmentation_points = [float(p) for p in parts[1:]]\n",
    "            annotations.append({\n",
    "                'class_id': class_id,\n",
    "                'segmentation_points': segmentation_points\n",
    "            })\n",
    "    return annotations\n",
    "\n",
    "def get_masks_from_image(yolo_dataset_path, image_location):\n",
    "    label_file_path = get_label_file_path(yolo_dataset_path, image_location)\n",
    "    annotations = read_annotations(label_file_path)\n",
    "    masks = [dataset_utils.create_mask_0_1(annot['segmentation_points'], (512, 512)) for annot in annotations]\n",
    "    return masks\n",
    "\n",
    "def get_classes_from_image(yolo_dataset_path, image_location):\n",
    "    label_file_path = get_label_file_path(yolo_dataset_path, image_location)\n",
    "    annotations = read_annotations(label_file_path)\n",
    "    class_ids = [annot['class_id'] for annot in annotations]\n",
    "    return class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperparameters docs: https://docs.ultralytics.com/usage/cfg/#train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OmQtdzgx2Noc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_masks(masks, ax, random_color=False):\n",
    "    for mask in masks:\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        else:\n",
    "                color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couple YOLO bboxes with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((anns.shape[1], anns.shape[2], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in range(anns.shape[0]):\n",
    "        m = anns[ann].bool()\n",
    "        m=m.cpu().numpy()\n",
    "        color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def batch_iterator(batch_size: int, *args) -> Generator[List[Any], None, None]:\n",
    "    assert len(args) > 0 and all(\n",
    "        len(a) == len(args[0]) for a in args\n",
    "    ), \"Batched iteration must have inputs of all the same size.\"\n",
    "    n_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)\n",
    "    for b in range(n_batches):\n",
    "        yield [arg[b * batch_size : (b + 1) * batch_size] for arg in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**load SAM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/ft_mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with ft_mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/ft_mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with ft_mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/ft_mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with ft_mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/ft_mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with ft_mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/ft_mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with ft_mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/opt/conda/envs/env_py311/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/opt/conda/envs/env_py311/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/opt/conda/envs/env_py311/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/opt/conda/envs/env_py311/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/opt/conda/envs/env_py311/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/')\n",
    "from ft_mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/weights/mobile_sam.pt\"\n",
    "mobile_sam_checkpoint = \"./ft_mobile_sam_final.pth\"\n",
    "\n",
    "yolov8_pretrained_model = YOLO('./yolov8-segm-ft_no_stars-n/200_epochs-no_stars-n-even_fewer_obj25/weights/best.pt');\n",
    "yolov8_pretrained_model.to(f'cuda:{device_id}');\n",
    "\n",
    "device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model = sam_model_registry[\"vit_h\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device)\n",
    "predictor = SamPredictor(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = yolo_dataset_path+'train/images/'\n",
    "valid_dir = yolo_dataset_path+'valid/images/'\n",
    "train_image_files = os.listdir(train_dir)\n",
    "valid_image_files = os.listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    params_to_train = ['mask_tokens', 'output_upscaling', 'output_hypernetworks_mlps', 'iou_prediction_head']\n",
    "    if 'mask_decoder' in name: # and any(s in name for s in params_to_train):\n",
    "    # if True:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ The model has 0 trainable parameters.\n",
      "\n",
      "âŒ Param image_encoder.pos_embed  doesn't require grad.\n",
      "âŒ Param image_encoder.patch_embed.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.patch_embed.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.0.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.1.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.2.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.3.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.4.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.5.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.6.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.7.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.8.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.9.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.10.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.11.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.12.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.13.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.14.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.15.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.16.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.17.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.18.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.19.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.20.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.21.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.22.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.23.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.24.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.25.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.26.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.27.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.28.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.29.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.30.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.norm1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.norm1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.attn.rel_pos_h  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.attn.rel_pos_w  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.attn.qkv.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.attn.qkv.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.attn.proj.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.attn.proj.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.norm2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.norm2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.blocks.31.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.neck.0.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.neck.1.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.neck.1.bias  doesn't require grad.\n",
      "âŒ Param image_encoder.neck.2.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.neck.3.weight  doesn't require grad.\n",
      "âŒ Param image_encoder.neck.3.bias  doesn't require grad.\n",
      "âŒ Param prompt_encoder.point_embeddings.0.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.point_embeddings.1.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.point_embeddings.2.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.point_embeddings.3.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.not_a_point_embed.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.0.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.0.bias  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.1.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.1.bias  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.3.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.3.bias  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.4.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.4.bias  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.6.weight  doesn't require grad.\n",
      "âŒ Param prompt_encoder.mask_downscaling.6.bias  doesn't require grad.\n",
      "âŒ Param prompt_encoder.no_mask_embed.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.self_attn.q_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.self_attn.q_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.self_attn.k_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.self_attn.k_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.self_attn.v_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.self_attn.v_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.self_attn.out_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.self_attn.out_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.norm1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.norm1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.norm2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.norm2.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.norm3.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.norm3.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.norm4.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.norm4.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.self_attn.q_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.self_attn.q_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.self_attn.k_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.self_attn.k_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.self_attn.v_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.self_attn.v_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.self_attn.out_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.self_attn.out_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.norm1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.norm1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.norm2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.norm2.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.mlp.lin1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.mlp.lin1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.mlp.lin2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.mlp.lin2.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.norm3.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.norm3.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.norm4.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.norm4.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.final_attn_token_to_image.q_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.final_attn_token_to_image.q_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.final_attn_token_to_image.k_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.final_attn_token_to_image.k_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.final_attn_token_to_image.v_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.final_attn_token_to_image.v_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.final_attn_token_to_image.out_proj.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.final_attn_token_to_image.out_proj.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.norm_final_attn.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.transformer.norm_final_attn.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.iou_token.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.mask_tokens.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_upscaling.0.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_upscaling.0.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_upscaling.1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_upscaling.1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_upscaling.3.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_upscaling.3.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.0.layers.0.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.0.layers.0.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.0.layers.1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.0.layers.1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.0.layers.2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.0.layers.2.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.1.layers.0.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.1.layers.0.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.1.layers.1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.1.layers.1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.1.layers.2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.1.layers.2.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.2.layers.0.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.2.layers.0.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.2.layers.1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.2.layers.1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.2.layers.2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.2.layers.2.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.3.layers.0.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.3.layers.0.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.3.layers.1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.3.layers.1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.3.layers.2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.output_hypernetworks_mlps.3.layers.2.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.iou_prediction_head.layers.0.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.iou_prediction_head.layers.0.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.iou_prediction_head.layers.1.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.iou_prediction_head.layers.1.bias  doesn't require grad.\n",
      "âŒ Param mask_decoder.iou_prediction_head.layers.2.weight  doesn't require grad.\n",
      "âŒ Param mask_decoder.iou_prediction_head.layers.2.bias  doesn't require grad.\n"
     ]
    }
   ],
   "source": [
    "def check_requires_grad(model, show=True):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and show:\n",
    "            print(\"âœ… Param\", name, \" requires grad.\")\n",
    "        elif param.requires_grad == False:\n",
    "            print(\"âŒ Param\", name, \" doesn't require grad.\")\n",
    "\n",
    "print(f\"ğŸš€ The model has {sum(p.numel() for p in mobile_sam_model.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "check_requires_grad(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m wd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m\n\u001b[1;32m     13\u001b[0m parameters_to_optimize \u001b[38;5;241m=\u001b[39m [param \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m mobile_sam_model\u001b[38;5;241m.\u001b[39mmask_decoder\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[0;32m---> 14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters_to_optimize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#betas=(0.9, 0.999))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/env_py311/lib/python3.11/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m/opt/conda/envs/env_py311/lib/python3.11/site-packages/torch/optim/optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    259\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    263\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 12\n",
    "train_num_batches = len(train_image_files) // batch_size\n",
    "valid_num_batches = len(valid_image_files) // batch_size\n",
    "\n",
    "lr=6e-5\n",
    "wd=0.0005\n",
    "parameters_to_optimize = [param for param in mobile_sam_model.mask_decoder.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.Adam(parameters_to_optimize, lr=lr, weight_decay=wd) #betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "def run_epoch(phase, image_files, images_dir, num_batches, model_, optimizer=None, train_encoders=False):\n",
    "    assert phase in ['train', 'val'], \"Phase must be 'train' or 'val'\"\n",
    "    \n",
    "    if phase == 'train':\n",
    "        model_.train()  \n",
    "    else:\n",
    "        model_.eval() \n",
    "\n",
    "    epoch_sam_loss = []\n",
    "    epoch_yolo_loss = []\n",
    "    all_preds, all_gts, all_pred_cls, all_gt_cls, = [], [], [], []\n",
    "    for batch_idx in tqdm(range(num_batches), desc=f'{phase[0].upper()+phase[1:]} Progress', bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}'):\n",
    "        start_idx = batch_idx * 289\n",
    "        end_idx = start_idx + 289\n",
    "        batch_files = image_files[start_idx:end_idx]\n",
    "\n",
    "        batch_losses_sam = []\n",
    "        batch_losses_yolo = []\n",
    "\n",
    "        for image_name in batch_files:\n",
    "            image_path = images_dir + image_name\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            obj_results = yolov8_pretrained_model.predict(image_path, verbose=False, conf=0.2) \n",
    "            if train_encoders:\n",
    "                predictor.set_image(image)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    predictor.set_image(image)\n",
    "            # sets a specific mean for each image\n",
    "            # image_T = np.transpose(image, (2, 1, 0))\n",
    "            # mean_ = np.mean(image_T[image_T>0])\n",
    "            # std_ = np.std(image_T[image_T>0]) \n",
    "            # pixel_mean = torch.as_tensor([mean_, mean_, mean_], dtype=torch.float, device=device)\n",
    "            # pixel_std = torch.as_tensor([std_, std_, std_], dtype=torch.float, device=device)\n",
    "    \n",
    "            # mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).unsqueeze(-1).unsqueeze(-1), False) # not in SAM\n",
    "            # mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).unsqueeze(-1).unsqueeze(-1), False) # not in SAM\n",
    "                \n",
    "            gt_masks = get_masks_from_image(images_dir, image_name) \n",
    "            gt_classes = get_classes_from_image(images_dir, image_name) \n",
    "            if len(obj_results[0]) == 0 or len(gt_masks) == 0:\n",
    "                continue\n",
    "                \n",
    "            input_boxes1 = obj_results[0].boxes.xyxy\n",
    "            expand_by = 2.5\n",
    "            enlarged_bbox = input_boxes1.clone() \n",
    "            enlarged_bbox[:, :2] -= expand_by  \n",
    "            enlarged_bbox[:, 2:] += expand_by  \n",
    "            input_boxes1 = enlarged_bbox\n",
    "            input_boxes = input_boxes1.cpu().numpy()\n",
    "            input_boxes = predictor.transform.apply_boxes(input_boxes, predictor.original_size)\n",
    "            input_boxes = torch.from_numpy(input_boxes).to(device)\n",
    "            sam_mask, yolo_masks = [], []\n",
    "            \n",
    "            if train_encoders:\n",
    "                image_embedding=predictor.features\n",
    "                prompt_embedding=model_.prompt_encoder.get_dense_pe()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    image_embedding=predictor.features\n",
    "                    prompt_embedding=model_.prompt_encoder.get_dense_pe()\n",
    "                \n",
    "            non_resized_masks = obj_results[0].masks.data.cpu().numpy()\n",
    "            \n",
    "            for i in range(len(non_resized_masks)):\n",
    "                    yolo_masks.append(cv2.resize(non_resized_masks[i], image.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)) \n",
    "\n",
    "            for (boxes,) in batch_iterator(320, input_boxes): \n",
    "                if train_encoders:\n",
    "                    sparse_embeddings, dense_embeddings = model_.prompt_encoder(\n",
    "                        points=None,\n",
    "                        boxes=boxes,\n",
    "                        masks=None,)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        sparse_embeddings, dense_embeddings = model_.prompt_encoder(\n",
    "                            points=None,\n",
    "                            boxes=boxes,\n",
    "                            masks=None,)\n",
    "\n",
    "                if phase == 'val':\n",
    "                    with torch.no_grad():\n",
    "                        low_res_masks, _ = model_.mask_decoder(\n",
    "                            image_embeddings=image_embedding,\n",
    "                            image_pe=prompt_embedding,\n",
    "                            sparse_prompt_embeddings=sparse_embeddings,\n",
    "                            dense_prompt_embeddings=dense_embeddings,\n",
    "                            multimask_output=False,\n",
    "                        )\n",
    "                else:\n",
    "                    low_res_masks, _ = model_.mask_decoder(\n",
    "                            image_embeddings=image_embedding,\n",
    "                            image_pe=prompt_embedding,\n",
    "                            sparse_prompt_embeddings=sparse_embeddings,\n",
    "                            dense_prompt_embeddings=dense_embeddings,\n",
    "                            multimask_output=False,\n",
    "                        )\n",
    "                    \n",
    "                low_res_masks=predictor.model.postprocess_masks(low_res_masks, predictor.input_size, predictor.original_size)\n",
    "                threshold_masks = torch.sigmoid(low_res_masks - model_.mask_threshold) \n",
    "                # threshold_masks = normalize(threshold(low_res_masks, 0.0, 0)).to(device)\n",
    "                sam_mask_pre = (threshold_masks > 0.5)*1.0\n",
    "                sam_mask.append(sam_mask_pre.squeeze(1))\n",
    "\n",
    "                print('pred', obj_results[0].boxes.cls.detach().cpu().numpy())\n",
    "                print('gt', gt_classes)\n",
    "                \n",
    "                print('gt masks', len(gt_masks))\n",
    "                print('pred masks', threshold_masks.shape)\n",
    "                \n",
    "                # reshape gt_masks to same shape as predicted masks\n",
    "                gt_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in gt_masks], dim=0).to(device)\n",
    "                yolo_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in yolo_masks], dim=0).to(device)\n",
    "                segm_loss_yolo, preds, gts, gt_classes_match, pred_classes_match = loss.segm_loss_match_hungarian(threshold_masks, threshold_masks, gt_masks_tensor, \n",
    "                                                                                  obj_results[0].boxes.cls.detach().cpu().numpy(), gt_classes)\n",
    "                segm_loss_sam, _, _2, _3, _  = loss.segm_loss_match_hungarian(yolo_masks_tensor, yolo_masks_tensor, gt_masks_tensor, \n",
    "                                                                                  obj_results[0].boxes.cls.detach().cpu().numpy(), gt_classes)\n",
    "                all_preds.append(preds)\n",
    "                all_gts.append(gts)\n",
    "                all_gt_cls.append(gt_classes_match)\n",
    "                all_pred_cls.append(pred_classes_match)\n",
    "                \n",
    "                batch_losses_sam.append(segm_loss_sam)\n",
    "                batch_losses_yolo.append(segm_loss_yolo)\n",
    "                del sparse_embeddings, dense_embeddings, low_res_masks, gt_masks, \n",
    "                del yolo_masks_tensor, segm_loss_sam, segm_loss_yolo\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # if phase == 'val':\n",
    "                #     fig, axes = plt.subplots(1, 4, figsize=(18, 6)) \n",
    "                    \n",
    "                #     # Plot 1: GT Masks\n",
    "                #     axes[0].imshow(image)\n",
    "                #     axes[0].set_title('GT Masks')\n",
    "                #     show_masks(gt_masks_tensor.squeeze(1).squeeze(1).detach().cpu().numpy(), axes[0], random_color=True)\n",
    "                    \n",
    "                #     # Plot 2: YOLO Masks\n",
    "                #     axes[1].imshow(image)\n",
    "                #     axes[1].set_title('YOLOv8n predicted Masks')\n",
    "                #     show_masks(yolo_masks, axes[1], random_color=True)\n",
    "                    \n",
    "                #     # Plot 3: Bounding Boxes\n",
    "                #     image1 = cv2.resize(image, (1024, 1024))\n",
    "                #     for bbox in boxes:\n",
    "                #         x1, y1, x2, y2 = bbox.detach().cpu().numpy()\n",
    "                #         x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                #         cv2.rectangle(image1, (x1, y1), (x2, y2), (0, 255, 0), 2) \n",
    "                #     image1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "                #     axes[2].imshow(image1_rgb)\n",
    "                #     axes[2].set_title('YOLOv8n predicted Bboxes')\n",
    "                    \n",
    "                #     # Plot 4: SAM Masks\n",
    "                #     sam_masks_numpy = sam_mask[0].detach().cpu().numpy()\n",
    "                #     axes[3].imshow(image)\n",
    "                #     show_masks(sam_masks_numpy, axes[3], random_color=True)\n",
    "                #     axes[3].set_title('MobileSAM predicted masks')\n",
    "                #     plt.tight_layout() \n",
    "                #     # plt.savefig(f'./plots/combined_plots.png')\n",
    "                #     plt.show()\n",
    "\n",
    "        mean_loss_sam = torch.mean(torch.stack(batch_losses_sam)) * 1e5\n",
    "        mean_loss_yolo = torch.mean(torch.stack(batch_losses_yolo))\n",
    "        epoch_sam_loss.append(mean_loss_sam.item()/1e5)\n",
    "        epoch_yolo_loss.append(mean_loss_yolo.item())\n",
    "\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            mean_loss_sam.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # print(f'Epoch {epoch}, {phase.capitalize()} Segmentation loss SAM: {np.mean(epoch_sam_loss)}. YOLO: {np.mean(epoch_yolo_loss)}')\n",
    "    return np.mean(epoch_sam_loss), np.mean(epoch_yolo_loss), model_, all_preds, all_gts, all_gt_cls, all_pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_valid_loss = float('inf')\n",
    "# num_epochs = 15\n",
    "# n_epochs_stop = 5 + num_epochs//10\n",
    "# epoch_sam_loss_train_list, epoch_sam_loss_val_list, epoch_yolo_loss_train_list, epoch_yolo_loss_val_list = [], [], [], []\n",
    "\n",
    "# model = mobile_sam_model\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_sam_loss_train, epoch_yolo_loss_train, model = run_epoch('train', train_image_files, train_dir, train_num_batches, model, optimizer, train_encoders=True)\n",
    "#     epoch_sam_loss_val, epoch_yolo_loss_val, model = run_epoch('val', valid_image_files, valid_dir, valid_num_batches, model)\n",
    "\n",
    "#     epoch_sam_loss_train_list.append(epoch_sam_loss_train)\n",
    "#     epoch_sam_loss_val_list.append(epoch_sam_loss_val)\n",
    "#     epoch_yolo_loss_train_list.append(epoch_yolo_loss_train)\n",
    "#     epoch_yolo_loss_val_list.append(epoch_yolo_loss_val)\n",
    "    \n",
    "#     if use_wandb:\n",
    "#         wandb.log({'epoch train SAM loss': epoch_sam_loss_train, 'epoch valid SAM loss': epoch_sam_loss_val})\n",
    "#         wandb.log({'epoch train YOLO loss': epoch_yolo_loss_train, 'epoch valid YOLO loss': epoch_yolo_loss_val})\n",
    "\n",
    "#     if epoch_sam_loss_val < best_valid_loss:\n",
    "#         best_valid_loss = epoch_sam_loss_val\n",
    "#         best_model = model\n",
    "#         epochs_no_improve = 0\n",
    "#     else:\n",
    "#         epochs_no_improve += 1\n",
    "#         if epochs_no_improve == n_epochs_stop:\n",
    "#             print(\"Early stopping initiated.\")\n",
    "#             early_stop = True\n",
    "#             break\n",
    "\n",
    "# torch.save(best_model.state_dict(), f'yolo_sam_final.pth')\n",
    "# if use_wandb:\n",
    "#     run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(range(len(epoch_sam_loss_train_list))), epoch_sam_loss_train_list)\n",
    "# plt.plot(list(range(len(epoch_sam_loss_val_list))), epoch_sam_loss_val_list)\n",
    "# # plt.plot(list(range(len(epoch_yolo_loss_train_list))), epoch_yolo_loss_train_list)\n",
    "# # plt.plot(list(range(len(epoch_yolo_loss_val_list))), epoch_yolo_loss_val_list)\n",
    "\n",
    "# plt.title('Mean epoch loss \\n YOLO-SAM')\n",
    "# plt.xlabel('Epoch Number')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.savefig('loss_yolo_SAM.png')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Progress:   0%|          | 0/1 [00:00<?, ?it/s]                                                                                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "pred [2.]\n",
      "gt [2]\n",
      "gt masks 1\n",
      "pred masks torch.Size([1, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 0\n",
      "pred_idx 0 gt_idx 0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "pred [0. 2. 2. 2. 1.]\n",
      "gt [0, 2, 1, 2]\n",
      "gt masks 4\n",
      "pred masks torch.Size([5, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 0\n",
      "pred_idx 1 gt_idx 3\n",
      "pred_idx 2 gt_idx 1\n",
      "pred_idx 4 gt_idx 2\n",
      "pred_idx 0 gt_idx 0\n",
      "pred_idx 1 gt_idx 3\n",
      "pred_idx 2 gt_idx 1\n",
      "pred_idx 4 gt_idx 2\n",
      "1\n",
      "1\n",
      "pred [0. 2.]\n",
      "gt [0]\n",
      "gt masks 1\n",
      "pred masks torch.Size([2, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 0\n",
      "pred_idx 0 gt_idx 0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "pred [2. 1. 0. 2.]\n",
      "gt [1, 0, 2]\n",
      "gt masks 3\n",
      "pred masks torch.Size([4, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 2\n",
      "pred_idx 1 gt_idx 0\n",
      "pred_idx 2 gt_idx 1\n",
      "pred_idx 0 gt_idx 2\n",
      "pred_idx 1 gt_idx 0\n",
      "pred_idx 2 gt_idx 1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "pred [1. 0.]\n",
      "gt [0, 1]\n",
      "gt masks 2\n",
      "pred masks torch.Size([2, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 1\n",
      "pred_idx 1 gt_idx 0\n",
      "pred_idx 0 gt_idx 1\n",
      "pred_idx 1 gt_idx 0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "pred [2. 2. 0. 1.]\n",
      "gt [2, 1, 2, 0, 2, 2, 2, 2]\n",
      "gt masks 8\n",
      "pred masks torch.Size([4, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 0\n",
      "pred_idx 1 gt_idx 2\n",
      "pred_idx 2 gt_idx 3\n",
      "pred_idx 3 gt_idx 7\n",
      "pred_idx 0 gt_idx 0\n",
      "pred_idx 1 gt_idx 2\n",
      "pred_idx 2 gt_idx 3\n",
      "pred_idx 3 gt_idx 7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "pred [2. 2. 0. 1. 1. 2. 2. 2.]\n",
      "gt [2, 2, 0, 1, 1, 2, 2, 2]\n",
      "gt masks 8\n",
      "pred masks torch.Size([8, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 1\n",
      "pred_idx 1 gt_idx 6\n",
      "pred_idx 2 gt_idx 2\n",
      "pred_idx 3 gt_idx 3\n",
      "pred_idx 4 gt_idx 4\n",
      "pred_idx 5 gt_idx 5\n",
      "pred_idx 6 gt_idx 7\n",
      "pred_idx 7 gt_idx 0\n",
      "pred_idx 0 gt_idx 1\n",
      "pred_idx 1 gt_idx 6\n",
      "pred_idx 2 gt_idx 2\n",
      "pred_idx 3 gt_idx 3\n",
      "pred_idx 4 gt_idx 4\n",
      "pred_idx 5 gt_idx 5\n",
      "pred_idx 6 gt_idx 7\n",
      "pred_idx 7 gt_idx 0\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "pred [2. 0. 2. 1. 1.]\n",
      "gt [1, 2, 2]\n",
      "gt masks 3\n",
      "pred masks torch.Size([5, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 1\n",
      "pred_idx 2 gt_idx 2\n",
      "pred_idx 3 gt_idx 0\n",
      "pred_idx 0 gt_idx 1\n",
      "pred_idx 2 gt_idx 2\n",
      "pred_idx 3 gt_idx 0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "pred [0.]\n",
      "gt [0]\n",
      "gt masks 1\n",
      "pred masks torch.Size([1, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 0\n",
      "pred_idx 0 gt_idx 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.75s/it]                                                                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "pred [0.]\n",
      "gt [1, 0, 1]\n",
      "gt masks 3\n",
      "pred masks torch.Size([1, 1, 512, 512])\n",
      "pred_idx 0 gt_idx 1\n",
      "pred_idx 0 gt_idx 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_sam_loss_val, epoch_yolo_loss_val, model, preds, gts, gt_classes, pred_classes = run_epoch('val', valid_image_files, valid_dir, 1, mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds), len(gts), len(gt_classes), len(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[4][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.0],\n",
       " [0.0, 2.0, 2.0, 1.0],\n",
       " [0.0],\n",
       " [2.0, 1.0, 0.0],\n",
       " [1.0, 0.0],\n",
       " [2.0, 2.0, 0.0, 1.0],\n",
       " [2.0, 2.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0],\n",
       " [2.0, 2.0, 1.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [2.0, 2.0],\n",
       " [2.0, 1.0],\n",
       " [0.0, 2.0, 2.0, 2.0],\n",
       " [2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [0.0, 2.0],\n",
       " [2.0],\n",
       " [2.0, 1.0, 2.0],\n",
       " [1.0, 2.0],\n",
       " [0.0, 2.0, 1.0, 1.0],\n",
       " [2.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [0.0, 1.0, 1.0],\n",
       " [2.0, 2.0],\n",
       " [1.0, 1.0, 2.0, 1.0, 1.0],\n",
       " [0.0, 2.0],\n",
       " [0.0, 1.0],\n",
       " [0.0],\n",
       " [2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0],\n",
       " [2.0],\n",
       " [1.0, 0.0, 1.0, 2.0],\n",
       " [2.0, 1.0, 2.0],\n",
       " [2.0, 2.0, 1.0],\n",
       " [0.0, 1.0],\n",
       " [0.0, 1.0, 1.0],\n",
       " [0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0],\n",
       " [0.0, 2.0, 1.0],\n",
       " [0.0],\n",
       " [2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0],\n",
       " [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 2.0, 2.0, 1.0],\n",
       " [0.0],\n",
       " [2.0],\n",
       " [2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0],\n",
       " [0.0, 1.0, 1.0, 1.0, 2.0],\n",
       " [0.0],\n",
       " [2.0, 0.0],\n",
       " [2.0, 1.0, 0.0],\n",
       " [0.0],\n",
       " [0.0, 2.0, 1.0, 2.0, 2.0],\n",
       " [2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 1.0],\n",
       " [1.0],\n",
       " [0.0, 2.0],\n",
       " [2.0, 2.0],\n",
       " [0.0],\n",
       " [2.0, 2.0],\n",
       " [0.0, 2.0],\n",
       " [0.0, 1.0],\n",
       " [2.0, 2.0, 1.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [2.0, 0.0, 2.0, 1.0, 1.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [2.0, 1.0],\n",
       " [2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 2.0],\n",
       " [0.0],\n",
       " [2.0, 0.0, 2.0, 1.0],\n",
       " [1.0, 1.0, 1.0, 1.0],\n",
       " [1.0, 1.0, 2.0, 2.0],\n",
       " [2.0, 0.0, 2.0],\n",
       " [2.0, 2.0, 1.0, 0.0, 2.0],\n",
       " [2.0, 0.0, 1.0, 2.0],\n",
       " [1.0],\n",
       " [2.0, 0.0, 2.0, 2.0, 1.0],\n",
       " [2.0, 2.0, 1.0, 2.0, 2.0, 0.0],\n",
       " [2.0, 0.0],\n",
       " [2.0, 1.0],\n",
       " [1.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0],\n",
       " [2.0, 2.0, 2.0, 2.0, 2.0],\n",
       " [2.0, 0.0, 1.0],\n",
       " [0.0, 1.0, 2.0],\n",
       " [0.0],\n",
       " [2.0, 2.0],\n",
       " [1.0],\n",
       " [0.0, 2.0],\n",
       " [1.0, 1.0, 1.0, 0.0],\n",
       " [2.0, 0.0],\n",
       " [0.0, 1.0],\n",
       " [2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0],\n",
       " [2.0, 1.0, 2.0, 2.0, 2.0],\n",
       " [1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0],\n",
       " [0.0, 1.0],\n",
       " [2.0, 2.0],\n",
       " [2.0, 0.0],\n",
       " [2.0, 0.0, 2.0],\n",
       " [2.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [0.0, 1.0],\n",
       " [2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0],\n",
       " [2.0, 2.0, 2.0, 2.0],\n",
       " [0.0],\n",
       " [0.0, 2.0],\n",
       " [0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " [2.0, 2.0, 1.0, 2.0],\n",
       " [0.0],\n",
       " [2.0, 2.0, 2.0],\n",
       " [0.0],\n",
       " [2.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0],\n",
       " [2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0],\n",
       " [2.0, 2.0],\n",
       " [1.0],\n",
       " [2.0, 2.0, 1.0, 1.0, 1.0, 1.0],\n",
       " [2.0, 2.0, 2.0, 1.0, 1.0, 1.0],\n",
       " [2.0, 2.0, 2.0, 0.0],\n",
       " [0.0, 2.0],\n",
       " [2.0],\n",
       " [0.0, 1.0],\n",
       " [1.0],\n",
       " [2.0, 1.0, 1.0],\n",
       " [0.0],\n",
       " [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0],\n",
       " [2.0, 1.0],\n",
       " [2.0, 2.0],\n",
       " [2.0, 2.0, 2.0, 1.0, 2.0],\n",
       " [0.0],\n",
       " [0.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " [2.0, 0.0, 2.0, 2.0],\n",
       " [2.0, 1.0, 0.0, 1.0, 1.0],\n",
       " [0.0],\n",
       " [0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0],\n",
       " [0.0],\n",
       " [0.0, 1.0, 1.0],\n",
       " [0.0, 2.0, 1.0],\n",
       " [0.0, 2.0],\n",
       " [1.0, 2.0, 2.0, 2.0, 1.0],\n",
       " [2.0, 2.0, 2.0],\n",
       " [0.0, 2.0],\n",
       " [1.0],\n",
       " [0.0, 2.0, 2.0],\n",
       " [0.0, 1.0],\n",
       " [2.0, 2.0, 1.0, 1.0],\n",
       " [0.0, 1.0, 1.0, 2.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [0.0, 2.0],\n",
       " [0.0],\n",
       " [0.0, 1.0],\n",
       " [0.0, 2.0],\n",
       " [0.0],\n",
       " [1.0, 1.0],\n",
       " [2.0, 1.0, 2.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [2.0, 1.0, 2.0, 2.0],\n",
       " [1.0, 1.0, 2.0, 1.0, 1.0],\n",
       " [2.0, 2.0],\n",
       " [1.0],\n",
       " [2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0],\n",
       " [0.0, 2.0],\n",
       " [2.0, 2.0, 2.0, 0.0],\n",
       " [2.0],\n",
       " [0.0, 1.0, 1.0],\n",
       " [0.0],\n",
       " [1.0],\n",
       " [0.0, 2.0, 2.0, 1.0],\n",
       " [1.0, 1.0, 2.0, 2.0],\n",
       " [2.0],\n",
       " [2.0, 2.0],\n",
       " [0.0],\n",
       " [0.0, 1.0],\n",
       " [2.0],\n",
       " [1.0, 2.0, 1.0, 1.0, 1.0],\n",
       " [0.0, 1.0, 1.0],\n",
       " [0.0, 2.0],\n",
       " [1.0],\n",
       " [0.0],\n",
       " [2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0],\n",
       " [0.0, 1.0, 1.0],\n",
       " [0.0],\n",
       " [1.0],\n",
       " [0.0, 1.0],\n",
       " [2.0, 2.0, 2.0, 2.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [2.0, 1.0, 1.0],\n",
       " [2.0],\n",
       " [2.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0],\n",
       " [0.0],\n",
       " [2.0, 2.0],\n",
       " [2.0, 1.0, 2.0, 2.0, 1.0],\n",
       " [2.0, 1.0],\n",
       " [0.0, 1.0, 2.0],\n",
       " [2.0, 0.0],\n",
       " [0.0, 2.0],\n",
       " [2.0, 2.0, 2.0],\n",
       " [0.0],\n",
       " [2.0, 2.0, 2.0, 1.0],\n",
       " [2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0],\n",
       " [0.0, 1.0, 2.0, 1.0],\n",
       " [2.0, 0.0, 1.0],\n",
       " [1.0],\n",
       " [0.0, 1.0, 2.0, 1.0, 0.0, 1.0],\n",
       " [2.0, 2.0],\n",
       " [1.0],\n",
       " [0.0, 2.0],\n",
       " [2.0, 2.0, 0.0, 1.0],\n",
       " [2.0],\n",
       " [1.0, 2.0, 1.0],\n",
       " [0.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0],\n",
       " [0.0, 2.0],\n",
       " [0.0],\n",
       " [0.0, 1.0],\n",
       " [2.0, 1.0, 1.0, 1.0],\n",
       " [0.0],\n",
       " [2.0, 2.0, 1.0, 2.0, 1.0, 2.0],\n",
       " [2.0, 2.0, 1.0, 0.0, 1.0],\n",
       " [0.0, 1.0],\n",
       " [2.0, 2.0, 2.0, 0.0, 1.0],\n",
       " [1.0],\n",
       " [2.0, 1.0, 1.0, 1.0, 0.0],\n",
       " [2.0, 1.0, 1.0, 1.0, 2.0],\n",
       " [2.0, 0.0, 2.0],\n",
       " [2.0, 1.0],\n",
       " [2.0],\n",
       " [0.0, 2.0, 1.0, 2.0, 2.0, 2.0],\n",
       " [1.0],\n",
       " [2.0, 2.0, 0.0],\n",
       " [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0],\n",
       " [0.0],\n",
       " [2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " [1.0],\n",
       " [0.0],\n",
       " [2.0, 0.0, 2.0],\n",
       " [2.0],\n",
       " [0.0],\n",
       " [2.0],\n",
       " [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0],\n",
       " [2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0],\n",
       " [2.0, 2.0],\n",
       " [1.0],\n",
       " [0.0],\n",
       " [0.0],\n",
       " [0.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0],\n",
       " [2.0, 0.0, 1.0, 2.0, 1.0, 1.0],\n",
       " [2.0],\n",
       " [0.0, 1.0],\n",
       " [0.0, 2.0],\n",
       " [2.0],\n",
       " [0.0, 2.0],\n",
       " [0.0],\n",
       " [0.0, 2.0],\n",
       " [0.0],\n",
       " [1.0],\n",
       " [2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " [0.0, 1.0, 2.0, 2.0],\n",
       " [0.0, 1.0, 1.0],\n",
       " [2.0, 0.0],\n",
       " [0.0],\n",
       " [2.0, 2.0, 2.0, 1.0, 1.0, 2.0],\n",
       " [2.0, 0.0],\n",
       " [2.0, 2.0, 2.0, 2.0, 2.0, 2.0],\n",
       " [0.0],\n",
       " [0.0, 2.0, 1.0, 1.0, 1.0, 2.0],\n",
       " [2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0],\n",
       " [1.0, 1.0, 0.0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(gts)):\n",
    "    plt.imshow(gts[i][0][0].detach().cpu().numpy())\n",
    "    plt.title(gt_classes[i][0].detach().cpu().numpy()))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.imshow(preds[i][0][0].detach().cpu().numpy())\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([preds[i][0][0].detach().cpu().numpy() for i in range(len(preds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = np.array([gts[i][0][0].detach().cpu().numpy() for i in range(len(gts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((274, 512, 512), (274, 512, 512))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, gts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def compute_iou(pred_mask, gt_mask):\n",
    "    intersection = np.logical_and(pred_mask, gt_mask)\n",
    "    union = np.logical_or(pred_mask, gt_mask)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return iou_score\n",
    "\n",
    "def compute_precision_recall(pred_masks, gt_masks, iou_thresholds):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    # Iterate over a range of IoU thresholds\n",
    "    for iou_threshold in iou_thresholds:\n",
    "        tp = fp = fn = 0\n",
    "        for pred_mask, gt_mask in zip(pred_masks, gt_masks):\n",
    "            iou_score = compute_iou(pred_mask, gt_mask)\n",
    "            if iou_score > iou_threshold:\n",
    "                tp += 1  # true positive\n",
    "            else:\n",
    "                fp += 1  # false positive\n",
    "            # Assume only one gt_mask per image\n",
    "        fn = len(gt_masks) - tp  # false negatives\n",
    "        precision.append(tp / (tp + fp))\n",
    "        recall.append(tp / (tp + fn))\n",
    "    return precision, recall\n",
    "\n",
    "def compute_map(precision, recall):\n",
    "    # Sort recall values and corresponding precision\n",
    "    precision, recall = zip(*sorted(zip(precision, recall)))\n",
    "    # Compute AP as the area under curve\n",
    "    ap = auc(recall, precision)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Example data initialization\n",
    "pred_masks_all = preds  # List of all predicted masks\n",
    "gt_masks_all = gts    # List of all ground truth masks\n",
    "# class_ids_all = [...]   # Corresponding class IDs for each mask\n",
    "\n",
    "# Organize masks by class ID into dictionaries\n",
    "pred_masks_by_class = defaultdict(list)\n",
    "gt_masks_by_class = defaultdict(list)\n",
    "\n",
    "for pred_mask, gt_mask, class_id in zip(pred_masks_all, gt_masks_all):\n",
    "    pred_masks_by_class[class_id].append(pred_mask)\n",
    "    gt_masks_by_class[class_id].append(gt_mask)\n",
    "    \n",
    "\n",
    "# IoU thresholds is a list, e.g., [0.5, 0.55, ..., 0.95]\n",
    "iou_thresholds = np.linspace(0.5, 0.95, 10)\n",
    "aps = []\n",
    "for class_id in class_ids:  # Iterate over all class ids\n",
    "    pred_masks = get_pred_masks_for_class(class_id)  # Implement this\n",
    "    gt_masks = get_gt_masks_for_class(class_id)  # Implement this\n",
    "    precision, recall = compute_precision_recall(pred_masks, gt_masks, iou_thresholds)\n",
    "    ap = compute_map(precision, recall)\n",
    "    aps.append(ap)\n",
    "\n",
    "# Compute the mean of the Average Precision values for all classes\n",
    "mean_ap = np.mean(aps)\n",
    "print(f\"mAP: {mean_ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "def export_image_det_to_Roboflow(input_dir, filename, masks, obj_results):\n",
    "    class_names = obj_results[0].names\n",
    "    class_labels = obj_results[0].boxes.data[:, -1].int().tolist()\n",
    "    \n",
    "    objects = []\n",
    "    for i in range(len(masks)):\n",
    "        # masks[i]: [ 1, H, W]\n",
    "        mask_np = masks[i].detach().cpu().numpy()\n",
    "        polygon = binary_image_to_polygon(mask_np[0])\n",
    "        bbox = mask_to_bbox(mask_np)\n",
    "        if class_names[class_labels[i]] != 'star' and class_names[class_labels[i]] != 'other': # ignore stars and 'other' label\n",
    "            objects.append({\n",
    "                'name': class_names[class_labels[i]],\n",
    "                'bbox': bbox,\n",
    "                'segmentations': polygon[0]\n",
    "            })\n",
    "    if len(objects)>0:\n",
    "        create_annotation_SAM(filename=filename, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "        image_path = input_dir+filename\n",
    "        annotation_filename = filename.replace(\".png\", \".xml\")\n",
    "        upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "        os.remove(annotation_filename)\n",
    "    else:\n",
    "        print(\"No objects after label filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional Roboflow export in VOC format given filenames\n",
    "export_to_Roboflow = False\n",
    "import time\n",
    "\n",
    "# best_model = mobile_sam_model.cpu()\n",
    "if export_to_Roboflow:\n",
    "    # Initialize Roboflow client\n",
    "    rf = Roboflow(api_key=\"EBeK30tpU3HW2VGGl0xa\")\n",
    "    upload_project = rf.workspace(\"orij\").project(\"xmm_om_images_512_sg_sr_cr_only\") # error if the project doesn't exist\n",
    "\n",
    "new_images_dir = '../XMM_OM_dataset/zscaled_512_stretched/'\n",
    "new_image_files =  os.listdir(new_images_dir)\n",
    "best_model.eval()\n",
    "    \n",
    "with torch.no_grad(): \n",
    "    # eg_img = 'S0018141301_M.png'\n",
    "    for eg_img in new_image_files[701:711]:\n",
    "    # if True:\n",
    "            print('Image', new_images_dir+eg_img)\n",
    "            image = cv2.imread(new_images_dir + eg_img)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            start_time = time.time()\n",
    "            obj_results = yolov8_pretrained_model.predict(new_images_dir + eg_img, conf=0.2)  \n",
    "            predictor.set_image(image)\n",
    "        \n",
    "            if len(obj_results[0]) == 0:\n",
    "                print(f\"No masks for {eg_img}.\")\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                continue\n",
    "    \n",
    "            input_boxes1 = obj_results[0].boxes.xyxy\n",
    "            expand_by = 2.5\n",
    "            enlarged_bbox = input_boxes1.clone() \n",
    "            enlarged_bbox[:, :2] -= expand_by  \n",
    "            enlarged_bbox[:, 2:] += expand_by  \n",
    "            input_boxes1 = enlarged_bbox\n",
    "    \n",
    "            input_boxes = input_boxes1.cpu().numpy()\n",
    "            input_boxes = predictor.transform.apply_boxes(input_boxes, predictor.original_size)\n",
    "            input_boxes = torch.from_numpy(input_boxes).to(device)\n",
    "            sam_mask, yolo_masks = [], []\n",
    "            image_embedding=predictor.features\n",
    "            prompt_embedding=best_model.prompt_encoder.get_dense_pe()\n",
    "            non_resized_masks = obj_results[0].masks.data.cpu().numpy()\n",
    "            for i in range(len(non_resized_masks)):\n",
    "                    yolo_masks.append(cv2.resize(non_resized_masks[i], image.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)) \n",
    "        \n",
    "            for (boxes,) in batch_iterator(320, input_boxes): \n",
    "                with torch.no_grad():\n",
    "                    image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\n",
    "                    prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\n",
    "                    sparse_embeddings, dense_embeddings = best_model.prompt_encoder(\n",
    "                        points=None,\n",
    "                        boxes=boxes.cpu(),\n",
    "                        masks=None,)\n",
    "                    low_res_masks, _ = best_model.mask_decoder(\n",
    "                        image_embeddings=image_embedding,\n",
    "                        image_pe=prompt_embedding,\n",
    "                        sparse_prompt_embeddings=sparse_embeddings,\n",
    "                        dense_prompt_embeddings=dense_embeddings,\n",
    "                        multimask_output=False,\n",
    "                    )\n",
    "                    low_res_masks=predictor.model.postprocess_masks(low_res_masks, predictor.input_size, predictor.original_size)\n",
    "                    threshold_masks = torch.sigmoid(low_res_masks - best_model.mask_threshold) \n",
    "                    sam_mask_pre = (threshold_masks > 0.5)*1.0\n",
    "                    sam_mask.append(sam_mask_pre.squeeze(1))\n",
    "    \n",
    "                    yolo_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in yolo_masks], dim=0)\n",
    "                    if export_to_Roboflow:\n",
    "                        export_image_det_to_Roboflow(new_images_dir, image_name, sam_mask_pre, obj_results)\n",
    "                    print(\"Inference time per image:\", time.time()-start_time)\n",
    "                    fig, axes = plt.subplots(1, 3, figsize=(18, 6)) \n",
    "                    \n",
    "                    # Plot 1: YOLO Masks\n",
    "                    axes[0].imshow(image)\n",
    "                    axes[0].set_title('YOLOv8n predicted Masks')\n",
    "                    show_masks(yolo_masks, axes[0]) #, random_color=True)\n",
    "                    \n",
    "                    # Plot 2: Bounding Boxes\n",
    "                    image1 = cv2.resize(image, (1024, 1024))\n",
    "                    for bbox in boxes:\n",
    "                        x1, y1, x2, y2 = bbox.detach().cpu().numpy()\n",
    "                        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                        cv2.rectangle(image1, (x1, y1), (x2, y2), (0, 255, 0), 2) \n",
    "                    image1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "                    axes[1].imshow(image1_rgb)\n",
    "                    axes[1].set_title('YOLOv8n predicted Bboxes')\n",
    "                    \n",
    "                    # Plot 3: SAM Masks\n",
    "                    sam_masks_numpy = sam_mask[0].detach().cpu().numpy()\n",
    "                    axes[2].imshow(image)\n",
    "                    show_masks(sam_masks_numpy, axes[2]) #, random_color=True)\n",
    "                    axes[2].set_title('MobileSAM predicted masks')\n",
    "                    plt.tight_layout() \n",
    "                    # plt.savefig(f'./plots/combined_plots.png')\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mobile_sam_state_dict = best_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mobile_sam_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(yolov8_pretrained_model.state_dict(), 'yolo_model.bin')\n",
    "# torch.save(best_model.state_dict(), 'pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config_dict = {\n",
    "    \"hidden_size\": model.hidden_size,\n",
    "    \"num_attention_heads\": model.num_attention_heads,\n",
    "    \"num_hidden_layers\": model.num_hidden_layers,\n",
    "}\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolov8_pretrained_model.export(format='onnx', imgsz=[512,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/')\n",
    "\n",
    "# from importlib import reload\n",
    "# from ft_mobile_sam import sam_model_registry, SamPredictor\n",
    "# from ft_mobile_sam.utils.onnx import SamOnnxModel\n",
    "\n",
    "# import onnxruntime\n",
    "# from onnxruntime.quantization import QuantType\n",
    "# from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "\n",
    "# import warnings\n",
    "# onnx_model_path = None  # Set to use an already exported model, then skip to the next section.\n",
    "\n",
    "# onnx_model_path = \"sam_onnx_example.onnx\"\n",
    "\n",
    "# sam = mobile_sam_model.to('cpu') # the model must be set on CP\n",
    "# onnx_model = SamOnnxModel(sam, return_single_mask=True)\n",
    "\n",
    "# dynamic_axes = {\n",
    "#     \"point_coords\": {1: \"num_points\"},\n",
    "#     \"point_labels\": {1: \"num_points\"},\n",
    "# }\n",
    "\n",
    "# embed_dim = sam.prompt_encoder.embed_dim\n",
    "# embed_size = sam.prompt_encoder.image_embedding_size\n",
    "# mask_input_size = [4 * x for x in embed_size]\n",
    "# dummy_inputs = {\n",
    "#     \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "#     \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
    "#     \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
    "#     \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "#     \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "#     # \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
    "# }\n",
    "# output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "#     warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "#     with open(onnx_model_path, \"wb\") as f:\n",
    "#         torch.onnx.export(\n",
    "#             onnx_model,\n",
    "#             tuple(dummy_inputs.values()),\n",
    "#             f,\n",
    "#             export_params=True,\n",
    "#             verbose=False,\n",
    "#             opset_version=16,\n",
    "#             do_constant_folding=True,\n",
    "#             input_names=list(dummy_inputs.keys()),\n",
    "#             output_names=output_names,\n",
    "#             dynamic_axes=dynamic_axes,\n",
    "#         )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: export to ONNX\n",
    "\n",
    "# !python scripts/export_onnx_model.py --checkpoint ./weights/mobile_sam.pt --model-type vit_t --output ./mobile_sam.onnx"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "env_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
