{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb  9 10:28:59 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    60W / 400W |  39510MiB / 40960MiB |      4%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    81W / 400W |   8060MiB / 40960MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    75W / 400W |   8060MiB / 40960MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    78W / 400W |  29828MiB / 40960MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM...  On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    70W / 400W |  23946MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM...  On   | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    64W / 400W |  23864MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    80W / 400W |  23620MiB / 40960MiB |     50%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    62W / 400W |  20790MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from pycocotools import mask as maskUtils\n",
    "import cv2\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "import os\n",
    "# !pip install astropy\n",
    "import astropy\n",
    "from astropy.io import fits\n",
    "from scipy.interpolate import interp1d\n",
    "from astropy.visualization import ZScaleInterval, ImageNormalize\n",
    "import torch\n",
    "import random\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import my files\n",
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OM_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'\n",
    "OM_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Roboflow annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0==1:\n",
    "\n",
    "    def display_masks(image_path, masks):\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "    \n",
    "        for mask in masks:\n",
    "            if isinstance(mask[0], list):  # If the mask is a polygon\n",
    "                polygon_points = np.array(mask[0]).reshape(-1, 2)\n",
    "                polygon = Polygon(polygon_points, edgecolor='g', facecolor='none')\n",
    "                ax.add_patch(polygon)\n",
    "            else:  # If the mask is RLE\n",
    "                binary_mask = maskUtils.decode(mask)\n",
    "                ax.imshow(binary_mask, alpha=0.5, cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "    # Load the JSON file\n",
    "    dir_train_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code/OM_sky_images-6/train/'\n",
    "    json_file_path = dir_train_path+'_annotations.coco.json'\n",
    "    \n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f) # dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n",
    "    \n",
    "    # Iterate through each image and its annotations\n",
    "    for image_info in data['images']:\n",
    "        image_id = image_info['id']\n",
    "        image_path = dir_train_path + image_info['file_name']\n",
    "        \n",
    "        # Find annotations for the current image\n",
    "        annotations = [anno for anno in data['annotations'] if anno['image_id'] == image_id]\n",
    "        # annotations: dict_keys(['id', 'image_id', 'category_id', 'bbox', 'area', 'segmentation', 'iscrowd'])\n",
    "        \n",
    "        # Extract and display masks for the image\n",
    "        masks = [anno['segmentation'] for anno in annotations]\n",
    "        display_masks(image_path, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mobile SAM *\n",
    ">\n",
    "> MobileSAM is approximately 5 times smaller and 7 times faster than the current FastSAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda:7'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/')\n",
    "\n",
    "import cv2\n",
    "import mobile_sam\n",
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-master/weights/mobile_sam.pt\"\n",
    "\n",
    "device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device=device)\n",
    "mobile_sam_model.eval();\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import math\n",
    "\n",
    "def MobileSAM_predict(image_path, show_annots = True, mask_on_negative = None):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # using the pixel mean and std specific to each image instead of the standard one.\n",
    "        image_T = np.transpose(image, (2, 1, 0))\n",
    "        pixel_mean = torch.as_tensor([np.mean(image_T[0]), np.mean(image_T[1]),np.mean(image_T[2])], dtype=torch.float, device=device)\n",
    "        pixel_std = torch.as_tensor([np.std(image_T[0]), np.std(image_T[1]),np.std(image_T[2])], dtype=torch.float, device=device)\n",
    "        \n",
    "        mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
    "                \n",
    "        predictor = SamPredictor(mobile_sam_model)\n",
    "        predictor.set_image(image)\n",
    "        \n",
    "        masks_np, iou_predictions_np, low_res_masks_np = predictor.predict()\n",
    "        if show_annots:\n",
    "            image_bgr = cv2.imread(image_path)\n",
    "            \n",
    "            mask_generator = SamAutomaticMaskGenerator(mobile_sam_model, points_per_side=None, point_grids=img_points)\n",
    "            mobile_sam_result = mask_generator.generate(image)\n",
    "            mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "            \n",
    "            if mask_on_negative is not None:\n",
    "                mobile_sam_result = remove_masks(sam_result=mobile_sam_result, mask_on_negative=mask_on_negative, threshold=50, remove_big_masks=True, img_shape = image.shape)\n",
    "    \n",
    "            detections = sv.Detections.from_sam(mobile_sam_result)\n",
    "        \n",
    "            annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "            image = Image.fromarray(annotated_image)\n",
    "            output_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/mobile_sam/\"+image_path.split('/')[-1].replace(\".png\", \"_mobile_sam_nonnegative.png\")\n",
    "            image.save(output_path)\n",
    "            \n",
    "        '''\n",
    "        predictor.predict() returns:\n",
    "              (np.ndarray): The output masks in CxHxW format, where C is the\n",
    "                number of masks, and (H, W) is the original image size.\n",
    "              (np.ndarray): An array of length C containing the model's\n",
    "                predictions for the quality of each mask.\n",
    "              (np.ndarray): An array of shape CxHxW, where C is the number\n",
    "                of masks and H=W=256. These low resolution logits can be passed to\n",
    "                a subsequent iteration as mask input.\n",
    "        '''\n",
    "        \n",
    "        return masks_np, iou_predictions_np, low_res_masks_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "    extracted_bboxes_points = json.load(f)\n",
    "extracted_bboxes_points['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "image_path = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/S0720251301_L.png'\n",
    "\n",
    "with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "    extracted_bboxes_points = json.load(f)\n",
    "\n",
    "img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                       if filename in image_path.split(\"/\")[-1]])\n",
    "img_points = img_points/255.0\n",
    "\n",
    "mask_on_negative = mask_and_plot_image(image_path.replace(\".png\", \".fits\").replace('clahe_', ''))\n",
    "\n",
    "start_time = time.time()\n",
    "output_mobile_sam = MobileSAM_predict(image_path, mask_on_negative=mask_on_negative)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Mobile SAM predict time/img: {end_time-start_time} s\")\n",
    "image = cv2.imread('/workspace/raid/OM_DeepLearning/XMM_OM_dataset/mobile_sam/'+image_path.split('/')[-1].replace(\".png\", \"_mobile_sam_nonnegative.png\"))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.title('Mobile SAM pred. on image input')\n",
    "plt.imsave('mobile_sam_entire_image.png', image)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(image), np.std(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(img_points[0][:, 0]*255, img_points[0][:, 1]*255, s=10, c='red')\n",
    "# plt.gca().invert_yaxis() \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stophere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN3DPGZSn57p"
   },
   "source": [
    "# Install Segment Anything Model (SAM) and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1H9YruJen0Q8",
    "outputId": "9098dc8e-3476-4166-d48e-d88e3ba76267"
   },
   "outputs": [],
   "source": [
    "# %cd {HOME}\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3CtzYroC2Lb",
    "outputId": "9e3c46a8-69d0-42cc-dbfe-01a435fa4daf"
   },
   "outputs": [],
   "source": [
    "# !pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VeYIWh1iDWW"
   },
   "source": [
    "### Download SAM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aszw1OxBwowI",
    "outputId": "64f3e115-2442-4181-9a7e-4eafdb4464f2"
   },
   "outputs": [],
   "source": [
    "# %cd {HOME}\n",
    "# !mkdir {HOME}/weights\n",
    "# %cd {HOME}/weights\n",
    "\n",
    "# !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ SAM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlhbd_f4xfiJ"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/raid/OM_DeepLearning/XMM_OM_code_git/weights/sam_vit_h_4b8939.pth ; exist: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "t6_9PSZupghA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())] # find all available GPUs in the cluster\n",
    "DEVICE = torch.device(f'cuda:7' if torch.cuda.is_available() else 'cpu') # take one available\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
    "sam.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ The predictor function (can remove masks on negative pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "    extracted_bboxes_points = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images-contrast-512-v5-3/train/S0202130101_L_png.rf.180755edd461c3df5224f9311c184ba6.jpg\"\n",
    "mask_on_negative = mask_and_plot_image(\"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/S0720251301_L.fits\")\n",
    "\n",
    "img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                       if filename in image_path.split(\"/\")[-1]])\n",
    "img_points = img_points/255\n",
    "\n",
    "start_time = time.time()\n",
    "_, _, annotated_image1 = SAM_predictor(SamAutomaticMaskGenerator, sam, image_path, mask_on_negative=None, img_grid_points=None)\n",
    "print(f\"original SAM predict time/img: {time.time()-start_time} s\")\n",
    "\n",
    "# start_time2 = time.time()\n",
    "# _, _, annotated_image2 = SAM_predictor(SamAutomaticMaskGenerator, sam, image_path, mask_on_negative=mask_on_negative, img_grid_points=img_points)\n",
    "# print(f\"original SAM predict time/img: {time.time()-start_time2} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs= plt.subplots(1, 3, figsize=(10, 10)) \n",
    "\n",
    "# axs[0].imshow(annotated_image1)\n",
    "# axs[0].set_title(f'Original SAM inference \\ntime: {round(end_time-start_time, 3)}s, {1024} grid points', \\\n",
    "#                  fontfamily='monospace', fontsize=10)\n",
    "\n",
    "# axs[1].imshow(annotated_image2)\n",
    "# axs[1].set_title(f'Original SAM inference\\ntime: {round(end_time2-start_time2, 3)}s, {img_points.shape[1]} grid points', \\\n",
    "#                 fontfamily='monospace', fontsize=10)\n",
    "\n",
    "# axs[2].imshow(cv2.cvtColor(cv2.imread(image_path2), cv2.COLOR_BGR2RGB))\n",
    "# axs[2].scatter(img_points[0][:, 0]*255, img_points[0][:, 1]*255, s=10, c='red')\n",
    "# axs[2].set_title(f'Extracted sources grid points', \\\n",
    "#                  fontfamily='monospace', fontsize=10)\n",
    "# axs[2].set_aspect('equal', 'box')  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig('plots/sam_grid_points_comparison.png', dpi=1000)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "\n",
    "generate_all_predictions = True\n",
    "\n",
    "inference_times = []\n",
    "if generate_all_predictions:\n",
    "    dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "    for file_ in dir_files:\n",
    "        try:\n",
    "            if '.png' in file_ and 'clahe' in file_:\n",
    "                \n",
    "                mask_on_negative = mask_and_plot_image(OM_dir+file_.replace('.png', '.fits').replace('clahe_', ''))\n",
    "                img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                                       if filename in file_.split(\"/\")[-1]])\n",
    "                img_points = img_points/255.0\n",
    "                \n",
    "                start_time = time.time()\n",
    "                _, _, annotated_image = SAM_predictor(SamAutomaticMaskGenerator, sam, OM_dir+file_, mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                print(f\"original SAM predict time/img: {end_time-start_time} s\")\n",
    "                inference_times.append(end_time-start_time)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    # with open('cell_SAM_predict_with_threshold.txt', 'w') as f:\n",
    "    #     f.write(str(cap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(inference_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "\n",
    "generate_all_predictions = True\n",
    "\n",
    "inference_times = []\n",
    "if generate_all_predictions:\n",
    "    dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "    for file_ in dir_files:\n",
    "        try:\n",
    "            if '.png' in file_ and 'clahe' in file_:\n",
    "                \n",
    "                mask_on_negative = mask_and_plot_image(OM_dir+file_.replace('.png', '.fits').replace('clahe_', ''))\n",
    "                img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                                       if filename in file_.split(\"/\")[-1]])\n",
    "                img_points = img_points/255.0\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "                if len(img_points)==0:\n",
    "                    img_points = None\n",
    "                    \n",
    "                _, _, annotated_image = SAM_predictor(SamAutomaticMaskGenerator, sam, OM_dir+file_, mask_on_negative=mask_on_negative, \n",
    "                                                           img_grid_points=img_points)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                print(f\"original SAM predict time/img: {end_time-start_time} s\")\n",
    "                inference_times.append(end_time-start_time)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(inference_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stophere2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture cap\n",
    "# # print(f\"Segmented images with threshold masks-on-negative.\")\n",
    "# generate_all_predictions = True\n",
    "\n",
    "# if generate_all_predictions:\n",
    "        \n",
    "#     dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "#     # print(dir_files)\n",
    "#     for file_ in dir_files:\n",
    "#         try:\n",
    "#             if '.png' in file_ and 'clahe' in file_:\n",
    "#                 mask_on_negative = mask_and_plot_image(OM_dir+file_.replace('.png', '.fits').replace('clahe_', ''))\n",
    "#                 SAM_predictor(OM_dir+file_, mask_on_negative)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             pass\n",
    "    \n",
    "#     # with open('cell_SAM_predict_with_threshold.txt', 'w') as f:\n",
    "#     #     f.write(str(cap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Generate annotation json file (COCO format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download Roboflow annotations, v5\n",
    "\n",
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "# project = rf.workspace(\"orij\").project(\"om_sky_images\")\n",
    "# dataset = project.version(5).download(\"coco-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    print(box[0], box[1], box[2],  box[3]) \n",
    "    w, h = box[2], box[3] \n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def numpy_to_list(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: numpy_to_list(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [numpy_to_list(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Initialize a dictionary to store all images and annotations\n",
    "categories = []\n",
    "\n",
    "# coco_style_annotations = {'categories': categories, 'images': [], 'annotations': []}\n",
    "coco_style_annotations = {'annotations': []}\n",
    "\n",
    "\n",
    "def get_SAM_annotations(IMAGE_PATH, mask_on_negative = None, output_mode=\"binary_mask\"):\n",
    "    \"\"\"\n",
    "    This function calls SAM (Segment Anything) and gets annotations for a given image.\n",
    "    Args:\n",
    "        IMAGE_PATH (str): The path to the image file.\n",
    "        remove_masks_on_negative (bool, optional): If True, masks on negative detections are removed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the SAM results, detections, and the annotated image.\n",
    "    \"\"\"\n",
    "    image_bgr = cv2.imread(IMAGE_PATH)\n",
    "    annotated_image, detections, sam_result = None, None, None\n",
    "\n",
    "    # try:\n",
    "    if True:\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        mask_generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode)\n",
    "        \n",
    "        sam_result = mask_generator.generate(image_rgb)\n",
    "        if mask_on_negative is not None:\n",
    "            sam_result = remove_masks(sam_result=sam_result,mask_on_negative=mask_on_negative, threshold=50)\n",
    "            \n",
    "        mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "        annotated_image = mask_annotator.annotate(scene=image_rgb.copy(), detections=detections)\n",
    "        # plt.imshow(annotated_image)\n",
    "        # plt.show()\n",
    "    # except Exception as e:\n",
    "    #     print(\"Exception:\\n\", IMAGE_PATH, e)\n",
    "    #     pass\n",
    "        \n",
    "    return sam_result, detections, annotated_image\n",
    "\n",
    "\n",
    "# Run SAM for .png files in the directory and create the annotation json file in ~COCO format\n",
    "\n",
    "def generate_json_file(input_dir, coco_style_annotations):\n",
    "    for file_ in os.listdir(input_dir):\n",
    "    # if True:\n",
    "        # file_ = 'S0720251301_L.png'\n",
    "        if \"png\" in file_:\n",
    "\n",
    "            print(file_)\n",
    "            mask_on_negative = mask_and_plot_image(input_dir+file_.replace('.png', '.fits'))\n",
    "            sam_result_i, detections_i, annotated_image_i = get_SAM_annotations(input_dir+file_, mask_on_negative.astype(int))\n",
    "            \n",
    "            sam_result_i = numpy_to_list(sam_result_i)\n",
    "            img = cv2.imread(os.path.join(input_dir, file_))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "            height, width, _ = img.shape\n",
    "            # print(k)\n",
    "            # plt.figure(figsize=(10,10))\n",
    "            # print(type(annotated_image_i))\n",
    "            # plt.imshow(annotated_image_i)\n",
    "            # # show_box(annotation['bbox'], plt.gca())\n",
    "            # # plt.axis('off')\n",
    "            # plt.show()\n",
    "            # coco_style_annotations['images'].append({\n",
    "            #             'id': file_.split('.')[0],\n",
    "            #             'license': 1,\n",
    "            #             'file_name': input_dir+file_,\n",
    "            #             'height':height, \n",
    "            #             'width': width,  \n",
    "            #             'date_captured': datetime.now().isoformat(), \n",
    "            #         })\n",
    "            k=0\n",
    "            for annotation in sam_result_i:\n",
    "                # xyhw = annotation['bbox']\n",
    "                # if (xyhw[2] >2 or xyhw[3] >2) and (xyhw[2]*1.0/height < 0.7 and xyhw[3]*1.0/width < 0.7):\n",
    "                if True:\n",
    "                    coco_style_annotations['annotations'].append({\n",
    "                            'id': f'{file_.split(\".\")[0]}_mask{k}',\n",
    "                            'image_id': file_.split('.')[0], \n",
    "                            'category_id': 0,  \n",
    "                            'segmentation': annotation['segmentation'],\n",
    "                            'area': annotation['area'],\n",
    "                            'bbox': annotation['bbox'],\n",
    "                            'iscrowd': 0,\n",
    "                        })\n",
    "                    k+=1\n",
    "    \n",
    "    with open('SAM_annotations_coco_style_v2.json', 'w') as f:\n",
    "    # with open('SAM_annotations_coco_style_img1.json', 'w') as f:\n",
    "        json.dump(coco_style_annotations, f)\n",
    "\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/'\n",
    "generate_json_file(input_dir, coco_style_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Generate annotation json file (VOC format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "upload_project = rf.workspace(\"orij\").project(\"xmm_om_images_v4-contrast-512-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and export SAM annotations in VOC format to Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n"
     ]
    }
   ],
   "source": [
    "#import my files\n",
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils, voc_annotate_and_Roboflow_export\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "reload(voc_annotate_and_Roboflow_export)\n",
    "\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "from voc_annotate_and_Roboflow_export import * # moved the files there\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/zscaled_512_stretched/'\n",
    "\n",
    "k=0\n",
    "if 1==1:\n",
    "    input_fits_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'\n",
    "    for file_ in os.listdir(input_dir):\n",
    "        # if k<=500:\n",
    "        #     k+=1\n",
    "        # elif k<=506:\n",
    "        #     k+=1\n",
    "            mask_on_negative = mask_and_plot_image(input_fits_dir+file_.replace('.png', '.fits'), plot_=False)\n",
    "    \n",
    "            sam_result_i, detections_i, annotated_image_i = SAM_predictor(SamAutomaticMaskGenerator, sam, input_dir+file_, \n",
    "                                                                          mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "            if sam_result_i is not None and detections_i is not None and annotated_image_i is not None:\n",
    "                objects = []\n",
    "                for annotation in sam_result_i: # a mask over an image is a binary array with shape (img_h, img_w)\n",
    "                    polygon = binary_image_to_polygon(annotation['segmentation'])\n",
    "                    # plot_polygon(polygon[0], annotated_image_i) # to see the masks polygons\n",
    "                    objects.append({\n",
    "                        'name': 'star',\n",
    "                        'bbox': annotation['bbox'],\n",
    "                        'segmentations': polygon[0]\n",
    "                    })\n",
    "                    \n",
    "                create_annotation_SAM(filename=file_, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "                image_path = input_dir+file_\n",
    "                annotation_filename = file_.replace(\".png\", \".xml\")\n",
    "                \n",
    "                upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "                os.remove(annotation_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert annotations from COCO format to VOC and mount to Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n",
      " Check the coorrdinates! \n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "import json \n",
    "\n",
    "import voc_annotate_and_Roboflow_export\n",
    "reload(voc_annotate_and_Roboflow_export)\n",
    "from voc_annotate_and_Roboflow_export import * # moved the files there\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images-contrast-512-v5-3/train/'\n",
    "json_file_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images-contrast-512-v5-3/train/_annotations.coco.json'\n",
    "\n",
    "output_d = \"/workspace/raid/OM_DeepLearning/VOC_xmm_om_images-contrast-512-v5-3/\"\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    data_in = json.load(f)\n",
    "\n",
    "k=0\n",
    "if 1==0:\n",
    "    for im in data_in['images']: \n",
    "        objects = []\n",
    "        file_ = im['file_name']\n",
    "        extension = \".\"+file_.split(\".\")[-1]\n",
    "        masks = [data_in['annotations'][a] for a in range(len(data_in['annotations'])) if data_in['annotations'][a]['image_id'] == im['id']]\n",
    "        classes = [data_in['annotations'][a]['category_id'] for a in range(len(data_in['annotations'])) if data_in['annotations'][a]['image_id'] == im['id']]\n",
    "        class_categories = {data_in['categories'][a]['id']:data_in['categories'][a]['name'] for a in range(len(data_in['categories']))}\n",
    "\n",
    "        temp_img = cv2.imread(input_dir+im[\"file_name\"])\n",
    "        temp_img = cv2.cvtColor(temp_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for i in range(len(masks)):\n",
    "            segmentation = masks[i]['segmentation']\n",
    "            if isinstance(segmentation, list):\n",
    "                if len(segmentation) > 0 and isinstance(segmentation[0], list):\n",
    "                    points = segmentation[0]\n",
    "                    h_img, w_img = temp_img.shape[:2]\n",
    "                        \n",
    "            binary_m = create_mask(points, (h_img, w_img)) # COCO segmentations are polygon points, and must be converted to masks\n",
    "                          \n",
    "            objects.append({\n",
    "                'name': class_categories[classes[i]],\n",
    "                'bbox': mask_to_bbox(binary_m),\n",
    "                'segmentations': segmentation[0]\n",
    "            })\n",
    "            \n",
    "        create_annotation(filename=file_, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "        image_path = input_dir+file_\n",
    "        annotation_filename = file_.replace(extension, \".xml\")\n",
    "\n",
    "        new_lines = ['<annotation>\\n','\t<folder></folder>\\n']\n",
    "        \n",
    "        with open(annotation_filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        del lines[:3]\n",
    "        \n",
    "        modified_lines = new_lines + lines\n",
    "        \n",
    "        with open(annotation_filename, 'w') as file:\n",
    "            file.writelines(modified_lines)\n",
    "\n",
    "        upload_project.upload(image_path, annotation_filename, overwrite=True)\n",
    "        del temp_img\n",
    "        os.remove(annotation_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export images and VOC annotations in VOC format to Roboflow (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "# project = rf.workspace(\"orij\").project(\"xmm_om_images_v4-contrast-512\")\n",
    "# dataset = project.version(1).download(\"coco-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "S0674810801_M_png\n",
      "S0048740101_U_png\n",
      "S0655571401_L_png\n",
      "S0893811101_V_png\n",
      "S0804790401_M_png\n",
      "S0044350101_L_png\n",
      "S0801800201_L_png\n",
      "S0872590101_M_png\n",
      "S0761100201_B_png\n",
      "S0822400101_M_png\n",
      "S0761100201_S_png\n",
      "S0411780301_U_png\n",
      "S0135520401_B_png\n",
      "S0025541201_U_png\n",
      "S0109890501_V_png\n",
      "S0109950101_B_png\n",
      "S0672401339_L_png\n",
      "S0823432701_S_png\n",
      "S0406450201_M_png\n",
      "S0405180501_L_png\n",
      "S0651330501_M_png\n",
      "S0300961001_B_png\n",
      "S0203840201_L_png\n",
      "S0303360901_V_png\n",
      "S0165971901_B_png\n",
      "S0150651201_U_png\n",
      "S0077340101_S_png\n",
      "S0655380501_M_png\n",
      "S0694510101_U_png\n",
      "S0882950101_S_png\n",
      "S0500670301_S_png\n",
      "S0404430301_L_png\n",
      "S0304201001_U_png\n",
      "S0760350401_M_png\n",
      "S0827202101_S_png\n",
      "S0862840401_M_png\n",
      "S0600110101_L_png\n",
      "S0150790101_V_png\n",
      "S0677680132_L_png\n",
      "S0106660701_L_png\n",
      "S0604310201_M_png\n",
      "S0161360501_L_png\n",
      "S0306050101_B_png\n",
      "S0605581201_S_png\n",
      "S0164570601_S_png\n",
      "S0744370301_L_png\n",
      "S0093031001_M_png\n",
      "S0830800301_U_png\n",
      "S0800971201_S_png\n",
      "S0675010501_B_png\n",
      "S0852980101_L_png\n",
      "S0151590201_L_png\n",
      "S0202730101_M_png\n",
      "S0800040101_U_png\n",
      "S0109130201_L_png\n",
      "S0782060401_M_png\n",
      "S0741032301_M_png\n",
      "S0503190301_V_png\n",
      "S0675010501_S_png\n",
      "S0600450401_M_png\n",
      "S0804300501_M_png\n",
      "S0200730301_L_png\n",
      "S0412600201_S_png\n",
      "S0603150401_U_png\n",
      "S0882950101_L_png\n",
      "S0205260201_L_png\n",
      "S0765020201_M_png\n",
      "S0840840201_U_png\n",
      "S0654480101_L_png\n",
      "S0804670601_V_png\n",
      "S0600450401_L_png\n",
      "S0690745001_L_png\n",
      "S0743120301_U_png\n",
      "S0148210401_B_png\n",
      "S0693490201_L_png\n",
      "S0785100201_M_png\n",
      "S0305600501_U_png\n",
      "S0721630101_M_png\n",
      "S0152860101_S_png\n",
      "S0652010201_U_png\n",
      "S0827051101_M_png\n",
      "S0674810901_M_png\n",
      "S0830551201_L_png\n",
      "S0200020201_B_png\n",
      "S0870830101_L_png\n",
      "S0553300401_S_png\n",
      "S0881560201_L_png\n",
      "S0653300101_L_png\n",
      "S0304531701_L_png\n",
      "S0163960601_U_png\n",
      "S0675320101_V_png\n",
      "S0747440142_L_png\n",
      "S0109460201_S_png\n",
      "S0770180301_L_png\n",
      "S0505760201_L_png\n",
      "S0723780201_S_png\n",
      "S0200020201_V_png\n",
      "S0744210101_B_png\n",
      "S0560180401_U_png\n",
      "S0402560401_M_png\n",
      "S0152680601_S_png\n",
      "S0109070201_B_png\n",
      "S0727760601_U_png\n",
      "S0504010101_L_png\n",
      "S0677830139_L_png\n",
      "S0124930501_B_png\n",
      "S0152860101_L_png\n",
      "S0795781101_L_png\n",
      "S0151590201_S_png\n",
      "S0057740301_U_png\n",
      "S0862220101_L_png\n",
      "S0780451101_S_png\n",
      "S0145800101_V_png\n",
      "S0823432101_U_png\n",
      "S0017740701_V_png\n",
      "S0721600501_V_png\n",
      "S0110890701_S_png\n",
      "S0780561301_L_png\n",
      "S0677770133_L_png\n",
      "S0693661801_U_png\n",
      "S0743420801_L_png\n",
      "S0693970301_U_png\n",
      "S0674560201_M_png\n",
      "S0783540201_L_png\n",
      "S0804860201_M_png\n",
      "S0810401301_U_png\n",
      "S0862770901_M_png\n",
      "S0841320201_L_png\n",
      "S0780500301_B_png\n",
      "S0200800101_V_png\n",
      "S0305560101_M_png\n",
      "S0790181801_L_png\n",
      "S0747590131_M_png\n",
      "S0883040201_S_png\n",
      "S0671430101_S_png\n",
      "S0852060201_B_png\n",
      "S0550950101_L_png\n",
      "S0610000701_M_png\n",
      "S0556212201_L_png\n",
      "S0505150401_M_png\n",
      "S0679380701_V_png\n",
      "S0822530301_B_png\n",
      "S0891800601_U_png\n",
      "S0862470301_L_png\n",
      "S0690744101_L_png\n",
      "S0830390401_M_png\n",
      "S0161160101_S_png\n",
      "S0304071801_U_png\n",
      "S0506531401_L_png\n",
      "S0200810301_M_png\n",
      "S0111550101_L_png\n",
      "S0005010301_S_png\n",
      "S0303550301_L_png\n",
      "S0552670301_B_png\n",
      "S0843270201_S_png\n",
      "S0652510101_L_png\n",
      "S0748590133_S_png\n",
      "S0691010901_B_png\n",
      "S0727760601_M_png\n",
      "S0065140201_V_png\n",
      "S0601211601_L_png\n",
      "S0882060801_S_png\n",
      "S0550950101_U_png\n",
      "S0691250301_M_png\n",
      "S0723650701_L_png\n",
      "S0673541101_U_png\n",
      "S0694500101_M_png\n",
      "S0109070201_V_png\n",
      "S0782720301_M_png\n",
      "S0761670301_U_png\n",
      "S0111290601_V_png\n",
      "S0550950101_V_png\n",
      "S0723450201_U_png\n",
      "S0406421401_M_png\n",
      "S0305240301_S_png\n",
      "S0112490301_S_png\n",
      "S0827221101_L_png\n",
      "S0551760101_M_png\n",
      "S0212480201_B_png\n",
      "S0305750501_M_png\n",
      "S0690800601_M_png\n",
      "S0550670301_L_png\n",
      "S0822530301_U_png\n",
      "S0872391401_M_png\n",
      "S0111310101_B_png\n",
      "S0770980601_L_png\n",
      "S0109070201_L_png\n",
      "S0795630101_L_png\n",
      "S0551430301_U_png\n",
      "S0670170101_L_png\n",
      "S0403760401_L_png\n",
      "S0127920801_V_png\n",
      "S0670590101_B_png\n",
      "S0203390701_M_png\n",
      "S0677640139_M_png\n",
      "S0109090101_M_png\n",
      "S0803990601_M_png\n",
      "S0883210601_M_png\n",
      "S0853220201_S_png\n",
      "S0691010101_M_png\n",
      "S0411980201_L_png\n",
      "S0723802101_V_png\n",
      "S0605581401_S_png\n",
      "S0304160401_L_png\n",
      "S0301150301_U_png\n",
      "S0108260201_M_png\n",
      "S0650381301_M_png\n",
      "S0109270101_S_png\n",
      "S0743120301_M_png\n",
      "S0147511301_S_png\n",
      "S0677770137_M_png\n",
      "S0124930501_U_png\n",
      "S0881901201_U_png\n",
      "S0207050101_L_png\n",
      "S0081341001_B_png\n",
      "S0510390201_B_png\n",
      "S0112250301_L_png\n",
      "S0804090401_U_png\n",
      "S0852180501_B_png\n",
      "S0823650501_U_png\n",
      "S0795730101_L_png\n",
      "S0740530401_M_png\n",
      "S0720700201_U_png\n",
      "S0692931401_M_png\n",
      "S0086950201_S_png\n",
      "S0844210101_S_png\n",
      "S0300910601_L_png\n",
      "S0406610501_M_png\n",
      "S0720250901_L_png\n",
      "S0651580101_L_png\n",
      "S0113070101_S_png\n",
      "S0677740137_M_png\n",
      "S0852180501_U_png\n",
      "S0785120801_L_png\n",
      "S0801740601_B_png\n",
      "S0741160301_L_png\n",
      "S0414190701_B_png\n",
      "S0108061901_M_png\n",
      "S0677670136_L_png\n",
      "S0795630101_M_png\n",
      "S0604010201_L_png\n",
      "S0411782101_B_png\n",
      "S0701230101_L_png\n",
      "S0505150401_L_png\n",
      "S0305600501_M_png\n",
      "S0842730201_L_png\n",
      "S0303420201_S_png\n",
      "S0112370801_B_png\n",
      "S0913990101_L_png\n",
      "S0604680201_B_png\n",
      "S0784450601_M_png\n",
      "S0112280101_L_png\n",
      "S0782530701_U_png\n",
      "S0603510701_M_png\n",
      "S0092360201_B_png\n",
      "S0412990201_L_png\n",
      "S0500970301_M_png\n",
      "S0605581401_M_png\n",
      "S0411782101_S_png\n",
      "S0864960101_M_png\n",
      "S0126700601_B_png\n",
      "S0672190301_M_png\n",
      "S0550930201_V_png\n",
      "S0651470401_L_png\n",
      "S0741050201_S_png\n",
      "S0743950501_V_png\n",
      "S0205920501_V_png\n",
      "S0156960201_B_png\n",
      "S0723802101_L_png\n",
      "S0843830901_L_png\n",
      "S0881990201_U_png\n",
      "S0811023301_M_png\n",
      "S0864080701_L_png\n",
      "S0658400401_U_png\n",
      "S0159760301_L_png\n",
      "S0303110401_M_png\n",
      "S0302581601_U_png\n",
      "S0743420801_M_png\n",
      "S0106660701_S_png\n",
      "S0414380901_L_png\n",
      "S0301150301_S_png\n",
      "S0550460301_V_png\n",
      "S0790620101_S_png\n",
      "S0780452001_L_png\n",
      "S0510390101_L_png\n",
      "S0800840201_U_png\n",
      "S0653880201_L_png\n",
      "S0102640701_V_png\n",
      "S0861080201_L_png\n",
      "S0853380101_S_png\n",
      "S0202730101_U_png\n",
      "S0748390901_M_png\n",
      "S0049350101_U_png\n",
      "S0827021001_M_png\n",
      "S0110930501_L_png\n",
      "S0560580301_S_png\n",
      "S0201290201_B_png\n",
      "S0304071701_U_png\n",
      "S0865350301_U_png\n",
      "S0722520101_M_png\n",
      "S0109080101_L_png\n",
      "S0130720101_S_png\n",
      "S0305360501_S_png\n",
      "S0404120101_M_png\n",
      "S0511581601_U_png\n",
      "S0505380701_L_png\n",
      "S0743830601_S_png\n",
      "S0201901701_M_png\n",
      "S0863810301_L_png\n",
      "S0844860301_S_png\n",
      "S0860260501_M_png\n",
      "S0840133101_L_png\n",
      "S0747390134_L_png\n",
      "S0804090501_B_png\n",
      "S0506211301_L_png\n",
      "S0600040101_U_png\n",
      "S0691950101_B_png\n",
      "S0651330501_S_png\n",
      "S0805200401_V_png\n",
      "S0112370801_U_png\n",
      "S0652010401_U_png\n",
      "S0655340140_L_png\n",
      "S0673000143_L_png\n",
      "S0694030101_M_png\n",
      "S0110010701_S_png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"GGtO5x2eJ77Wa0rLpQSt\")\n",
    "upload_project = rf.workspace(\"orij\").project(\"xmm_om_images_v4-contrast-512-3\")\n",
    "\n",
    "dataset_images_folder = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/zscaled_512_stretched/'\n",
    "annotations_voc_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images_v4-contrast-512-1/train/'\n",
    "annotations_ = []\n",
    "\n",
    "for annot in os.listdir(annotations_voc_dir):\n",
    "    if annot.endswith('.xml'):\n",
    "        annotations_.append(annot)\n",
    "\n",
    "for image_name in os.listdir(dataset_images_folder):\n",
    "    image_path = os.path.join(dataset_images_folder, image_name)\n",
    "    if os.path.isfile(image_path):\n",
    "        print(image_path.split('/')[-1].replace('.', '_'))\n",
    "        annotations_voc_filename = [annotation for annotation in annotations_ if annotation.startswith(image_path.split('/')[-1].replace('.', '_'))]\n",
    "        if len(annotations_voc_filename):\n",
    "            upload_project.upload(image_path, annotations_voc_dir+annotations_voc_filename[0], overwrite=True)\n",
    "\n",
    "print(\"Image upload complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi3C4uDWo10h"
   },
   "source": [
    "## AutomaticMask Generation\n",
    "\n",
    "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0Pm0RYArgm9"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# IMAGE_NAME = \"/workspace/raid/OM_DeepLearning/XMM_OM_code/gaussian_.png\"\n",
    "# IMAGE_PATH = os.path.join(OM_dir,IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdgL88fUuelk"
   },
   "source": [
    "### Generate masks with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!The predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # usually the pip install \"opencv-python-headless<4.3\" solves this problem:\n",
    "# # AttributeError: partially initialized module 'cv2' has no attribute '_registerMatType' (most likely due to a circular import)\n",
    "# # !pip install \"opencv-python-headless<4.3\"\n",
    "# # !pip install jupyter-bbox-widget\n",
    "# import cv2\n",
    "# import supervision as sv\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# # !pip install astropy\n",
    "# import astropy\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from astropy.io import fits\n",
    "# from scipy.interpolate import interp1d\n",
    "# from astropy.visualization import ZScaleInterval, ImageNormalize\n",
    "\n",
    "# def SAM_predictor(IMAGE_PATH, remove_masks_on_negative = False):\n",
    "    \n",
    "#     image_bgr = cv2.imread(IMAGE_PATH)\n",
    "#     # print(IMAGE_PATH)\n",
    "#     annotated_image = None\n",
    "#     try:\n",
    "#         image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "#         # image_rgb = image_bgr\n",
    "    \n",
    "#         sam_result = mask_generator.generate(image_rgb)\n",
    "#         output_file = IMAGE_PATH.replace(\"scaled_raw\", \"segmented_SAM\").replace(\".png\", \"_segmented.png\")\n",
    "        \n",
    "#         if remove_masks_on_negative:\n",
    "#             sam_to_keep = []\n",
    "#             for segm_index in range(len(sam_result)):\n",
    "#                 if (np.any((sam_result[segm_index]['segmentation']==1) & (mask_on_negative==1)))==0:\n",
    "#                     sam_to_keep.append(sam_result[segm_index])\n",
    "#             sam_result = sam_to_keep\n",
    "#             output_file = IMAGE_PATH.replace(\"scaled_raw\", \"segmented_SAM\").replace(\".png\", \"_segmented_removed_negative.png\")\n",
    "            \n",
    "#         mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "#         detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "#         annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "#         image = Image.fromarray(annotated_image)\n",
    "#         image.save(output_file)\n",
    "    \n",
    "#         sv.plot_images_grid(\n",
    "#             images=[image_bgr, annotated_image],\n",
    "#             grid_size=(1, 2),\n",
    "#             titles=['source image', 'segmented image']\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(\"Exception:\\n\", e)\n",
    "#         pass\n",
    "        \n",
    "#     return image_bgr, annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM_predictor(\"/workspace/raid/OM_DeepLearning/colored_atrous_wavelet_decomposition.png\")\n",
    "# SAM_predictor(IMAGE_NAME)\n",
    "\n",
    "# for file_ in os.listdir(OM_dir+\"scaled_raw/\"):\n",
    "#     if \"png\" in file_:\n",
    "#         SAM_predictor(OM_dir+\"scaled_raw/\"+file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def save_4(image1, image2, image3, image4):\n",
    "#     fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    \n",
    "#     # Plot each image in the corresponding subplot\n",
    "#     axs[0, 0].imshow(image1, cmap=\"gray\") \n",
    "#     axs[0, 0].axis('off')\n",
    "#     axs[0, 0].set_title('source image w/o distribution')\n",
    "    \n",
    "#     axs[0, 1].imshow(image2, cmap=\"gray\")\n",
    "#     axs[0, 1].axis('off')\n",
    "#     axs[0, 1].set_title('modified image w/o distribution')\n",
    "    \n",
    "#     axs[1, 0].imshow(image3, cmap=\"gray\")\n",
    "#     axs[1, 0].axis('off')\n",
    "#     axs[1, 0].set_title('source image w/ distribution')\n",
    "    \n",
    "#     axs[1, 1].imshow(image4, cmap=\"gray\")\n",
    "#     axs[1, 1].axis('off')\n",
    "#     axs[1, 1].set_title('modified image w/ distribution')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # Save the figure\n",
    "#     plt.savefig('2x2_images_grid.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "#     # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# image_bgr, annotated_image = SAM_predictor(\"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/S0112231601_V.png\")\n",
    "# image_bgr_filled, annotated_image_filled = SAM_predictor(\"/workspace/raid/OM_DeepLearning/XMM_OM_code/S0112231601_V_gaussian.png\")\n",
    "\n",
    "# sv.plot_images_grid(\n",
    "#     images=[image_bgr, annotated_image, image_bgr_filled, annotated_image_filled],\n",
    "#     grid_size=(2, 2),\n",
    "#     titles=['source image', 'segmented image', 'source image w/ distribution', 'segmented image w/ distribution']\n",
    "# )\n",
    "\n",
    "# save_4(image_bgr, annotated_image, image_bgr_filled, annotated_image_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # image_bgr, annotated_image = SAM_predictor(\"/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/S0655570201_L.png\")\n",
    "# # image_bgr_filled, annotated_image_filled = SAM_predictor(\"/workspace/raid/OM_DeepLearning/XMM_OM_code/S0655570201_L_gaussian.png\")\n",
    "\n",
    "# sv.plot_images_grid(\n",
    "#     images=[image_bgr, annotated_image, image_bgr_filled, annotated_image_filled],\n",
    "#     grid_size=(2, 2),\n",
    "#     titles=['source image', 'segmented image', 'source image w/ distribution', 'segmented image w/ distribution']\n",
    "# )\n",
    "# save_4(image_bgr, annotated_image, image_bgr_filled, annotated_image_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUNhAvdPjZ-Y"
   },
   "source": [
    "### Output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxO265XOymA2"
   },
   "source": [
    "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
    "\n",
    "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
    "* `area` - `[int]` - the area of the mask in pixels\n",
    "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
    "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
    "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
    "* `stability_score` - `[float]` - an additional measure of mask quality\n",
    "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsdFDDQnjhkP"
   },
   "source": [
    "### Interaction with segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CS_WhY60XMNL",
    "outputId": "7affc2c7-87c3-4981-a5a9-c7e135622d32"
   },
   "outputs": [],
   "source": [
    "# masks = [\n",
    "#     mask['segmentation']\n",
    "#     for mask\n",
    "#     in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
    "# ]\n",
    "\n",
    "# sv.plot_images_grid(\n",
    "#     images=masks,\n",
    "#     grid_size=(9, int(len(masks) / 8)),\n",
    "#     size=(16, 16)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
